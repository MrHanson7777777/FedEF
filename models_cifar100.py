#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
CIFAR-100æ•°æ®é›†ä¸“ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹åº“
===============================

æœ¬æ–‡ä»¶åŒ…å«é’ˆå¯¹CIFAR-100è‡ªç„¶å›¾åƒåˆ†ç±»æ•°æ®é›†ï¼ˆ32x32åƒç´ ï¼Œ3é€šé“å½©è‰²å›¾åƒï¼Œ100ä¸ªç±»åˆ«ï¼‰
ä¼˜åŒ–è®¾è®¡çš„ä¸‰ç§ä¸»æµæ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ï¼š

1. ResNet18è”é‚¦å­¦ä¹ ç‰ˆæœ¬ - åŸºäºæ®‹å·®è¿æ¥çš„æ·±åº¦ç½‘ç»œï¼Œä½¿ç”¨GroupNormé€‚é…è”é‚¦å­¦ä¹ 
2. EfficientNet-B3é£æ ¼æ¨¡å‹ - åŸºäºç§»åŠ¨ç«¯å€’ç½®æ®‹å·®å—çš„é«˜æ•ˆæ·±åº¦ç½‘ç»œ
3. DenseNetæ¨¡å‹ - åŸºäºå¯†é›†è¿æ¥çš„ç‰¹å¾å¤ç”¨ç½‘ç»œæ¶æ„

æ‰€æœ‰æ¨¡å‹éƒ½é’ˆå¯¹CIFAR-100æ•°æ®é›†çš„ç‰¹ç‚¹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œè€ƒè™‘åˆ°100ä¸ªç±»åˆ«
éœ€è¦æ›´å¼ºçš„ç‰¹å¾æå–èƒ½åŠ›å’Œæ›´å¤æ‚çš„åˆ†ç±»å™¨ã€‚

æ•°æ®é›†ç‰¹ç‚¹ï¼š
- å›¾åƒå°ºå¯¸: 32x32
- é€šé“æ•°: 3 (RGBå½©è‰²å›¾åƒ)
- ç±»åˆ«æ•°: 100 (åˆ†ä¸º20ä¸ªè¶…ç±»ï¼Œæ¯ä¸ªè¶…ç±»5ä¸ªç»†ç²’åº¦ç±»åˆ«)
- è®­ç»ƒæ ·æœ¬: 50,000 (æ¯ç±»500å¼ )
- æµ‹è¯•æ ·æœ¬: 10,000 (æ¯ç±»100å¼ )
- æŒ‘æˆ˜: ç±»åˆ«å¤šï¼Œæ¯ç±»æ ·æœ¬å°‘ï¼Œç±»é—´ç›¸ä¼¼æ€§é«˜

æ¨¡å‹ç‰¹è‰²ï¼š
- ResNet18: æ®‹å·®è¿æ¥ + æ³¨æ„åŠ›æœºåˆ¶ + è”é‚¦å­¦ä¹ å‹å¥½
- EfficientNet: ç§»åŠ¨ç«¯é«˜æ•ˆ + æ·±åº¦å¯åˆ†ç¦»å·ç§¯ + å¤åˆç¼©æ”¾
- DenseNet: å¯†é›†è¿æ¥ + ç‰¹å¾å¤ç”¨ + å‚æ•°æ•ˆç‡é«˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

def get_valid_group_count(num_channels, preferred_groups=8, max_groups=32):
    """
    è·å–æœ‰æ•ˆçš„GroupNormåˆ†ç»„æ•°ï¼Œç¡®ä¿èƒ½å¤Ÿæ•´é™¤é€šé“æ•°
    """
    # é¦–å…ˆå°è¯•ä½¿ç”¨é¦–é€‰åˆ†ç»„æ•°
    if num_channels % preferred_groups == 0:
        return preferred_groups
    
    # ä»æœ€å¤§å€¼å¼€å§‹ï¼Œå‘ä¸‹å¯»æ‰¾èƒ½æ•´é™¤çš„åˆ†ç»„æ•°
    for groups in range(min(max_groups, num_channels), 0, -1):
        if num_channels % groups == 0:
            return groups
    
    # æœ€åçš„ä¿é™©ï¼Œä½¿ç”¨1ç»„
    return 1

def replace_bn_with_gn(model, num_groups=8):
    """
    BatchNorm â†’ GroupNorm æ›¿æ¢å‡½æ•°ï¼ˆå°batchæ›´ç¨³å®šï¼‰
    ===========================================
    
    å°†æ¨¡å‹ä¸­çš„æ‰€æœ‰BatchNorm2då±‚æ›¿æ¢ä¸ºGroupNormå±‚ã€‚
    è¿™åœ¨è”é‚¦å­¦ä¹ çš„å°batchç¯å¢ƒä¸‹æ›´ç¨³å®šã€‚
    
    å‚æ•°:
        model: éœ€è¦æ›¿æ¢çš„æ¨¡å‹
        num_groups: GroupNormçš„ç»„æ•°ï¼ˆé»˜è®¤8ï¼‰
    
    ç‰¹ç‚¹:
    - è‡ªåŠ¨è®¡ç®—æœ€ä¼˜åˆ†ç»„æ•°ï¼Œç¡®ä¿é€šé“æ•°èƒ½æ•´é™¤ç»„æ•°
    - ä¿æŒåŸæœ‰çš„æƒé‡å’Œåç½®
    - é€‚é…å°batch sizeè®­ç»ƒåœºæ™¯
    """
    for name, module in model.named_children():
        if isinstance(module, nn.BatchNorm2d):
            # è·å–BatchNormçš„å‚æ•°
            num_channels = module.num_features
            groups = get_valid_group_count(num_channels, num_groups)
            
            # åˆ›å»ºGroupNormæ›¿æ¢
            group_norm = nn.GroupNorm(groups, num_channels, 
                                    eps=module.eps, 
                                    affine=module.affine)
            
            # å¦‚æœæœ‰è®­ç»ƒå¥½çš„æƒé‡ï¼Œå¤åˆ¶è¿‡æ¥
            if module.affine:
                group_norm.weight.data.copy_(module.weight.data)
                group_norm.bias.data.copy_(module.bias.data)
            
            # æ›¿æ¢æ¨¡å—
            setattr(model, name, group_norm)
        else:
            # é€’å½’å¤„ç†å­æ¨¡å—
            replace_bn_with_gn(module, num_groups)
    
    return model

def replace_bn_with_ln(model):
    """
    BatchNorm â†’ LayerNorm æ›¿æ¢å‡½æ•°
    =============================
    
    å°†æ¨¡å‹ä¸­çš„æ‰€æœ‰BatchNorm2då±‚æ›¿æ¢ä¸ºLayerNormå±‚ã€‚
    LayerNormåœ¨æŸäº›æƒ…å†µä¸‹æ¯”GroupNormæ›´ç¨³å®šã€‚
    
    å‚æ•°:
        model: éœ€è¦æ›¿æ¢çš„æ¨¡å‹
    """
    for name, module in model.named_children():
        if isinstance(module, nn.BatchNorm2d):
            # è·å–BatchNormçš„å‚æ•°
            num_channels = module.num_features
            
            # åˆ›å»ºLayerNormæ›¿æ¢ (å¯¹é€šé“ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–)
            layer_norm = nn.GroupNorm(1, num_channels, 
                                    eps=module.eps, 
                                    affine=module.affine)
            
            # å¦‚æœæœ‰è®­ç»ƒå¥½çš„æƒé‡ï¼Œå¤åˆ¶è¿‡æ¥
            if module.affine:
                layer_norm.weight.data.copy_(module.weight.data)
                layer_norm.bias.data.copy_(module.bias.data)
            
            # æ›¿æ¢æ¨¡å—
            setattr(model, name, layer_norm)
        else:
            # é€’å½’å¤„ç†å­æ¨¡å—
            replace_bn_with_ln(module)
    
    return model

import math


class SEBlock(nn.Module):
    """
    Squeeze-and-Excitation (SE) æ³¨æ„åŠ›æ¨¡å—
    =====================================
    
    è½»é‡çº§çš„é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå­¦ä¹ é€šé“é—´çš„é‡è¦æ€§æƒé‡ã€‚
    é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–è·å¾—é€šé“ç»Ÿè®¡ä¿¡æ¯ï¼Œç„¶åç”¨å…¨è¿æ¥å±‚å­¦ä¹ é€šé“æƒé‡ã€‚
    
    ä¼˜åŠ¿ï¼š
    - å‚æ•°é‡å°‘ï¼šä»…å¢åŠ å¾ˆå°‘çš„å‚æ•°é‡
    - æ€§èƒ½æå‡æ˜æ˜¾ï¼šé€šå¸¸èƒ½æå‡1-2%çš„å‡†ç¡®ç‡
    - å³æ’å³ç”¨ï¼šå¯ä»¥è½»æ¾é›†æˆåˆ°ä»»ä½•CNNæ¶æ„ä¸­
    """
    
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        # Squeeze: å…¨å±€å¹³å‡æ± åŒ–
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        
        # Excitation: ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œå­¦ä¹ é€šé“æƒé‡
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b, c, _, _ = x.size()
        # Squeeze: å…¨å±€å¹³å‡æ± åŒ– [B, C, H, W] -> [B, C, 1, 1]
        y = self.avg_pool(x).view(b, c)
        
        # Excitation: å­¦ä¹ é€šé“æƒé‡ [B, C] -> [B, C]
        y = self.fc(y).view(b, c, 1, 1)
        
        # é‡æ–°åŠ æƒç‰¹å¾å›¾ [B, C, H, W] * [B, C, 1, 1] -> [B, C, H, W]
        return x * y.expand_as(x)


class ECABlock(nn.Module):
    """
    Efficient Channel Attention (ECA) æ¨¡å—
    ======================================
    
    æ›´é«˜æ•ˆçš„é€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œé¿å…äº†SEæ¨¡å—çš„é™ç»´æ“ä½œã€‚
    ä½¿ç”¨1Då·ç§¯ç›´æ¥å­¦ä¹ é€šé“é—´çš„äº¤äº’å…³ç³»ã€‚
    
    ä¼˜åŠ¿ï¼š
    - æ›´å°‘çš„å‚æ•°ï¼šä¸éœ€è¦é™ç»´æ“ä½œ
    - æ›´é«˜çš„æ•ˆç‡ï¼šè®¡ç®—å¤æ‚åº¦æ›´ä½
    - æ€§èƒ½ç›¸å½“ï¼šæ•ˆæœæ¥è¿‘æˆ–ä¼˜äºSEæ¨¡å—
    """
    
    def __init__(self, channels, gamma=2, b=1):
        super(ECABlock, self).__init__()
        # è‡ªé€‚åº”è®¡ç®—å·ç§¯æ ¸å¤§å°
        t = int(abs((math.log(channels, 2) + b) / gamma))
        k_size = t if t % 2 else t + 1
        
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        b, c, _, _ = x.size()
        
        # å…¨å±€å¹³å‡æ± åŒ–å¹¶è°ƒæ•´ç»´åº¦ç”¨äº1Då·ç§¯
        y = self.avg_pool(x).squeeze(-1).transpose(-1, -2)  # [B, 1, C]
        
        # 1Då·ç§¯å­¦ä¹ é€šé“äº¤äº’
        y = self.conv(y).transpose(-1, -2).unsqueeze(-1)  # [B, C, 1, 1]
        
        # Sigmoidæ¿€æ´»å¹¶é‡æ–°åŠ æƒ
        y = self.sigmoid(y)
        return x * y.expand_as(x)


class BasicBlock_CIFAR100(nn.Module):
    """
    CIFAR-100ä¸“ç”¨ResNetåŸºç¡€å—
    =========================
    
    é’ˆå¯¹100åˆ†ç±»ä»»åŠ¡ä¼˜åŒ–çš„ResNetåŸºç¡€æ®‹å·®å—ã€‚
    ä½¿ç”¨æ›´å¼ºçš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œé€‚åº”æ›´ç»†ç²’åº¦çš„åˆ†ç±»éœ€æ±‚ã€‚
    """
    expansion = 1
    
    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.1):
        super(BasicBlock_CIFAR100, self).__init__()
        # ç¬¬ä¸€ä¸ª3x3å·ç§¯å±‚ï¼Œå¯èƒ½æ”¹å˜ç©ºé—´åˆ†è¾¨ç‡
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, 
                              padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        
        # ç¬¬äºŒä¸ª3x3å·ç§¯å±‚ï¼Œä¿æŒç©ºé—´åˆ†è¾¨ç‡ä¸å˜
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, 
                              padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        
        # æ·»åŠ Dropoutæé«˜æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else nn.Identity()
        
        # æ®‹å·®è¿æ¥çš„å¿«æ·è·¯å¾„ï¼Œå½“è¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒ¹é…æ—¶éœ€è¦æŠ•å½±
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != planes:
            # ä½¿ç”¨1x1å·ç§¯è¿›è¡Œç»´åº¦åŒ¹é…
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes)
            )

    def forward(self, x):
        # ç¬¬ä¸€ä¸ªå·ç§¯ -> BN -> ReLU
        out = F.relu(self.bn1(self.conv1(x)))
        # åº”ç”¨Dropoutæ­£åˆ™åŒ–
        out = self.dropout(out)
        # ç¬¬äºŒä¸ªå·ç§¯ -> BNï¼ˆä¸åŠ æ¿€æ´»å‡½æ•°ï¼‰
        out = self.bn2(self.conv2(out))
        # æ®‹å·®è¿æ¥ï¼šè¾“å‡º = F(x) + x
        out += self.shortcut(x)
        # æœ€ç»ˆæ¿€æ´»
        out = F.relu(out)
        return out


class SEBlock(nn.Module):
    """
    æŒ¤å‹æ¿€åŠ±æ³¨æ„åŠ›æ¨¡å— (Squeeze-and-Excitation Block)
    ================================================
    
    é€šè¿‡å­¦ä¹ é€šé“é—´çš„ä¾èµ–å…³ç³»æ¥é‡æ–°æ ¡å‡†ç‰¹å¾å›¾ã€‚
    å¯¹äºCIFAR-100è¿™æ ·çš„ç»†ç²’åº¦åˆ†ç±»ä»»åŠ¡ç‰¹åˆ«æœ‰æ•ˆã€‚
    
    å·¥ä½œåŸç†ï¼š
    1. Squeeze: å…¨å±€å¹³å‡æ± åŒ–å‹ç¼©ç©ºé—´ç»´åº¦
    2. Excitation: ä¸¤ä¸ªå…¨è¿æ¥å±‚å­¦ä¹ é€šé“é‡è¦æ€§
    3. Scale: ç”¨å­¦åˆ°çš„æƒé‡é‡æ–°æ ‡å®šåŸç‰¹å¾å›¾
    """
    
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        # å…¨å±€å¹³å‡æ± åŒ–ï¼Œå°†HÃ—Wå‹ç¼©ä¸º1Ã—1
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        # ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œå­¦ä¹ é€šé“é‡è¦æ€§
        self.fc = nn.Sequential(
            # é™ç»´å±‚ï¼Œå‡å°‘å‚æ•°é‡
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            # å‡ç»´å±‚ï¼Œæ¢å¤åˆ°åŸé€šé“æ•°
            nn.Linear(channels // reduction, channels, bias=False),
            # Sigmoidç¡®ä¿è¾“å‡ºåœ¨[0,1]èŒƒå›´
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        # Squeeze: å…¨å±€å¹³å‡æ± åŒ– [B, C, H, W] -> [B, C, 1, 1] -> [B, C]
        y = self.avg_pool(x).view(b, c)
        # Excitation: å­¦ä¹ é€šé“é‡è¦æ€§ [B, C] -> [B, C]
        y = self.fc(y).view(b, c, 1, 1)
        # Scale: é‡æ–°æ ‡å®šç‰¹å¾å›¾ [B, C, H, W] * [B, C, 1, 1]
        return x * y.expand_as(x)


class ResNet18_CIFAR100_Fed(nn.Module):
    """
    CIFAR-100ä¸“ç”¨è”é‚¦å­¦ä¹ ResNet18æ¨¡å‹
    =================================
    
    ä¸“é—¨ä¸ºCIFAR-100è®¾è®¡çš„ResNet18å˜ä½“ï¼Œæ”¯æŒ100ä¸ªç±»åˆ«çš„åˆ†ç±»ã€‚
    ä½¿ç”¨GroupNormæ›¿ä»£BatchNormï¼Œå¢åŠ æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜ç»†ç²’åº¦åˆ†ç±»èƒ½åŠ›ã€‚
    
    ç½‘ç»œæ¶æ„ï¼š
    - è¾“å…¥ï¼š32x32x3çš„RGBå›¾åƒ
    - 4ä¸ªæ®‹å·®å±‚ï¼š[2, 2, 2, 2]ä¸ªåŸºç¡€å—
    - æŒ¤å‹æ¿€åŠ±æ³¨æ„åŠ›æ¨¡å—
    - è¾“å‡ºï¼š100ä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ
    
    é’ˆå¯¹CIFAR-100ä¼˜åŒ–ï¼š
    1. æ›´å®½çš„ç½‘ç»œé€šé“æ•°
    2. æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç‰¹å¾è¡¨ç¤º
    3. æ›´å¤æ‚çš„åˆ†ç±»å™¨
    4. é€‚å½“çš„æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
    """
    
    def __init__(self, num_classes=100, use_groupnorm=True, num_groups=8, 
                 use_se=True, dropout_rate=0.3):
        super(ResNet18_CIFAR100_Fed, self).__init__()
        self.in_planes = 64  # å½“å‰å±‚çš„è¾“å…¥é€šé“æ•°
        self.use_groupnorm = use_groupnorm
        self.num_groups = num_groups
        self.use_se = use_se
        
        # åˆå§‹å·ç§¯å±‚ï¼š3é€šé“RGB -> 64é€šé“ç‰¹å¾å›¾
        # ä½¿ç”¨è¾ƒå°çš„kernelé¿å…CIFAR-100å°å›¾åƒä¿¡æ¯ä¸¢å¤±
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        
        # 4ä¸ªæ®‹å·®å±‚ï¼ˆæ¯”CIFAR-10ä½¿ç”¨æ›´å¤šé€šé“é€‚åº”100ç±»ï¼‰
        # layer1: 64é€šé“ï¼Œç©ºé—´å°ºå¯¸32x32
        self.layer1 = self._make_layer(64, 2, stride=1, dropout_rate=0.1)
        # layer2: 128é€šé“ï¼Œç©ºé—´å°ºå¯¸16x16
        self.layer2 = self._make_layer(128, 2, stride=2, dropout_rate=0.1)
        # layer3: 256é€šé“ï¼Œç©ºé—´å°ºå¯¸8x8
        self.layer3 = self._make_layer(256, 2, stride=2, dropout_rate=0.2)
        # layer4: 512é€šé“ï¼Œç©ºé—´å°ºå¯¸4x4
        self.layer4 = self._make_layer(512, 2, stride=2, dropout_rate=0.2)
        
        # æ³¨æ„åŠ›æ¨¡å—ï¼Œå¢å¼ºç‰¹å¾è¡¨ç¤ºèƒ½åŠ›
        if use_se:
            self.se = SEBlock(512)
        
        # å…¨å±€å¹³å‡æ± åŒ–ï¼Œå°†4x4ç‰¹å¾å›¾å‹ç¼©ä¸º1x1
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        # åˆ†ç±»å™¨ - ä¸º100ç±»è®¾è®¡çš„æ›´å¤æ‚åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            # ç¬¬ä¸€å±‚ï¼š512 -> 256ï¼Œå¢åŠ éçº¿æ€§è¡¨è¾¾èƒ½åŠ›
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate / 2),
            # ç¬¬äºŒå±‚ï¼š256 -> 100ç±»è¾“å‡º
            nn.Linear(256, num_classes)
        )
        
        # å¦‚æœä½¿ç”¨GroupNormï¼Œåˆ™æ›¿æ¢æ‰€æœ‰BatchNormï¼ˆè”é‚¦å­¦ä¹ å‹å¥½ï¼‰
        if use_groupnorm:
            self._replace_bn_with_gn()
        
        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()

    def _make_layer(self, planes, blocks, stride, dropout_rate=0.1):
        """
        æ„å»ºæ®‹å·®å±‚
        
        Args:
            planes: è¾“å‡ºé€šé“æ•°
            blocks: æ®‹å·®å—æ•°é‡
            stride: ç¬¬ä¸€ä¸ªå—çš„æ­¥é•¿
            dropout_rate: Dropoutæ¯”ç‡
        """
        strides = [stride] + [1] * (blocks - 1)  # åªæœ‰ç¬¬ä¸€ä¸ªå—å¯èƒ½æ”¹å˜ç©ºé—´å°ºå¯¸
        layers = []
        for s in strides:
            layers.append(BasicBlock_CIFAR100(self.in_planes, planes, s, dropout_rate))
            self.in_planes = planes  # æ›´æ–°è¾“å…¥é€šé“æ•°
        return nn.Sequential(*layers)

    def forward(self, x):
        # åˆå§‹ç‰¹å¾æå–ï¼š[B, 3, 32, 32] -> [B, 64, 32, 32]
        out = F.relu(self.bn1(self.conv1(x)))
        
        # 4ä¸ªæ®‹å·®é˜¶æ®µï¼Œé€æ­¥æå–æ›´é«˜çº§ç‰¹å¾
        out = self.layer1(out)  # [B, 64, 32, 32]
        out = self.layer2(out)  # [B, 128, 16, 16]
        out = self.layer3(out)  # [B, 256, 8, 8]
        out = self.layer4(out)  # [B, 512, 4, 4]
        
        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç‰¹å¾è¡¨ç¤º
        if self.use_se:
            out = self.se(out)  # [B, 512, 4, 4] -> [B, 512, 4, 4]
        
        # å…¨å±€å¹³å‡æ± åŒ–ï¼š[B, 512, 4, 4] -> [B, 512, 1, 1]
        out = self.avgpool(out)
        # å±•å¹³ï¼š[B, 512, 1, 1] -> [B, 512]
        out = torch.flatten(out, 1)
        # åˆ†ç±»å™¨ï¼š[B, 512] -> [B, 100]
        out = self.classifier(out)
        # è¿”å›logæ¦‚ç‡åˆ†å¸ƒ
        return F.log_softmax(out, dim=1)
    
    def _replace_bn_with_gn(self):
        """
        å°†BatchNormæ›¿æ¢ä¸ºGroupNorm
        GroupNormåœ¨è”é‚¦å­¦ä¹ ä¸­æ›´ç¨³å®šï¼Œä¸ä¾èµ–batchç»Ÿè®¡
        """
        def replace_bn(module):
            for name, child in module.named_children():
                if isinstance(child, nn.BatchNorm2d):
                    num_channels = child.num_features
                    # ç¡®ä¿groupsæ•°èƒ½æ•´é™¤é€šé“æ•°
                    groups = get_valid_group_count(num_channels, self.num_groups)
                    gn = nn.GroupNorm(groups, num_channels)
                    setattr(module, name, gn)
                else:
                    # é€’å½’å¤„ç†å­æ¨¡å—
                    replace_bn(child)
        
        replace_bn(self)
    
    def _initialize_weights(self):
        """
        æƒé‡åˆå§‹åŒ–ï¼Œä½¿ç”¨Heåˆå§‹åŒ–ç­‰æ ‡å‡†æ–¹æ³•
        """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                # å·ç§¯å±‚ä½¿ç”¨Kaimingåˆå§‹åŒ–
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                # å½’ä¸€åŒ–å±‚æƒé‡åˆå§‹åŒ–ä¸º1ï¼Œåç½®ä¸º0ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if hasattr(m, 'weight') and m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if hasattr(m, 'bias') and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                # å…¨è¿æ¥å±‚ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
                nn.init.normal_(m.weight, 0, 0.01)
                if hasattr(m, 'bias') and m.bias is not None:
                    nn.init.constant_(m.bias, 0)


class MBConvBlock_CIFAR100(nn.Module):
    """
    CIFAR-100ä¸“ç”¨å¢å¼ºMBConvå— (Mobile Inverted Bottleneck Convolution)
    ================================================================
    
    é’ˆå¯¹100åˆ†ç±»ä»»åŠ¡å¢å¼ºçš„MobileNetå€’ç½®æ®‹å·®å—ã€‚
    ç»“åˆäº†æ·±åº¦å¯åˆ†ç¦»å·ç§¯å’Œå€’ç½®æ®‹å·®è¿æ¥çš„é«˜æ•ˆè®¾è®¡ã€‚
    
    å·¥ä½œæµç¨‹ï¼š
    1. Expansion: 1x1å·ç§¯æ‰©å±•é€šé“æ•°
    2. Depthwise: 3x3æ·±åº¦å¯åˆ†ç¦»å·ç§¯æå–ç©ºé—´ç‰¹å¾
    3. SE: æŒ¤å‹æ¿€åŠ±æ³¨æ„åŠ›æœºåˆ¶
    4. Projection: 1x1å·ç§¯å‹ç¼©é€šé“æ•°
    5. Residual: æ®‹å·®è¿æ¥ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
    """
    
    def __init__(self, in_channels, out_channels, stride, expand_ratio, 
                 se_ratio=0.25, dropout_rate=0.1):
        super(MBConvBlock_CIFAR100, self).__init__()
        self.stride = stride
        # åªæœ‰å½“æ­¥é•¿ä¸º1ä¸”è¾“å…¥è¾“å‡ºé€šé“ç›¸åŒæ—¶æ‰ä½¿ç”¨æ®‹å·®è¿æ¥
        self.use_residual = stride == 1 and in_channels == out_channels
        self.dropout_rate = dropout_rate
        
        # æ‰©å±•é˜¶æ®µï¼šå¢åŠ é€šé“æ•°ä»¥æé«˜è¡¨è¾¾èƒ½åŠ›
        if expand_ratio != 1:
            expanded_channels = in_channels * expand_ratio
            self.expand_conv = nn.Sequential(
                # 1x1å·ç§¯æ‰©å±•é€šé“
                nn.Conv2d(in_channels, expanded_channels, 1, bias=False),
                nn.BatchNorm2d(expanded_channels),
                nn.ReLU(inplace=True)
            )
            self.has_expansion = True
        else:
            # æ‰©å±•æ¯”ä¸º1æ—¶ä¸éœ€è¦æ‰©å±•
            expanded_channels = in_channels
            self.expand_conv = nn.Identity()
            self.has_expansion = False
            
        self.expanded_channels = expanded_channels
            
        # æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼šæ¯ä¸ªé€šé“ç‹¬ç«‹è¿›è¡Œç©ºé—´å·ç§¯
        self.depthwise_conv = nn.Sequential(
            nn.Conv2d(expanded_channels, expanded_channels, 3, stride, 1, 
                     groups=expanded_channels, bias=False),  # groups=è¾“å…¥é€šé“æ•°å®ç°æ·±åº¦å¯åˆ†ç¦»
            nn.BatchNorm2d(expanded_channels),
            nn.ReLU(inplace=True)
        )
        
        # å¢å¼ºçš„æŒ¤å‹æ¿€åŠ±æ³¨æ„åŠ›æœºåˆ¶
        if se_ratio > 0:
            se_channels = max(1, int(in_channels * se_ratio))
            self.se = nn.Sequential(
                nn.AdaptiveAvgPool2d(1),  # å…¨å±€å¹³å‡æ± åŒ–
                nn.Conv2d(expanded_channels, se_channels, 1),  # é™ç»´
                nn.ReLU(inplace=True),
                nn.Conv2d(se_channels, expanded_channels, 1),  # å‡ç»´
                nn.Sigmoid()  # è¾“å‡º[0,1]æƒé‡
            )
        else:
            self.se = nn.Identity()
            
        # ç‚¹å·ç§¯æŠ•å½±ï¼šå‹ç¼©é€šé“æ•°åˆ°ç›®æ ‡ç»´åº¦
        self.project_conv = nn.Sequential(
            nn.Conv2d(expanded_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
        
        # Dropoutæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
        if dropout_rate > 0:
            self.dropout = nn.Dropout2d(dropout_rate)
        else:
            self.dropout = nn.Identity()
        
    def forward(self, x):
        identity = x  # ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
        
        # 1. æ‰©å±•é˜¶æ®µ
        if self.has_expansion:
            x = self.expand_conv(x)
        
        # 2. æ·±åº¦å¯åˆ†ç¦»å·ç§¯
        x = self.depthwise_conv(x)
        
        # 3. æŒ¤å‹æ¿€åŠ±æ³¨æ„åŠ›
        if hasattr(self.se, 'weight') or len(list(self.se.modules())) > 1:
            se_weight = self.se(x)
            x = x * se_weight
            
        # 4. æŠ•å½±åˆ°è¾“å‡ºç»´åº¦
        x = self.project_conv(x)
        
        # 5. Dropoutæ­£åˆ™åŒ–
        x = self.dropout(x)
        
        # 6. æ®‹å·®è¿æ¥ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if self.use_residual:
            x = x + identity
            
        return x


class EfficientNet_CIFAR100(nn.Module):
    """
    CIFAR-100ä¸“ç”¨EfficientNet-B3é£æ ¼æ¨¡å‹
    ===================================
    
    åŸºäºEfficientNet-B3çš„æ·±åº¦ç½‘ç»œï¼Œä¸“é—¨ä¸ºCIFAR-100çš„100åˆ†ç±»ä»»åŠ¡è®¾è®¡ã€‚
    ä½¿ç”¨å¤åˆç¼©æ”¾æ–¹æ³•å¹³è¡¡ç½‘ç»œæ·±åº¦ã€å®½åº¦å’Œåˆ†è¾¨ç‡ã€‚
    
    ç½‘ç»œç‰¹ç‚¹ï¼š
    - æ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼šå‡å°‘å‚æ•°é‡å’Œè®¡ç®—é‡
    - å€’ç½®æ®‹å·®ç»“æ„ï¼šæé«˜ç‰¹å¾è¡¨è¾¾èƒ½åŠ›
    - æŒ¤å‹æ¿€åŠ±æ³¨æ„åŠ›ï¼šå¢å¼ºé‡è¦ç‰¹å¾
    - å¤åˆç¼©æ”¾ï¼šå¹³è¡¡å„ç»´åº¦çš„æ‰©å±•
    
    é€‚ç”¨åœºæ™¯ï¼š
    - é«˜ç²¾åº¦CIFAR-100åˆ†ç±»
    - èµ„æºç›¸å¯¹å……è¶³çš„åœºæ™¯
    - éœ€è¦é«˜æ•ˆæ¨ç†çš„ä»»åŠ¡
    """
    
    def __init__(self, num_classes=100, dropout_rate=0.3):
        super(EfficientNet_CIFAR100, self).__init__()
        
        # Stemå±‚ï¼šåˆå§‹ç‰¹å¾æå–
        # å°†3é€šé“RGBå›¾åƒè½¬æ¢ä¸º40é€šé“ç‰¹å¾å›¾
        self.stem = nn.Sequential(
            nn.Conv2d(3, 40, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(40),
            nn.ReLU(inplace=True)
        )
        
        # 7ä¸ªMBConvé˜¶æ®µï¼Œé€æ­¥æå–æ›´é«˜çº§ç‰¹å¾
        # æ¯ä¸ªé˜¶æ®µä½¿ç”¨ä¸åŒçš„é€šé“æ•°å’Œé‡å¤æ¬¡æ•°
        self.stage1 = self._make_stage(40, 24, 1, 1, expand_ratio=1)     # 32x32ï¼ŒåŸºç¡€ç‰¹å¾
        self.stage2 = self._make_stage(24, 32, 2, 2, expand_ratio=6)     # 16x16ï¼Œæµ…å±‚ç‰¹å¾
        self.stage3 = self._make_stage(32, 48, 3, 2, expand_ratio=6)     # 8x8ï¼Œä¸­å±‚ç‰¹å¾  
        self.stage4 = self._make_stage(48, 96, 4, 2, expand_ratio=6)     # 4x4ï¼Œæ·±å±‚ç‰¹å¾
        self.stage5 = self._make_stage(96, 136, 4, 1, expand_ratio=6)    # 4x4ï¼Œæ›´æ·±ç‰¹å¾
        self.stage6 = self._make_stage(136, 232, 5, 2, expand_ratio=6)   # 2x2ï¼Œé«˜çº§ç‰¹å¾
        self.stage7 = self._make_stage(232, 384, 2, 1, expand_ratio=6)   # 2x2ï¼Œæœ€é«˜çº§ç‰¹å¾
        
        # Headå±‚ï¼šæœ€ç»ˆç‰¹å¾è½¬æ¢
        # å°†384é€šé“æ‰©å±•åˆ°1536é€šé“ä»¥å¢å¼ºè¡¨è¾¾èƒ½åŠ›
        self.head_conv = nn.Conv2d(384, 1536, kernel_size=1, bias=False)
        self.head_bn = nn.BatchNorm2d(1536)
        self.avgpool = nn.AdaptiveAvgPool2d(1)  # å…¨å±€å¹³å‡æ± åŒ–
        
        # æ›´å¤æ‚çš„åˆ†ç±»å™¨é€‚åº”100ä¸ªç±»åˆ«
        # ä½¿ç”¨å¤šå±‚ç»“æ„é€æ­¥é™ç»´åˆ°ç±»åˆ«æ•°
        self.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(1536, 512),  # ç¬¬ä¸€å±‚é™ç»´
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate / 2),
            nn.Linear(512, 256),   # ç¬¬äºŒå±‚é™ç»´
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate / 4),
            nn.Linear(256, num_classes)  # è¾“å‡º100ä¸ªç±»åˆ«
        )
        
        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
        
    def _make_stage(self, in_channels, out_channels, num_blocks, stride, expand_ratio):
        """
        æ„å»ºMBConvé˜¶æ®µ
        
        Args:
            in_channels: è¾“å…¥é€šé“æ•°
            out_channels: è¾“å‡ºé€šé“æ•°  
            num_blocks: è¯¥é˜¶æ®µçš„MBConvå—æ•°é‡
            stride: ç¬¬ä¸€ä¸ªå—çš„æ­¥é•¿
            expand_ratio: é€šé“æ‰©å±•æ¯”ä¾‹
        """
        layers = []
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½æ”¹å˜ç©ºé—´åˆ†è¾¨ç‡
        layers.append(MBConvBlock_CIFAR100(in_channels, out_channels, stride, expand_ratio))
        # åç»­å—ä¿æŒç©ºé—´åˆ†è¾¨ç‡ä¸å˜
        for _ in range(1, num_blocks):
            layers.append(MBConvBlock_CIFAR100(out_channels, out_channels, 1, expand_ratio))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # Stemç‰¹å¾æå–ï¼š[B, 3, 32, 32] -> [B, 40, 32, 32]
        x = self.stem(x)
        
        # 7ä¸ªMBConvé˜¶æ®µï¼Œé€æ­¥æå–å±‚æ¬¡åŒ–ç‰¹å¾
        x = self.stage1(x)  # [B, 40, 32, 32] -> [B, 24, 32, 32]
        x = self.stage2(x)  # [B, 24, 32, 32] -> [B, 32, 16, 16]
        x = self.stage3(x)  # [B, 32, 16, 16] -> [B, 48, 8, 8]
        x = self.stage4(x)  # [B, 48, 8, 8] -> [B, 96, 4, 4]
        x = self.stage5(x)  # [B, 96, 4, 4] -> [B, 136, 4, 4]
        x = self.stage6(x)  # [B, 136, 4, 4] -> [B, 232, 2, 2]
        x = self.stage7(x)  # [B, 232, 2, 2] -> [B, 384, 2, 2]
        
        # Headå±‚ç‰¹å¾å¢å¼ºï¼š[B, 384, 2, 2] -> [B, 1536, 2, 2]
        x = F.relu(self.head_bn(self.head_conv(x)))
        
        # å…¨å±€å¹³å‡æ± åŒ–ï¼š[B, 1536, 2, 2] -> [B, 1536, 1, 1]
        x = self.avgpool(x)
        # å±•å¹³ï¼š[B, 1536, 1, 1] -> [B, 1536]
        x = torch.flatten(x, 1)
        # åˆ†ç±»å™¨ï¼š[B, 1536] -> [B, 100]
        x = self.classifier(x)
        
        # è¿”å›logæ¦‚ç‡åˆ†å¸ƒ
        return F.log_softmax(x, dim=1)
    
    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                if hasattr(m, 'weight') and m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if hasattr(m, 'bias') and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if hasattr(m, 'bias') and m.bias is not None:
                    nn.init.constant_(m.bias, 0)

class DenseBlock(nn.Module):
    """
    æ”¹è¿›çš„DenseNeté£æ ¼å¯†é›†è¿æ¥å—ï¼ˆå¢å¼ºç‰ˆï¼‰
    ======================================
    
    ğŸ”§ æ¨¡å‹å±‚é¢æ”¹è¿›ï¼š
    - BN â†’ GN/IN æ›¿æ¢ï¼šå°batchæ›´ç¨³å®š
    - æ³¨æ„åŠ›æœºåˆ¶ï¼šDenseBlockè¾“å‡ºåŠ ECAæ³¨æ„åŠ›
    - çµæ´»çš„å½’ä¸€åŒ–æ–¹å¼ï¼šæ”¯æŒBatchNormã€GroupNormã€LayerNorm
    
    ğŸ—ï¸ æ”¹è¿›ç‚¹ï¼š
    - æ·»åŠ ECAæ³¨æ„åŠ›æ¨¡å—ï¼šè½»é‡çº§é€šé“æ³¨æ„åŠ›ï¼Œæå‡1-2%å‡†ç¡®ç‡
    - æ”¯æŒå¤šç§å½’ä¸€åŒ–ï¼šæ›´é€‚åˆè”é‚¦å­¦ä¹ çš„å°batchåœºæ™¯
    - ä¼˜åŒ–çš„ç‰¹å¾èåˆï¼šæ›´å¥½çš„ç‰¹å¾å¤ç”¨
    """
    
    def __init__(self, in_channels, growth_rate, num_layers, dropout_rate=0.1, 
                 use_attention=True, normalization='groupnorm'):
        super(DenseBlock, self).__init__()
        self.layers = nn.ModuleList()
        self.use_attention = use_attention
        self.normalization = normalization
        
        # æ„å»ºå¯†é›†è¿æ¥çš„å·ç§¯å±‚
        for i in range(num_layers):
            # æ¯ä¸€å±‚çš„è¾“å…¥æ˜¯å‰é¢æ‰€æœ‰å±‚çš„ç‰¹å¾æ‹¼æ¥
            layer_input_channels = in_channels + i * growth_rate
            
            # é€‰æ‹©å½’ä¸€åŒ–æ–¹å¼
            def get_norm_layer(num_channels):
                if normalization == 'groupnorm':
                    groups = get_valid_group_count(num_channels)
                    return nn.GroupNorm(groups, num_channels)
                elif normalization == 'layernorm':
                    return nn.GroupNorm(1, num_channels)  # LayerNormç­‰ä»·äºGroupNorm(1, ...)
                else:  # batchnorm
                    return nn.BatchNorm2d(num_channels)
            
            # DenseNetçš„æ ‡å‡†å±‚ç»“æ„ï¼šNorm-ReLU-Conv1x1-Norm-ReLU-Conv3x3
            layer = nn.Sequential(
                # ç¬¬ä¸€ä¸ªNorm-ReLU-Convï¼šå‹ç¼©ç‰¹å¾ç»´åº¦
                get_norm_layer(layer_input_channels),
                nn.ReLU(inplace=True),
                nn.Conv2d(layer_input_channels, growth_rate * 4, 1, bias=False),  # 1x1å·ç§¯
                
                # ç¬¬äºŒä¸ªNorm-ReLU-Convï¼šæå–ç©ºé—´ç‰¹å¾
                get_norm_layer(growth_rate * 4),
                nn.ReLU(inplace=True),
                nn.Conv2d(growth_rate * 4, growth_rate, 3, padding=1, bias=False),  # 3x3å·ç§¯
                
                # Dropoutæ­£åˆ™åŒ–
                nn.Dropout2d(dropout_rate) if dropout_rate > 0 else nn.Identity()
            )
            self.layers.append(layer)
        
        # åœ¨DenseBlockæœ«å°¾æ·»åŠ æ³¨æ„åŠ›æ¨¡å—
        if self.use_attention:
            final_channels = in_channels + num_layers * growth_rate
            self.attention = ECABlock(final_channels)
    
    def forward(self, x):
        # ç‰¹å¾åˆ—è¡¨ï¼Œåˆå§‹åŒ…å«è¾“å…¥ç‰¹å¾
        features = [x]
        
        # é€å±‚å‰å‘ä¼ æ’­ï¼Œæ¯å±‚è¾“å…¥æ˜¯æ‰€æœ‰å‰å±‚ç‰¹å¾çš„æ‹¼æ¥
        for layer in self.layers:
            # æ‹¼æ¥æ‰€æœ‰å·²æœ‰ç‰¹å¾ä½œä¸ºå½“å‰å±‚è¾“å…¥
            new_feature = layer(torch.cat(features, 1))
            # å°†æ–°ç‰¹å¾åŠ å…¥ç‰¹å¾åˆ—è¡¨
            features.append(new_feature)
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾ï¼ˆåŒ…æ‹¬è¾“å…¥ï¼‰
        output = torch.cat(features, 1)
        
        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆåœ¨DenseBlockè¾“å‡ºåŠ æ³¨æ„åŠ›ï¼ŒCIFAR-100ä¸Šèƒ½æå‡1-2%ï¼‰
        if self.use_attention:
            output = self.attention(output)
        
        return output


class DenseNet_CIFAR100(nn.Module):
    """
    CIFAR-100ä¸“ç”¨DenseNetæ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰
    ================================
    
    é’ˆå¯¹CIFAR-100ä¼˜åŒ–çš„å¯†é›†è¿æ¥ç½‘ç»œï¼Œæ”¯æŒå¤šç§å¢å¼ºé…ç½®ï¼š
    
    ğŸ”§ æ¨¡å‹å±‚é¢æ”¹è¿›ï¼š
    - BN â†’ GN/IN æ›¿æ¢ï¼šå°batchæ›´ç¨³å®š
    - æ³¨æ„åŠ›æœºåˆ¶ï¼šDenseBlockè¾“å‡ºåŠ SEBlock/ECAæ³¨æ„åŠ›
    - æ›´æ·±/æ›´å®½é…ç½®ï¼šæ”¯æŒå¤šç§growth_rateå’Œå±‚æ•°é…ç½®
    
    ğŸ—ï¸ ç½‘ç»œæ¶æ„ï¼š
    - åˆå§‹å·ç§¯ï¼šæå–åŸºç¡€ç‰¹å¾
    - 4ä¸ªå¯†é›†å—ï¼šé€æ­¥åŠ æ·±ç½‘ç»œå¹¶å¤ç”¨ç‰¹å¾  
    - 3ä¸ªè¿‡æ¸¡å±‚ï¼šåœ¨å¯†é›†å—é—´é™ç»´å’Œä¸‹é‡‡æ ·
    - åˆ†ç±»å™¨ï¼šå…¨è¿æ¥å±‚è¾“å‡º100ç±»é¢„æµ‹
    
    âœ¨ ä¼˜åŠ¿ï¼š
    - å‚æ•°æ•ˆç‡é«˜ï¼šç‰¹å¾å¤ç”¨å‡å°‘å‚æ•°å†—ä½™
    - æ¢¯åº¦æµç•…ï¼šå¯†é›†è¿æ¥ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
    - ç‰¹å¾ä¸°å¯Œï¼šå¤šå°ºåº¦ç‰¹å¾èåˆ
    - è”é‚¦å‹å¥½ï¼šGroupNormé€‚é…å°batchåœºæ™¯
    """
    
    def __init__(self, num_classes=100, growth_rate=32, num_layers=[6, 12, 32, 24], 
                 use_attention=True, use_groupnorm=True, normalization='groupnorm'):
        """
        å¢å¼ºç‰ˆDenseNet_CIFAR100æ„é€ å‡½æ•°
        
        ğŸš€ æ¨¡å‹é…ç½®ï¼ˆæ›´æ·±æ›´å®½ç‰ˆæœ¬ï¼‰ï¼š
        - growth_rate: 24 -> 32 (æ›´å¼ºç‰¹å¾å­¦ä¹ èƒ½åŠ›)
        - num_layers: [6, 12, 24, 16] -> [6, 12, 32, 24] (æ›´æ·±ç½‘ç»œ)
        - use_attention: æ˜¯å¦ä½¿ç”¨ECAæ³¨æ„åŠ›æ¨¡å—
        - normalization: 'batchnorm', 'groupnorm', 'layernorm'
        - é’ˆå¯¹CIFAR-100çš„100ç±»ç²¾ç»†åˆ†ç±»ä¼˜åŒ–
        
        ğŸ“Š æ¨¡å‹é…ç½®é€‰é¡¹ï¼š
        - Standard: growth_rate=24, layers=[6,12,24,16] (DenseNet-121é£æ ¼)
        - Enhanced: growth_rate=32, layers=[6,12,32,24] (æ›´æ·±æ›´å®½ï¼Œæ˜¾å­˜éœ€æ±‚æ›´å¤§)
        - Lite: growth_rate=16, layers=[6,12,20,12] (è½»é‡çº§ç‰ˆæœ¬)
        """
        super(DenseNet_CIFAR100, self).__init__()
        
        self.growth_rate = growth_rate
        self.use_attention = use_attention
        self.normalization = normalization
        
        # é€‰æ‹©å½’ä¸€åŒ–æ–¹å¼
        def get_norm_layer(num_channels):
            if normalization == 'groupnorm' or use_groupnorm:
                groups = get_valid_group_count(num_channels)
                return nn.GroupNorm(groups, num_channels)
            elif normalization == 'layernorm':
                return nn.GroupNorm(1, num_channels)  # LayerNormç­‰ä»·äºGroupNorm(1, ...)
            else:  # batchnorm
                return nn.BatchNorm2d(num_channels)
        
        # åˆå§‹ç‰¹å¾æå–å±‚
        # å°†3é€šé“RGBå›¾åƒè½¬æ¢ä¸º64é€šé“ç‰¹å¾å›¾
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),
            get_norm_layer(64),
            nn.ReLU(inplace=True)
        )
        
        in_channels = 64  # å½“å‰é€šé“æ•°
        
        # æ„å»º4ä¸ªå¯†é›†å—å’Œ3ä¸ªè¿‡æ¸¡å±‚
        
        # ç¬¬ä¸€ä¸ªå¯†é›†å—ï¼š64 -> 64 + num_layers[0]*growth_rate é€šé“
        self.denseblock1 = DenseBlock(in_channels, growth_rate, num_layers[0], 
                                    use_attention=use_attention, 
                                    normalization=normalization)
        in_channels += num_layers[0] * growth_rate  # 64 + 6*32 = 256
        
        # ç¬¬ä¸€ä¸ªè¿‡æ¸¡å±‚ï¼šå‹ç¼©ä¸€åŠé€šé“æ•°ï¼Œç©ºé—´å°ºå¯¸å‡åŠ
        self.transition1 = self._make_transition_layer(in_channels, get_norm_layer)
        in_channels = in_channels // 2  # 256 -> 128
        
        # ç¬¬äºŒä¸ªå¯†é›†å—ï¼š128 -> 128 + num_layers[1]*growth_rate é€šé“  
        self.denseblock2 = DenseBlock(in_channels, growth_rate, num_layers[1], 
                                    use_attention=use_attention,
                                    normalization=normalization)
        in_channels += num_layers[1] * growth_rate  # 128 + 12*32 = 512
        
        # ç¬¬äºŒä¸ªè¿‡æ¸¡å±‚
        self.transition2 = self._make_transition_layer(in_channels, get_norm_layer)
        in_channels = in_channels // 2  # 512 -> 256
        
        # ç¬¬ä¸‰ä¸ªå¯†é›†å—ï¼š256 -> 256 + num_layers[2]*growth_rate é€šé“
        self.denseblock3 = DenseBlock(in_channels, growth_rate, num_layers[2], 
                                    use_attention=use_attention,
                                    normalization=normalization)
        in_channels += num_layers[2] * growth_rate  # 256 + 32*32 = 1280
        
        # ç¬¬ä¸‰ä¸ªè¿‡æ¸¡å±‚
        self.transition3 = self._make_transition_layer(in_channels, get_norm_layer)
        in_channels = in_channels // 2  # 1280 -> 640
        
        # ç¬¬å››ä¸ªå¯†é›†å—ï¼š640 -> 640 + num_layers[3]*growth_rate é€šé“
        self.denseblock4 = DenseBlock(in_channels, growth_rate, num_layers[3], 
                                    use_attention=use_attention,
                                    normalization=normalization)
        in_channels += num_layers[3] * growth_rate  # 640 + 24*32 = 1408
        
        # æœ€ç»ˆåˆ†ç±»å™¨ - ä¼˜åŒ–çš„åˆ†ç±»å™¨è®¾è®¡            
        self.classifier = nn.Sequential(
            get_norm_layer(in_channels),        # æœ€ç»ˆç‰¹å¾å½’ä¸€åŒ–
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1)),       # å…¨å±€å¹³å‡æ± åŒ–
            nn.Flatten(),                       # å±•å¹³ä¸ºå‘é‡
            nn.Dropout(0.3),                   # Dropouté˜²è¿‡æ‹Ÿåˆ
            nn.Linear(in_channels, 512),        # å¢åŠ ä¸­é—´å±‚æé«˜åˆ†ç±»èƒ½åŠ›
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(512, num_classes)         # æœ€ç»ˆè¾“å‡º100ç±»
        )
        
        # æƒé‡åˆå§‹åŒ– 
        self._initialize_weights()
        
        # æ‰“å°æ¨¡å‹é…ç½®ä¿¡æ¯
        print(f"ğŸ—ï¸ DenseNet_CIFAR100 é…ç½®:")
        print(f"   Growth Rate: {growth_rate}")
        print(f"   Layers: {num_layers}")
        print(f"   æœ€ç»ˆé€šé“æ•°: {in_channels}")
        print(f"   æ³¨æ„åŠ›æœºåˆ¶: {'âœ…' if use_attention else 'âŒ'}")
        print(f"   å½’ä¸€åŒ–æ–¹å¼: {normalization}")
    
    def _make_transition_layer(self, in_channels, get_norm_layer):
        """åˆ›å»ºè¿‡æ¸¡å±‚ï¼šå‹ç¼©é€šé“æ•°å¹¶ä¸‹é‡‡æ ·"""
        return nn.Sequential(
            get_norm_layer(in_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels // 2, 1, bias=False),
            nn.AvgPool2d(2, 2)
        )
            
        self.classifier = nn.Sequential(
            final_norm,                     # æœ€ç»ˆç‰¹å¾å½’ä¸€åŒ–
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1)),   # å…¨å±€å¹³å‡æ± åŒ–
            nn.Flatten(),                   # å±•å¹³ä¸ºå‘é‡
            nn.Dropout(0.3),               # Dropouté˜²è¿‡æ‹Ÿåˆ
            nn.Linear(in_channels, 512),    # å¢åŠ ä¸­é—´å±‚æé«˜åˆ†ç±»èƒ½åŠ›
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(512, num_classes)     # æœ€ç»ˆè¾“å‡º100ç±»
        )
        
        # æƒé‡åˆå§‹åŒ– - å…³é”®ä¿®å¤
        self._initialize_weights()
    
    def forward(self, x):
        # åˆå§‹ç‰¹å¾æå–ï¼š[B, 3, 32, 32] -> [B, 64, 32, 32]
        x = self.features(x)
        
        # ç¬¬ä¸€ä¸ªå¯†é›†å— + è¿‡æ¸¡å±‚ï¼š[B, 64, 32, 32] -> [B, 256, 16, 16]
        x = self.denseblock1(x)     # [B, 64, 32, 32] -> [B, 256, 32, 32]
        x = self.transition1(x)     # [B, 256, 32, 32] -> [B, 128, 16, 16]
        
        # ç¬¬äºŒä¸ªå¯†é›†å— + è¿‡æ¸¡å±‚ï¼š[B, 128, 16, 16] -> [B, 512, 8, 8]
        x = self.denseblock2(x)     # [B, 128, 16, 16] -> [B, 512, 16, 16]
        x = self.transition2(x)     # [B, 512, 16, 16] -> [B, 256, 8, 8]
        
        # ç¬¬ä¸‰ä¸ªå¯†é›†å— + è¿‡æ¸¡å±‚ï¼š[B, 256, 8, 8] -> [B, 1024, 4, 4]
        x = self.denseblock3(x)     # [B, 256, 8, 8] -> [B, 1024, 8, 8]
        x = self.transition3(x)     # [B, 1024, 8, 8] -> [B, 512, 4, 4]
        
        # ç¬¬å››ä¸ªå¯†é›†å—ï¼ˆæœ€åä¸€ä¸ªï¼‰ï¼š[B, 512, 4, 4] -> [B, 1024, 4, 4]
        x = self.denseblock4(x)
        
        # åˆ†ç±»å™¨ï¼š[B, 1024, 4, 4] -> [B, 100]
        x = self.classifier(x)
        
        # è¿”å›logæ¦‚ç‡åˆ†å¸ƒ
        return F.log_softmax(x, dim=1)
    
    def _initialize_weights(self):
        """
        DenseNetä¸“ç”¨æƒé‡åˆå§‹åŒ–
        ======================
        
        ä½¿ç”¨åˆé€‚çš„åˆå§‹åŒ–ç­–ç•¥ç¡®ä¿è®­ç»ƒç¨³å®šæ€§ã€‚
        è¿™æ˜¯è§£å†³å‡†ç¡®ç‡é—®é¢˜çš„å…³é”®éƒ¨åˆ†ã€‚
        """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                # å·ç§¯å±‚ä½¿ç”¨Kaimingåˆå§‹åŒ–
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                # BatchNormå±‚æƒé‡åˆå§‹åŒ–ä¸º1ï¼Œåç½®ä¸º0
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                # å…¨è¿æ¥å±‚ä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_normal_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0) 


def get_cifar100_model(model_type='resnet18_fed', **kwargs):
    """
    CIFAR-100æ¨¡å‹å·¥å‚å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
    =============================
    
    æ ¹æ®æŒ‡å®šç±»å‹è¿”å›ç›¸åº”çš„CIFAR-100æ¨¡å‹å®ä¾‹ï¼Œæ”¯æŒå¤šç§æ¨¡å‹é…ç½®ã€‚
    
    ğŸ—ï¸ å‚æ•°:
        model_type (str): æ¨¡å‹ç±»å‹
            - 'resnet18_fed': ResNet18è”é‚¦å­¦ä¹ ç‰ˆæœ¬ï¼ˆé»˜è®¤ï¼‰
            - 'efficientnet': EfficientNet-B3é£æ ¼æ¨¡å‹
            - 'densenet': DenseNetæ¨¡å‹ï¼ˆæ ‡å‡†é…ç½®ï¼‰
            - 'densenet_enhanced': DenseNetå¢å¼ºç‰ˆï¼ˆæ›´æ·±æ›´å®½ï¼‰
            - 'densenet_lite': DenseNetè½»é‡ç‰ˆ
        **kwargs: æ¨¡å‹ç‰¹å®šå‚æ•°
    
    ğŸš€ è¿”å›:
        nn.Module: å¯¹åº”çš„æ¨¡å‹å®ä¾‹
        
    ğŸ“ ä½¿ç”¨ç¤ºä¾‹:
        # åˆ›å»ºæ ‡å‡†DenseNetï¼ˆgrowth_rate=24ï¼‰
        model = get_cifar100_model('densenet')
        
        # åˆ›å»ºå¢å¼ºç‰ˆDenseNetï¼ˆæ›´æ·±æ›´å®½ï¼Œgrowth_rate=32ï¼‰
        model = get_cifar100_model('densenet_enhanced')
        
        # åˆ›å»ºå¸¦GroupNormçš„æ¨¡å‹ï¼ˆé€‚åˆè”é‚¦å­¦ä¹ ï¼‰
        model = get_cifar100_model('densenet', normalization='groupnorm')
        
        # åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„æ¨¡å‹
        model = get_cifar100_model('densenet', 
                                 growth_rate=40, 
                                 num_layers=[8,16,36,32],
                                 use_attention=True)
    """
    if model_type == 'resnet18_fed':
        return ResNet18_CIFAR100_Fed(**kwargs)
    elif model_type == 'resnet18_gn':
        # resnet18_gn æ˜¯ resnet18_fed çš„åˆ«åï¼Œå¼ºåˆ¶ä½¿ç”¨ GroupNorm
        kwargs['use_groupnorm'] = True
        return ResNet18_CIFAR100_Fed(**kwargs)
    elif model_type == 'efficientnet':
        return EfficientNet_CIFAR100(**kwargs)
    elif model_type == 'densenet':
        # æ ‡å‡†DenseNeté…ç½®ï¼ˆDenseNet-121é£æ ¼ï¼‰
        default_config = {
            'growth_rate': 24,
            'num_layers': [6, 12, 24, 16],
            'use_attention': True,
            'normalization': 'groupnorm'  # é»˜è®¤ä½¿ç”¨GroupNormï¼Œé€‚åˆè”é‚¦å­¦ä¹ 
        }
        default_config.update(kwargs)
        return DenseNet_CIFAR100(**default_config)
    elif model_type == 'densenet_enhanced':
        # å¢å¼ºç‰ˆDenseNeté…ç½®ï¼ˆæ›´æ·±æ›´å®½ï¼Œæ˜¾å­˜éœ€æ±‚æ›´å¤§ï¼‰
        enhanced_config = {
            'growth_rate': 32,
            'num_layers': [6, 12, 32, 24],
            'use_attention': True,
            'normalization': 'groupnorm'
        }
        enhanced_config.update(kwargs)
        return DenseNet_CIFAR100(**enhanced_config)
    elif model_type == 'densenet_lite':
        # è½»é‡ç‰ˆDenseNeté…ç½®ï¼ˆå‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«ï¼‰
        lite_config = {
            'growth_rate': 16,
            'num_layers': [6, 12, 20, 12],
            'use_attention': True,
            'normalization': 'groupnorm'
        }
        lite_config.update(kwargs)
        return DenseNet_CIFAR100(**lite_config)
    elif model_type == 'densenet_ultra':
        # è¶…å¼ºç‰ˆDenseNeté…ç½®ï¼ˆæœ€é«˜æ€§èƒ½ï¼Œæ˜¾å­˜éœ€æ±‚æœ€å¤§ï¼‰
        ultra_config = {
            'growth_rate': 40,
            'num_layers': [8, 16, 36, 32],
            'use_attention': True,
            'normalization': 'groupnorm'
        }
        ultra_config.update(kwargs)
        return DenseNet_CIFAR100(**ultra_config)
    else:
        raise ValueError(f"Unknown model type: {model_type}. "
                        f"Available types: ['resnet18_fed', 'resnet18_gn', 'efficientnet', 'densenet', 'densenet_enhanced', 'densenet_lite', 'densenet_ultra']")

def convert_model_normalization(model, target_norm='groupnorm', num_groups=8):
    """
    æ¨¡å‹å½’ä¸€åŒ–æ–¹å¼è½¬æ¢å‡½æ•°
    =====================
    
    å°†å·²æœ‰æ¨¡å‹çš„BatchNormè½¬æ¢ä¸ºGroupNormæˆ–LayerNormã€‚
    è¿™åœ¨è”é‚¦å­¦ä¹ çš„å°batchç¯å¢ƒä¸‹æ›´ç¨³å®šã€‚
    
    ğŸ”§ å‚æ•°:
        model: éœ€è¦è½¬æ¢çš„æ¨¡å‹ï¼ˆnn.Moduleå¯¹è±¡ï¼‰
        target_norm: ç›®æ ‡å½’ä¸€åŒ–æ–¹å¼ ('groupnorm', 'layernorm')
        num_groups: GroupNormçš„ç»„æ•°ï¼ˆé»˜è®¤8ï¼‰
    
    ğŸš€ è¿”å›:
        è½¬æ¢åçš„æ¨¡å‹
        
    ğŸ“ ä½¿ç”¨ç¤ºä¾‹:
        # å°†BatchNormæ¨¡å‹è½¬æ¢ä¸ºGroupNorm
        model = get_cifar100_model('densenet', normalization='batchnorm')
        model = convert_model_normalization(model, 'groupnorm')
        
        # è½¬æ¢ä¸ºLayerNorm
        model = convert_model_normalization(model, 'layernorm')
    """
    # åˆ›å»ºæ¨¡å‹çš„æ·±æ‹·è´ï¼Œé¿å…ä¿®æ”¹åŸæ¨¡å‹
    import copy
    converted_model = copy.deepcopy(model)
    
    if target_norm == 'groupnorm':
        converted_model = replace_bn_with_gn(converted_model, num_groups)
        print(f"âœ… æ¨¡å‹å·²è½¬æ¢ä¸ºGroupNormï¼ˆgroups={num_groups}ï¼‰")
    elif target_norm == 'layernorm':
        converted_model = replace_bn_with_ln(converted_model)
        print("âœ… æ¨¡å‹å·²è½¬æ¢ä¸ºLayerNorm")
    else:
        print(f"âŒ ä¸æ”¯æŒçš„å½’ä¸€åŒ–æ–¹å¼: {target_norm}")
        
    return converted_model


# æ¨¡å‹é…ç½®é¢„è®¾
CIFAR100_MODEL_CONFIGS = {
    'resnet18_fed_default': {
        'model_type': 'resnet18_fed',
        'use_groupnorm': True,
        'num_groups': 8,
        'use_se': True,
        'dropout_rate': 0.3
    },
    'resnet18_fed_heavy': {
        'model_type': 'resnet18_fed',
        'use_groupnorm': True,
        'num_groups': 16,
        'use_se': True,
        'dropout_rate': 0.4
    },
    'efficientnet_default': {
        'model_type': 'efficientnet',
        'dropout_rate': 0.3
    },
    'efficientnet_heavy': {
        'model_type': 'efficientnet',
        'dropout_rate': 0.4
    },
    'densenet_default': {
        'model_type': 'densenet',
        'growth_rate': 32
    },
    'densenet_compact': {
        'model_type': 'densenet',
        'growth_rate': 16,
        'num_layers': [6, 8, 12, 8]
    }
}


if __name__ == "__main__":
    """æµ‹è¯•æ¨¡å‹å®ä¾‹åŒ–å’Œå‰å‘ä¼ æ’­"""
    import torch
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥ï¼šæ¨¡æ‹ŸCIFAR-100æ•°æ®
    test_input = torch.randn(4, 3, 32, 32)  # batch_size=4, channels=3, height=32, width=32
    
    print("CIFAR-100æ¨¡å‹æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•ä¸»è¦æ¨¡å‹é…ç½®
    test_configs = ['resnet18_fed_default', 'efficientnet_default', 'densenet_default']
    
    for config_name in test_configs:
        print(f"\næµ‹è¯•é…ç½®: {config_name}")
        config = CIFAR100_MODEL_CONFIGS[config_name]
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = get_cifar100_model(**config)
        
        # è®¡ç®—æ¨¡å‹å‚æ•°é‡
        param_count = sum(p.numel() for p in model.parameters())
        
        # å‰å‘ä¼ æ’­æµ‹è¯•
        with torch.no_grad():
            output = model(test_input)
        
        print(f"  å‚æ•°é‡: {param_count:,}")
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")  # åº”è¯¥æ˜¯ [4, 100]
        print(f"  è¾“å‡ºèŒƒå›´: [{output.min():.4f}, {output.max():.4f}]")
        
        # éªŒè¯è¾“å‡ºæ˜¯logæ¦‚ç‡åˆ†å¸ƒ
        prob_sum = torch.exp(output).sum(dim=1)
        print(f"  æ¦‚ç‡å’Œ: {prob_sum.mean():.4f} (åº”è¯¥æ¥è¿‘1.0)")
    
    print(f"\næ‰€æœ‰CIFAR-100æ¨¡å‹æµ‹è¯•å®Œæˆï¼")
