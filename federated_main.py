#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6

# è®¾ç½®ç¼–ç 
import sys

# å¼ºåˆ¶è®¾ç½®UTF-8ç¼–ç 
if sys.platform.startswith('win'):
    import os
    os.system('chcp 65001 > nul')  # è®¾ç½®Windowsæ§åˆ¶å°ä¸ºUTF-8
    
# ç¡®ä¿printè¾“å‡ºä½¿ç”¨UTF-8ï¼Œå¹¶ç¦ç”¨ç¼“å†²
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8', line_buffering=True)
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8', line_buffering=True)

# å¼ºåˆ¶ç¦ç”¨Pythonè¾“å‡ºç¼“å†²
import os
os.environ['PYTHONUNBUFFERED'] = '1'

import os # ç”¨äºä¸æ“ä½œç³»ç»Ÿäº¤äº’ï¼Œä¾‹å¦‚è·¯å¾„æ“ä½œ
import copy # ç”¨äºæ·±æ‹·è´
import time # ç”¨äºè®¡æ—¶
import pickle # ç”¨äºåºåˆ—åŒ–å’Œååºåˆ—åŒ– Python å¯¹è±¡ (ä¾‹å¦‚ä¿å­˜è®­ç»ƒç»“æœ)
import numpy as np
from tqdm import tqdm # è¿›åº¦æ¡
import pandas as pd # ç”¨äºæ•°æ®å¤„ç†å’Œä¿å­˜CSV
import sys
from datetime import datetime

# è®¾ç½®è¾“å‡ºç¼–ç 
if hasattr(sys.stdout, 'reconfigure'):
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

import torch
from tensorboardX import SummaryWriter # ç”¨äº TensorBoard æ—¥å¿—è®°å½•ï¼Œå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹

from options import args_parser
from update import LocalUpdate, test_inference # ä» update.py å¯¼å…¥æœ¬åœ°æ›´æ–°ç±»å’Œæµ‹è¯•å‡½æ•°
from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar, CNNCifar100, ResNet18Fed, replace_bn_with_gn
from model_factory import get_model, get_recommended_model, list_available_models  # æ–°çš„æ¨¡å‹å·¥å‚
from utils import get_dataset, average_weights, exp_details, adaptive_federated_aggregation, calculate_diversity_scores # ä» utils.py å¯¼å…¥æ•°æ®åŠ è½½ã€æƒé‡å¹³å‡å’Œæ‰“å°å®éªŒç»†èŠ‚çš„å‡½æ•°

def print_training_details(epoch, global_round, num_users, frac, local_ep, local_bs, lr, device, model):
    """
    æ‰“å°è®­ç»ƒè¯¦ç»†ä¿¡æ¯
    
    Args:
        epoch: å½“å‰è½®æ¬¡
        global_round: å…¨å±€è½®æ•°
        num_users: å®¢æˆ·ç«¯æ€»æ•°
        frac: å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯æ¯”ä¾‹
        local_ep: æœ¬åœ°è®­ç»ƒè½®æ•°
        local_bs: æœ¬åœ°æ‰¹é‡å¤§å°
        lr: å­¦ä¹ ç‡
        device: è®¾å¤‡
        model: æ¨¡å‹
    """
    print(f"\nğŸŒŸ å¼€å§‹ç¬¬ {epoch+1}/{global_round} è½®è”é‚¦å­¦ä¹ ")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®: å®¢æˆ·ç«¯æ€»æ•°={num_users}, å‚ä¸æ¯”ä¾‹={frac:.1%}")
    print(f"ğŸ”§ æœ¬åœ°è®­ç»ƒ: è½®æ•°={local_ep}, æ‰¹é‡={local_bs}, å­¦ä¹ ç‡={lr}")
    print(f"ğŸ’» è®¾å¤‡: {device}")
    
    # è®¡ç®—æ¨¡å‹å‚æ•°æ€»æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ§  æ¨¡å‹å‚æ•°: æ€»è®¡{total_params:,}ä¸ª, å¯è®­ç»ƒ{trainable_params:,}ä¸ª")

def intelligent_client_selection(args, epoch, user_groups, client_ema_losses, EMA_ALPHA):
    """
    æ™ºèƒ½å®¢æˆ·ç«¯é€‰æ‹© - åŸºäºå†å²è¡¨ç°å’Œæ•°æ®åˆ†å¸ƒ
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        epoch: å½“å‰è½®æ¬¡
        user_groups: ç”¨æˆ·æ•°æ®åˆ†ç»„
        client_ema_losses: å®¢æˆ·ç«¯EMAæŸå¤±è®°å½•
        EMA_ALPHA: EMAå¹³æ»‘å› å­
        
    Returns:
        idxs_users: é€‰ä¸­çš„å®¢æˆ·ç«¯ç´¢å¼•
        client_ema_losses: æ›´æ–°åçš„EMAæŸå¤±è®°å½•
    """
    m = max(int(args.frac * args.num_users), 1)
    
    if epoch > 2:  # å‰å‡ è½®æ”¶é›†æ•°æ®
        # åŸºäºå†å²è¡¨ç°çš„æ™ºèƒ½å®¢æˆ·ç«¯é€‰æ‹©
        client_weights = []
        
        # è®¡ç®—ä¸€æ¬¡å…¨å±€æœ€å¤§æ•°æ®é‡ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤è®¡ç®—
        max_data_size = max(len(user_groups[i]) for i in range(args.num_users))
        
        for idx in range(args.num_users):
            # --- æ›´æ–°å®¢æˆ·ç«¯çš„EMAæŸå¤± ---
            last_loss = np.mean(args.client_history['losses'].get(idx, [1.0]))
            if idx not in client_ema_losses:
                client_ema_losses[idx] = last_loss
            else:
                client_ema_losses[idx] = EMA_ALPHA * last_loss + (1 - EMA_ALPHA) * client_ema_losses[idx]

            # ä½¿ç”¨EMAæŸå¤±æ¥è®¡ç®—å¾—åˆ†
            current_ema_loss = client_ema_losses[idx]
            loss_score = 1.0 / (1.0 + current_ema_loss) # ä½¿ç”¨å¹³æ»‘åçš„æŸå¤±
            
            # æ•°æ®é‡æƒé‡
            data_size = len(user_groups[idx])
            data_score = data_size / max_data_size
            
            # é¿å…è¿‡åº¦é€‰æ‹©åŒä¸€å®¢æˆ·ç«¯ - æ›´ç²¾ç¡®çš„é¢‘ç‡æƒ©ç½š
            # è®¡ç®—æœ€è¿‘å‡ è½®ä¸­è¯¥å®¢æˆ·ç«¯è¢«é€‰ä¸­çš„æ¬¡æ•°
            recent_window = min(6 * m, len(args.client_history['last_selected']))  # æœ€è¿‘6è½®çš„é€‰æ‹©
            recent_selections = args.client_history['last_selected'][-recent_window:] if recent_window > 0 else []
            frequency_penalty = 1.0 - (recent_selections.count(idx) * 0.15)  # é™ä½æƒ©ç½šå¼ºåº¦
            frequency_penalty = max(frequency_penalty, 0.2)  # æœ€å°ä¿æŒ20%æƒé‡
            
            # ç»¼åˆå¾—åˆ†
            combined_score = (0.5 * loss_score + 0.25 * data_score + 0.25 * frequency_penalty)
            client_weights.append(combined_score)
        
        # æ¦‚ç‡é€‰æ‹©ç¡®ä¿å¤šæ ·æ€§
        client_weights = np.array(client_weights)
        client_probs = client_weights / client_weights.sum()
        
        idxs_users = np.random.choice(range(args.num_users), m, replace=False, p=client_probs)
        
        # æ˜¾ç¤ºé€‰æ‹©è¯¦æƒ…ï¼ˆä»…åœ¨verboseæ¨¡å¼ä¸‹ï¼‰
        if args.verbose:
            selected_scores = [client_weights[idx] for idx in idxs_users]
            print(f'ğŸ§  æ™ºèƒ½é€‰æ‹©å®¢æˆ·ç«¯: {list(idxs_users)} (æƒé‡: {[f"{s:.3f}" for s in selected_scores]})')
        else:
            print(f'ğŸ§  æ™ºèƒ½é€‰æ‹©å®¢æˆ·ç«¯: {list(idxs_users)}')
    else:
        # å‰å‡ è½®éšæœºé€‰æ‹©æ”¶é›†åŸºå‡†æ•°æ®
        idxs_users = np.random.choice(range(args.num_users), m, replace=False)
        print(f'ğŸ² éšæœºé€‰æ‹©å®¢æˆ·ç«¯: {list(idxs_users)} (æ”¶é›†åŸºå‡†æ•°æ®)')
    
    return idxs_users, client_ema_losses


def update_client_history(args, idxs_users):
    """
    æ›´æ–°å®¢æˆ·ç«¯é€‰æ‹©å†å²è®°å½•
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        idxs_users: æœ¬è½®é€‰ä¸­çš„å®¢æˆ·ç«¯
    """
    m = max(int(args.frac * args.num_users), 1)
    
    # è®°å½•é€‰æ‹©çš„å®¢æˆ·ç«¯å†å²
    args.client_history['last_selected'].extend(idxs_users.tolist())
    args.client_history['round_selections'].append(idxs_users.tolist())  # æŒ‰è½®è®°å½•
    
    # ç»´æŠ¤åˆç†çš„å†å²çª—å£å¤§å°
    if len(args.client_history['last_selected']) > (m * 8): # ä¿æŒ8è½®çš„å†å²
        args.client_history['last_selected'] = args.client_history['last_selected'][-(m*8):]
    if len(args.client_history['round_selections']) > 8:  # ä¿æŒæœ€è¿‘8è½®çš„è½®çº§è®°å½•
        args.client_history['round_selections'] = args.client_history['round_selections'][-8:]


def perform_local_training(args, idxs_users, global_model, train_dataset, user_groups, epoch):
    """
    æ‰§è¡Œå®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        idxs_users: é€‰ä¸­çš„å®¢æˆ·ç«¯
        global_model: å…¨å±€æ¨¡å‹
        train_dataset: è®­ç»ƒæ•°æ®é›†
        user_groups: ç”¨æˆ·æ•°æ®åˆ†ç»„
        epoch: å½“å‰è½®æ¬¡
        
    Returns:
        local_weights: æœ¬åœ°æƒé‡åˆ—è¡¨
        local_losses: æœ¬åœ°æŸå¤±åˆ—è¡¨
        epoch_comm_cost: é€šä¿¡æˆæœ¬
    """
    local_weights, local_losses = [], []
    epoch_comm_cost = 0
    
    for idx in idxs_users:
        print(f'\n[CLIENT {idx}] å¼€å§‹æœ¬åœ°è®­ç»ƒ...')
        
        # åˆ›å»º LocalUpdate å®ä¾‹ï¼Œä¼ å…¥å®¢æˆ·ç«¯çš„æœ¬åœ°æ•°æ® (user_groups[idx])
        local_model = LocalUpdate(args=args, dataset=train_dataset,
                                  idxs=user_groups[idx], client_id=idx)
        # å®¢æˆ·ç«¯è¿›è¡Œæœ¬åœ°è®­ç»ƒ
        w, loss = local_model.update_weights(
            model=copy.deepcopy(global_model), global_round=epoch, 
            global_weights=global_model.state_dict() if epoch > 0 else None)
        local_weights.append(copy.deepcopy(w))
        local_losses.append(copy.deepcopy(loss))
        
        # è®¡ç®—é€šä¿¡æˆæœ¬
        client_comm_cost = sum(torch.numel(param) for param in w.values())
        epoch_comm_cost += client_comm_cost
        
        # æ›´æ–°å®¢æˆ·ç«¯æŸå¤±å†å²
        if idx not in args.client_history['losses']:
            args.client_history['losses'][idx] = []
        args.client_history['losses'][idx].append(loss)
        
        # ä¿æŒå†å²è®°å½•çª—å£å¤§å°
        if len(args.client_history['losses'][idx]) > 10:
            args.client_history['losses'][idx] = args.client_history['losses'][idx][-10:]
        
        print(f'[CLIENT {idx}] æœ¬åœ°è®­ç»ƒå®Œæˆï¼ŒæŸå¤±: {loss:.6f}ï¼Œä¸Šä¼ å‚æ•°é‡: {client_comm_cost:,}')
    
    return local_weights, local_losses, epoch_comm_cost


def select_clients_enhanced(num_users, frac, idxs_users=None):
    """
    é€‰æ‹©å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯ - å¢å¼ºç‰ˆ
    
    Args:
        num_users: å®¢æˆ·ç«¯æ€»æ•°
        frac: å‚ä¸æ¯”ä¾‹
        idxs_users: å¯é€‰çš„é¢„å®šä¹‰å®¢æˆ·ç«¯ç´¢å¼•
        
    Returns:
        selected_users: é€‰ä¸­çš„å®¢æˆ·ç«¯ç´¢å¼•åˆ—è¡¨
        m: å‚ä¸å®¢æˆ·ç«¯æ•°é‡
    """
    # è®¡ç®—å‚ä¸è®­ç»ƒçš„å®¢æˆ·ç«¯æ•°é‡
    m = max(int(frac * num_users), 1)
    
    if idxs_users is not None:
        selected_users = idxs_users
    else:
        selected_users = np.random.choice(range(num_users), m, replace=False)
    
    print(f"ğŸ‘¥ é€‰æ‹©å®¢æˆ·ç«¯: {selected_users} (å…±{len(selected_users)}ä¸ª)")
    return selected_users, m

def print_enhanced_communication_stats(epoch, selected_users, local_weights, enable_compression=False):
    """
    æ‰“å°é€šä¿¡ç»Ÿè®¡ä¿¡æ¯ - å¢å¼ºç‰ˆ
    
    Args:
        epoch: å½“å‰è½®æ¬¡
        selected_users: å‚ä¸çš„å®¢æˆ·ç«¯åˆ—è¡¨
        local_weights: æœ¬åœ°æ¨¡å‹æƒé‡åˆ—è¡¨
        enable_compression: æ˜¯å¦å¯ç”¨å‹ç¼©
    """
    print(f"\nğŸ“¡ ç¬¬{epoch+1}è½®é€šä¿¡ç»Ÿè®¡:")
    
    total_comm_cost = 0
    for i, idx in enumerate(selected_users):
        # è®¡ç®—å®¢æˆ·ç«¯æ¨¡å‹å‚æ•°æ•°é‡
        client_params = sum(torch.numel(param) for param in local_weights[i].values())
        total_comm_cost += client_params
        
        if enable_compression:
            print(f"[CLIENT {idx}] å‹ç¼©ä¼ è¾“: {client_params:,} å‚æ•°")
        else:
            print(f"[CLIENT {idx}] å¯†é›†ä¼ è¾“: {client_params:,} å‚æ•°")
    
    print(f"ğŸ“Š æœ¬è½®æ€»é€šä¿¡é‡: {total_comm_cost:,} å‚æ•°")
    return total_comm_cost

def print_enhanced_epoch_results(epoch, train_loss, train_accuracy, test_accuracy, ema_accuracy, comm_cost, best_ema_acc):
    """
    æ‰“å°è½®æ¬¡ç»“æœ - å¢å¼ºç‰ˆ
    
    Args:
        epoch: å½“å‰è½®æ¬¡
        train_loss: è®­ç»ƒæŸå¤±
        train_accuracy: è®­ç»ƒå‡†ç¡®ç‡  
        test_accuracy: æµ‹è¯•å‡†ç¡®ç‡
        ema_accuracy: EMAå¹³æ»‘å‡†ç¡®ç‡
        comm_cost: é€šä¿¡æˆæœ¬
        best_ema_acc: æœ€ä½³EMAå‡†ç¡®ç‡
    """
    print(f"\nğŸ“ˆ ç¬¬{epoch+1}è½®ç»“æœæ±‡æ€»:")
    print(f"   ğŸ”´ è®­ç»ƒæŸå¤±: {train_loss:.6f}")
    print(f"   ğŸŸ¢ è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.2f}%")
    print(f"   ğŸ”µ æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.2f}%")
    print(f"   ğŸ¯ EMAå¹³æ»‘å‡†ç¡®ç‡: {ema_accuracy:.2f}% (æœ€ä½³: {best_ema_acc:.2f}%)")
    print(f"   ğŸ“¡ é€šä¿¡æˆæœ¬: {comm_cost:,} å‚æ•°")
    print("=" * 80)

def save_enhanced_model_checkpoint(model, epoch, save_path, file_name):
    """
    ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ - å¢å¼ºç‰ˆ
    
    Args:
        model: æ¨¡å‹
        epoch: å½“å‰è½®æ¬¡
        save_path: ä¿å­˜è·¯å¾„
        file_name: æ–‡ä»¶å
    """
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    
    checkpoint_path = os.path.join(save_path, f'{file_name}_epoch_{epoch+1}.pth')
    torch.save(model.state_dict(), checkpoint_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {checkpoint_path}")

if __name__ == '__main__':
    start_time = time.time() # è®°å½•å¼€å§‹æ—¶é—´
    print(start_time)
    print('å°è¯•')

    # define paths
    path_project = os.path.abspath('..') # è·å–é¡¹ç›®ä¸Šçº§ç›®å½•çš„ç»å¯¹è·¯å¾„ (å¯èƒ½ç”¨äºä¿å­˜æ–‡ä»¶)
    logger = SummaryWriter('../logs') # åˆå§‹åŒ– TensorBoard çš„ SummaryWriterï¼Œæ—¥å¿—ä¿å­˜åœ¨ ../logs ç›®å½•ä¸‹

    args = args_parser() # è§£æå‘½ä»¤è¡Œå‚æ•°
    exp_details(args) # æ‰“å°å®éªŒé…ç½®è¯¦æƒ…
    '''
    åŠŸèƒ½ï¼šè°ƒç”¨ utils.py æ–‡ä»¶ä¸­çš„ exp_details(args) å‡½æ•°ï¼ŒæŠŠåˆšåˆšè§£æåˆ°çš„æ‰€æœ‰å®éªŒå‚æ•°è¯¦ç»†æ‰“å°å‡ºæ¥ã€‚
    æ„ä¹‰ï¼šæ–¹ä¾¿ä½ åœ¨æ§åˆ¶å°/æ—¥å¿—ä¸­ç¡®è®¤æœ¬æ¬¡å®éªŒçš„æ‰€æœ‰é…ç½®ï¼Œé¿å…å‚æ•°è®¾ç½®é”™è¯¯ï¼Œä¾¿äºåç»­å¤ç°å®éªŒå’Œè°ƒè¯•ã€‚
    '''

    # è®¾å¤‡æ£€æŸ¥å’Œè®¾ç½®
    print("æ£€æŸ¥CUDAæ”¯æŒ...")
    if args.gpu is not None and torch.cuda.is_available():
        try:
            # æµ‹è¯•CUDAæ˜¯å¦çœŸæ­£å¯ç”¨
            test_tensor = torch.randn(1, 1, 28, 28)
            test_tensor = test_tensor.cuda(int(args.gpu))
            print(f"âœ“ CUDAæµ‹è¯•æˆåŠŸï¼Œä½¿ç”¨GPU: {args.gpu}")
            torch.cuda.set_device(int(args.gpu))
            device = torch.device(f'cuda:{int(args.gpu)}')
            del test_tensor
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"âœ— GPU {args.gpu} ä¸å¯ç”¨: {str(e)}")
            print("åˆ‡æ¢åˆ°CPUæ¨¡å¼")
            device = torch.device('cpu')
    else:
        if args.gpu is not None:
            print(f"âœ— CUDAä¸å¯ç”¨ï¼Œä½†æŒ‡å®šäº†GPU {args.gpu}")
        else:
            print("æœªæŒ‡å®šGPU")
        print("ä½¿ç”¨CPUæ¨¡å¼")
        device = torch.device('cpu')
    
    print(f"æœ€ç»ˆè®¾å¤‡: {device}")

    # load dataset and user groups (åŠ è½½æ•°æ®é›†å’Œç”¨æˆ·æ•°æ®åˆ’åˆ†)
    train_dataset, test_dataset, user_groups = get_dataset(args)

    # BUILD MODEL (æ„å»ºå…¨å±€æ¨¡å‹ï¼Œæ”¯æŒåŸæœ‰æ¨¡å‹å’Œæ–°çš„ä¼˜åŒ–æ¨¡å‹)
    print(f"æ­£åœ¨æ„å»ºæ¨¡å‹: {args.model} for dataset: {args.dataset}")
    
    # é¦–å…ˆå°è¯•ä½¿ç”¨æ–°çš„ä¼˜åŒ–æ¨¡å‹
    try:
        if args.model in ['cnn']:
            # ä½¿ç”¨æ–°çš„æ ‡å‡†CNNæ¨¡å‹
            if args.dataset == 'mnist':
                global_model = get_model('mnist', 'cnn')  # CNN_MNIST
            elif args.dataset == 'cifar':
                global_model = get_model('cifar10', 'cnn')  # CNNCifar
            else:
                raise ValueError(f"æ ‡å‡†CNNä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
                
        elif args.model in ['optimized', 'cnn_optimized', 'cnn_opt', 'cnn_enhanced', 'optimized_gn']:
            # ä½¿ç”¨æ–°çš„ä¼˜åŒ–CNNæ¨¡å‹
            if args.dataset == 'mnist':
                if args.model == 'optimized_gn':
                    # ä½¿ç”¨GroupNormä¼˜åŒ–æ¨¡å‹ï¼Œé€‚åˆNon-IIDåœºæ™¯
                    global_model = get_model('mnist', 'optimized_gn')  # CNN_MNIST_Optimized_GN
                else:
                    global_model = get_model('mnist', 'optimized')  # CNN_MNIST_Optimized
            elif args.dataset == 'cifar':
                global_model = get_model('cifar10', 'cnn')  # CNNCifar (å¯¹äºCIFARä½¿ç”¨æ ‡å‡†CNNä½œä¸ºå¢å¼ºç‰ˆ)
            else:
                raise ValueError(f"ä¼˜åŒ–CNNç›®å‰ä»…æ”¯æŒMNISTå’ŒCIFARæ•°æ®é›†")
                
        elif args.model in ['resnet18', 'resnet18_fed', 'resnet', 'resnet_mini', 'resnet18_gn']:
            # ä½¿ç”¨æ–°çš„ResNet18Fedä¼˜åŒ–æ¨¡å‹
            if args.dataset == 'mnist':
                global_model = get_model('mnist', 'optimized')  # CNN_MNIST_Optimized
            elif args.dataset == 'cifar':
                # å¦‚æœæ˜¯GroupNormç‰ˆæœ¬ï¼Œä½¿ç”¨ç‰¹æ®Šå‚æ•°
                if args.model == 'resnet18_gn':
                    global_model = get_model('cifar10', 'resnet18_fed', use_groupnorm=True, num_groups=getattr(args, 'num_groups', 8))
                else:
                    global_model = get_model('cifar10', 'resnet18_fed')  # ResNet18_CIFAR10_Fed 
            elif args.dataset == 'cifar100':
                global_model = get_model('cifar100', 'resnet18_fed')  # ResNet18_CIFAR100_Fed
            else:
                raise ValueError(f"ResNet18ä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
                
        elif args.model in ['efficientnet', 'efficient']:
            # ä½¿ç”¨æ–°çš„EfficientNetä¼˜åŒ–æ¨¡å‹
            if args.dataset == 'cifar':
                global_model = get_model('cifar10', 'efficientnet')  # EfficientNet_CIFAR10
            elif args.dataset == 'cifar100':
                global_model = get_model('cifar100', 'efficientnet')  # EfficientNet_CIFAR100
            else:
                raise ValueError(f"EfficientNetä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
                
        elif args.model == 'densenet':
            # ä½¿ç”¨æ–°çš„DenseNetæ¨¡å‹ (ä»…CIFAR-100)
            if args.dataset == 'cifar100':
                # ä¼ é€’æ–°çš„å‚æ•°ç»™DenseNetæ¨¡å‹
                global_model = get_model('cifar100', 'densenet', 
                                        use_attention=bool(getattr(args, 'use_attention', 1)),
                                        use_groupnorm=bool(getattr(args, 'use_groupnorm', 1)))  # é»˜è®¤å¯ç”¨GroupNorm
            else:
                raise ValueError(f"DenseNetä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
                
        else:
            # ä½¿ç”¨æ¨èæ¨¡å‹æˆ–æŠ›å‡ºå¼‚å¸¸å°è¯•åŸæœ‰æ¨¡å‹
            raise ValueError("å°è¯•åŸæœ‰æ¨¡å‹")
            
        print(f"[SUCCESS] æˆåŠŸåŠ è½½ä¼˜åŒ–æ¨¡å‹: {global_model.__class__.__name__}")
        
    except Exception as e:
        print(f"[WARNING] ä¼˜åŒ–æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"[INFO] å›é€€åˆ°åŸæœ‰æ¨¡å‹...")
        
        # å›é€€åˆ°åŸæœ‰æ¨¡å‹æ„å»ºé€»è¾‘
        if args.model == 'cnn':
            # Convolutional neural netork
            if args.dataset == 'mnist':
                global_model = CNNMnist(args=args)
            elif args.dataset == 'fmnist':
                global_model = CNNFashion_Mnist(args=args)
            elif args.dataset == 'cifar':
                global_model = CNNCifar(args=args)
            elif args.dataset == 'cifar100':
                global_model = CNNCifar100(args=args)
        elif args.model == 'resnet':
            # ResNet for federated learning
            if args.dataset == 'cifar':
                global_model = ResNet18Fed(num_classes=args.num_classes)
                # ä½¿ç”¨GroupNormæ›¿æ¢BatchNormä»¥æé«˜è”é‚¦å­¦ä¹ æ€§èƒ½
                global_model = replace_bn_with_gn(global_model)
            elif args.dataset == 'cifar100':
                global_model = ResNet18Fed(num_classes=100)
                global_model = replace_bn_with_gn(global_model)
            else:
                print(f"ResNet not implemented for dataset {args.dataset}, using CNN instead")
                if args.dataset == 'mnist':
                    global_model = CNNMnist(args=args)
                elif args.dataset == 'fmnist':
                    global_model = CNNFashion_Mnist(args=args)
        elif args.model == 'mlp':
            # Multi-layer preceptron
            img_size = train_dataset[0][0].shape
            len_in = 1
            for x in img_size:
                len_in *= x
            global_model = MLP(dim_in=len_in, dim_hidden=64,
                               dim_out=args.num_classes)
        else:
            print(f"[ERROR] é”™è¯¯: ä¸æ”¯æŒçš„æ¨¡å‹ '{args.model}'")
            print("[INFO] æ”¯æŒçš„æ¨¡å‹:")
            print("   åŸæœ‰æ¨¡å‹: mlp, cnn, resnet")
            print("   ä¼˜åŒ–æ¨¡å‹: resnet18, efficientnet, densenet, cnn_optimized")
            exit('Error: unrecognized model')

    # Set the model to train and send it to device.
    global_model.to(device) # æ¨¡å‹ç§»è‡³è®¾å¤‡
    global_model.train() # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ (å°½ç®¡å…¨å±€æ¨¡å‹ä¸»è¦é€šè¿‡èšåˆæ›´æ–°ï¼Œä½†åˆå§‹çŠ¶æ€å’Œåˆ†å‘ç»™å®¢æˆ·ç«¯æ—¶åº”ä¸ºè®­ç»ƒæ¨¡å¼)
    print(global_model) # æ‰“å°æ¨¡å‹ç»“æ„

    # copy weights (è·å–å…¨å±€æ¨¡å‹çš„åˆå§‹æƒé‡)
    global_weights = global_model.state_dict() # state_dict() è¿”å›åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„å­—å…¸
    '''
    è·å–å½“å‰å…¨å±€æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼ˆæƒé‡å’Œåç½®ï¼‰å¹¶ä¿å­˜ä¸ºä¸€ä¸ªå­—å…¸
    global_model.state_dict() ä¼šè¿”å›ä¸€ä¸ªåŒ…å«æ¨¡å‹æ‰€æœ‰å¯å­¦ä¹ å‚æ•°(å¦‚æƒé‡ã€åç½®)çš„æœ‰åºå­—å…¸(OrderedDict)ã€‚
    è¿™æ ·åšçš„ç›®çš„æ˜¯ä¿å­˜å…¨å±€æ¨¡å‹çš„å½“å‰å‚æ•°çŠ¶æ€ï¼Œåç»­å¯ä»¥ç”¨è¿™äº›å‚æ•°åˆ†å‘ç»™å„ä¸ªå®¢æˆ·ç«¯ï¼Œæˆ–è€…åœ¨èšåˆåæ›´æ–°å…¨å±€æ¨¡å‹ã€‚
    åœ¨è”é‚¦å­¦ä¹ ä¸­ï¼Œæ¯ä¸€è½®å¼€å§‹æ—¶ï¼Œå®¢æˆ·ç«¯ä¼šæ‹¿åˆ°å…¨å±€æ¨¡å‹çš„å‚æ•°è¿›è¡Œæœ¬åœ°è®­ç»ƒï¼Œè®­ç»ƒåå†ä¸Šä¼ å‚æ•°ï¼Œæœ€åæœåŠ¡å™¨ç«¯èšåˆè¿™äº›å‚æ•°ï¼Œæ›´æ–°å…¨å±€æ¨¡å‹ã€‚
    ä¾‹å¦‚
    OrderedDict([
    ('fc1.weight', tensor([[ 0.01, -0.02, ...], [...], ...])),   # ç¬¬ä¸€å±‚å…¨è¿æ¥çš„æƒé‡
    ('fc1.bias', tensor([0.0, 0.0, ...])),                      # ç¬¬ä¸€å±‚å…¨è¿æ¥çš„åç½®
    ('fc2.weight', tensor([[...], [...], ...])),                # ç¬¬äºŒå±‚å…¨è¿æ¥çš„æƒé‡
    ('fc2.bias', tensor([0.0, 0.0, ...]))                       # ç¬¬äºŒå±‚å…¨è¿æ¥çš„åç½®
    ])
    '''

    # Training (è”é‚¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹)
    train_loss, local_test_accuracy = [], [] # è®°å½•æ¯è½®çš„å¹³å‡è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡
    communication_cost = []  # è®°å½•æ¯è½®çš„é€šä¿¡å¼€é”€
    val_acc_list, net_list = [], [] # (æœªä½¿ç”¨)
    cv_loss, cv_acc = [], [] # (æœªä½¿ç”¨)
    print_every = 1 # æ¯è½®éƒ½æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ï¼Œæä¾›æ›´è¯¦ç»†çš„è¾“å‡º
    
    # æ—©åœæœºåˆ¶å‚æ•°
    best_val_acc = 0.0
    patience = args.stopping_rounds  # ä½¿ç”¨å‚æ•°ä¸­çš„early stopping rounds
    patience_counter = 0
    best_global_weights = None

    # --- ä¿®æ”¹å¼€å§‹ï¼šåˆå§‹åŒ–å†å²è®°å½• ---
    history = {
        'epoch': [],
        'test_accuracy': [],
        'ema_accuracy': [],      # æ·»åŠ EMAå¹³æ»‘å‡†ç¡®ç‡è®°å½•
        'avg_train_loss': [],
        'learning_rate': [],
        'communication_cost': []  # æ·»åŠ é€šä¿¡å¼€é”€è®°å½•
    }
    # --- ä¿®æ”¹ç»“æŸ ---

    # åˆå§‹åŒ–å®¢æˆ·ç«¯çŠ¶æ€è·Ÿè¸ª
    if not hasattr(args, 'client_history'):
        args.client_history = {
            'losses': {},           # æ¯ä¸ªå®¢æˆ·ç«¯çš„å†å²æŸå¤±
            'last_selected': [],    # æ‰€æœ‰å†å²é€‰æ‹©çš„å®¢æˆ·ç«¯ï¼ˆç”¨äºç®€å•é¢‘ç‡ç»Ÿè®¡ï¼‰
            'round_selections': [], # æŒ‰è½®è®°å½•çš„é€‰æ‹©å†å²ï¼ˆç”¨äºæ›´ç²¾ç¡®çš„è½®çº§åˆ†æï¼‰
        }
    '''
    args å¯¹è±¡å°±åƒä¸€ä¸ªè´¯ç©¿æ•´ä¸ªç¨‹åºçš„â€œå…¨å±€ä¿¡æ¯æ¿â€
    æˆ‘ä»¬å¯ä»¥æ–¹ä¾¿åœ°ç”¨å®ƒæ¥å­˜å‚¨å’Œä¼ é€’åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€äº§ç”Ÿçš„ä¿¡æ¯ï¼ˆæ¯”å¦‚æ¯ä¸ªå®¢æˆ·ç«¯çš„å†å²æŸå¤±ï¼‰
    è€Œä¸éœ€è¦æŠŠå®ƒå®šä¹‰æˆä¸€ä¸ªå›ºå®šçš„å‘½ä»¤è¡Œå‚æ•°
    '''

    # --- åœ¨è®­ç»ƒå¾ªç¯ for epoch in pbar: ä¹‹å‰ï¼Œåˆå§‹åŒ–ä¸€ä¸ªæ–°çš„å­—å…¸æ¥å­˜å‚¨å¹³æ»‘æŸå¤± ---
    client_ema_losses = {}
    EMA_ALPHA = 0.3 # å¹³æ»‘å› å­ï¼Œå¯ä»¥ä½œä¸ºè¶…å‚æ•°è°ƒæ•´
    
    # EMAå¹³æ»‘å‡†ç¡®ç‡ç›¸å…³å˜é‡ï¼ˆä¸æ®‹å·®è”é‚¦å­¦ä¹ å¯¹é½ï¼‰
    ema_alpha = 0.4  # EMAå¹³æ»‘å› å­ï¼Œç”¨äºå¹³æ»‘å‡†ç¡®ç‡çš„å˜åŒ–ï¼Œé¿å…å› å•è½®æ³¢åŠ¨å¯¼è‡´è¯¯åˆ¤
    ema_acc = None   # å¹³æ»‘åçš„å‡†ç¡®ç‡ï¼Œåˆå§‹å€¼ä¸ºNoneï¼Œåç»­ä¼šåŠ¨æ€æ›´æ–°
    best_ema_acc = -1.0 # è®°å½•æœ€ä½³çš„å¹³æ»‘å‡†ç¡®ç‡ï¼Œç”¨äºåˆ¤æ–­æ¨¡å‹æ€§èƒ½æ˜¯å¦æå‡
    
    # SWA (Stochastic Weight Averaging) æ”¯æŒ
    swa_model = None
    swa_n = 0
    swa_start_epoch = getattr(args, 'swa_start', 150)
    enable_swa = getattr(args, 'enable_swa', 0) == 1
    
    if enable_swa:
        print(f"SWAå¯ç”¨: å°†åœ¨ç¬¬{swa_start_epoch}è½®å¼€å§‹æ”¶é›†æ¨¡å‹æƒé‡")
    
    # æ‰“å°å¯ç”¨çš„é«˜çº§ç‰¹æ€§
    print("\n=== å¯ç”¨çš„é«˜çº§ç‰¹æ€§ ===")
    print(f"ğŸ“Š EMAå¹³æ»‘å‡†ç¡®ç‡: å¯ç”¨ (Î±={ema_alpha})")
    print(f"ğŸ¯ Label Smoothing: {'å¯ç”¨' if getattr(args, 'criterion', 'cross_entropy') == 'label_smoothing' else 'ç¦ç”¨'}")
    if getattr(args, 'criterion', 'cross_entropy') == 'label_smoothing':
        print(f"   å¹³æ»‘å‚æ•°: {getattr(args, 'smoothing', 0.1)}")
    print(f"ğŸ”„ SWA: {'å¯ç”¨' if enable_swa else 'ç¦ç”¨'}")
    if enable_swa:
        print(f"   å¼€å§‹è½®æ¬¡: {swa_start_epoch}")
    print(f"ğŸ”€ CutMix: {'å¯ç”¨' if getattr(args, 'enable_cutmix', 0) == 1 else 'ç¦ç”¨'}")
    if getattr(args, 'enable_cutmix', 0) == 1:
        print(f"   Î±={getattr(args, 'cutmix_alpha', 1.0)}, æ¦‚ç‡={getattr(args, 'cutmix_prob', 0.5)}")
    print(f"ğŸ¨ Mixup: {'å¯ç”¨' if getattr(args, 'enable_mixup', 0) == 1 else 'ç¦ç”¨'}")
    if getattr(args, 'enable_mixup', 0) == 1:
        print(f"   Î±={getattr(args, 'mixup_alpha', 0.4)}")
    print(f"ğŸ§  çŸ¥è¯†è’¸é¦: {'å¯ç”¨' if getattr(args, 'enable_knowledge_distillation', 0) == 1 else 'ç¦ç”¨'}")
    if getattr(args, 'enable_knowledge_distillation', 0) == 1:
        print(f"   æ¸©åº¦={getattr(args, 'distill_temperature', 3.0)}, Î±={getattr(args, 'distill_alpha', 0.3)}")
    print(f"ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦: {getattr(args, 'lr_scheduler', 'none')}")
    if getattr(args, 'lr_scheduler', 'none') == 'cosine':
        print(f"   T_max={getattr(args, 'cosine_t_max', 50)}")
    print(f"ğŸ¤ èšåˆç­–ç•¥: {getattr(args, 'adaptive_aggregation', 'weighted_avg')}")
    if getattr(args, 'iid', 1) == 0 and getattr(args, 'mu', 0.0) > 0:
        print(f"ğŸ”§ FedProx: å¯ç”¨ (Î¼={getattr(args, 'mu', 0.01)})")
    print("=" * 30)
    # --- åˆå§‹åŒ–ç»“æŸ ---

    for epoch in range(args.epochs): # è”é‚¦å­¦ä¹ ä¸»å¾ªç¯
        # ä½¿ç”¨æ¨¡å—åŒ–å‡½æ•°æ‰“å°è®­ç»ƒè¯¦ç»†ä¿¡æ¯
        print_training_details(epoch, args.epochs, args.num_users, args.frac, 
                             args.local_ep, args.local_bs, args.lr, device, global_model)

        global_model.train() # ç¡®ä¿å…¨å±€æ¨¡å‹åœ¨åˆ†å‘ç»™å®¢æˆ·ç«¯å‰å¤„äºè®­ç»ƒæ¨¡å¼
        
        # æ™ºèƒ½å®¢æˆ·ç«¯é€‰æ‹©
        idxs_users, client_ema_losses = intelligent_client_selection(
            args, epoch, user_groups, client_ema_losses, EMA_ALPHA)
        
        # æ›´æ–°å®¢æˆ·ç«¯å†å²è®°å½•
        update_client_history(args, idxs_users)
        
        # æ‰§è¡Œæœ¬åœ°è®­ç»ƒ
        local_weights, local_losses, epoch_comm_cost = perform_local_training(
            args, idxs_users, global_model, train_dataset, user_groups, epoch)
        '''
        å¦‚æœä½ é€‰æ‹©äº† CNNMnist,é‚£ä¹ˆ:
        å…¨å±€çš„ global_model å°±æ˜¯ä¸€ä¸ª CNNMnist å®ä¾‹ã€‚
        æ¯ä¸ªç”¨æˆ·æœ¬åœ°è®­ç»ƒæ—¶ï¼Œä¹Ÿæ˜¯ç”¨ copy.deepcopy(global_model)ï¼Œå³æ¯ä¸ªå®¢æˆ·ç«¯æ‹¿åˆ°çš„éƒ½æ˜¯å’Œå…¨å±€æ¨¡å‹ä¸€æ¨¡ä¸€æ ·çš„ CNNMnist
        æ¯è½®é€šä¿¡æ—¶ï¼Œå®¢æˆ·ç«¯ä¼šåœ¨è‡ªå·±çš„æ•°æ®ä¸Šè®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Œç„¶åæŠŠæ›´æ–°åçš„å‚æ•°ä¸Šä¼ ï¼Œæœ€åæœåŠ¡å™¨ç«¯èšåˆè¿™äº›å‚æ•°ï¼Œæ›´æ–°å…¨å±€çš„ CNNMnist æ¨¡å‹ã€‚
        æ•´ä¸ªè”é‚¦å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ç»“æ„å§‹ç»ˆä¿æŒä¸€è‡´ï¼Œåªæ˜¯å‚æ•°åœ¨ä¸æ–­æ›´æ–°ã€‚
        '''
        # é€‰æ‹©å‚ä¸æœ¬è½®è®­ç»ƒçš„å®¢æˆ·ç«¯
        m = max(int(args.frac * args.num_users), 1) # è®¡ç®—å‚ä¸å®¢æˆ·ç«¯æ•°é‡ (è‡³å°‘ä¸º1)
                                                    # args.frac æ˜¯å‚ä¸æ¯”ä¾‹ï¼Œargs.num_users æ˜¯æ€»å®¢æˆ·ç«¯æ•°
        
        # æ”¹è¿›çš„å®¢æˆ·ç«¯é€‰æ‹©ç­–ç•¥
        if epoch > 2:  # å‰å‡ è½®æ”¶é›†æ•°æ®
            # åŸºäºå†å²è¡¨ç°çš„æ™ºèƒ½å®¢æˆ·ç«¯é€‰æ‹©
            client_weights = []
            
            # è®¡ç®—ä¸€æ¬¡å…¨å±€æœ€å¤§æ•°æ®é‡ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤è®¡ç®—
            max_data_size = max(len(user_groups[i]) for i in range(args.num_users))
            
            for idx in range(args.num_users):
                # --- æ›´æ–°å®¢æˆ·ç«¯çš„EMAæŸå¤± ---
                last_loss = np.mean(args.client_history['losses'].get(idx, [1.0]))
                if idx not in client_ema_losses:
                    client_ema_losses[idx] = last_loss
                else:
                    client_ema_losses[idx] = EMA_ALPHA * last_loss + (1 - EMA_ALPHA) * client_ema_losses[idx]

                # ä½¿ç”¨EMAæŸå¤±æ¥è®¡ç®—å¾—åˆ†
                current_ema_loss = client_ema_losses[idx]
                loss_score = 1.0 / (1.0 + current_ema_loss) # ä½¿ç”¨å¹³æ»‘åçš„æŸå¤±
                
                # æ•°æ®é‡æƒé‡
                data_size = len(user_groups[idx])
                data_score = data_size / max_data_size
                
                # é¿å…è¿‡åº¦é€‰æ‹©åŒä¸€å®¢æˆ·ç«¯ - æ›´ç²¾ç¡®çš„é¢‘ç‡æƒ©ç½š
                # è®¡ç®—æœ€è¿‘å‡ è½®ä¸­è¯¥å®¢æˆ·ç«¯è¢«é€‰ä¸­çš„æ¬¡æ•°
                recent_window = min(6 * m, len(args.client_history['last_selected']))  # æœ€è¿‘6è½®çš„é€‰æ‹©
                recent_selections = args.client_history['last_selected'][-recent_window:] if recent_window > 0 else []
                frequency_penalty = 1.0 - (recent_selections.count(idx) * 0.15)  # é™ä½æƒ©ç½šå¼ºåº¦
                frequency_penalty = max(frequency_penalty, 0.2)  # æœ€å°ä¿æŒ20%æƒé‡
                
                # ç»¼åˆå¾—åˆ†
                combined_score = (0.5 * loss_score + 0.25 * data_score + 0.25 * frequency_penalty)
                client_weights.append(combined_score)
            
            # æ¦‚ç‡é€‰æ‹©ç¡®ä¿å¤šæ ·æ€§
            client_weights = np.array(client_weights)
            client_probs = client_weights / client_weights.sum()
            
            idxs_users = np.random.choice(range(args.num_users), m, replace=False, p=client_probs)
            
            # æ˜¾ç¤ºé€‰æ‹©è¯¦æƒ…ï¼ˆä»…åœ¨verboseæ¨¡å¼ä¸‹ï¼‰
            if args.verbose:
                selected_scores = [client_weights[idx] for idx in idxs_users]
                print(f'ğŸ§  æ™ºèƒ½é€‰æ‹©å®¢æˆ·ç«¯: {list(idxs_users)} (æƒé‡: {[f"{s:.3f}" for s in selected_scores]})')
            else:
                print(f'ğŸ§  æ™ºèƒ½é€‰æ‹©å®¢æˆ·ç«¯: {list(idxs_users)}')
        else:
            # å‰å‡ è½®éšæœºé€‰æ‹©æ”¶é›†åŸºå‡†æ•°æ®
            idxs_users = np.random.choice(range(args.num_users), m, replace=False)
            print(f'ğŸ² éšæœºé€‰æ‹©å®¢æˆ·ç«¯: {list(idxs_users)} (æ”¶é›†åŸºå‡†æ•°æ®)')
        
        # è®°å½•é€‰æ‹©çš„å®¢æˆ·ç«¯å†å²
        args.client_history['last_selected'].extend(idxs_users.tolist())
        args.client_history['round_selections'].append(idxs_users.tolist())  # æŒ‰è½®è®°å½•
        
        # update global weights (èšåˆå®¢æˆ·ç«¯æƒé‡ï¼Œæ›´æ–°å…¨å±€æ¨¡å‹)
        lens = [len(user_groups[idx]) for idx in idxs_users]
        
        # ä½¿ç”¨è‡ªé€‚åº”èšåˆç­–ç•¥ï¼ˆNon-IIDåœºæ™¯ï¼‰
        if hasattr(args, 'iid') and args.iid == 0:
            # Non-IIDåœºæ™¯ä½¿ç”¨è‡ªé€‚åº”èšåˆ
            client_data_sizes = lens
            client_losses_vals = local_losses
            
            aggregation_method = getattr(args, 'adaptive_aggregation', 'loss_aware')
            
            # å¦‚æœä½¿ç”¨diversity_awareèšåˆï¼Œè®¡ç®—å¤šæ ·æ€§åˆ†æ•°
            diversity_scores = None
            if aggregation_method == 'diversity_aware':
                diversity_scores = calculate_diversity_scores(local_weights, client_data_sizes)
                print(f"ğŸ§® è®¡ç®—å¤šæ ·æ€§åˆ†æ•°: {[f'{score:.4f}' for score in diversity_scores]}")
            
            global_weights = adaptive_federated_aggregation(
                local_weights, 
                client_data_sizes, 
                client_losses_vals,
                aggregation_method=aggregation_method,
                diversity_scores=diversity_scores
            )
            print(f"ğŸ“Š ä½¿ç”¨è‡ªé€‚åº”èšåˆç­–ç•¥ ({aggregation_method})")
        else:
            # IIDåœºæ™¯ä½¿ç”¨æ ‡å‡†èšåˆ
            global_weights = average_weights(local_weights, lens) # è°ƒç”¨ utils.py ä¸­çš„ average_weights å‡½æ•°
            print(f"ğŸ“Š ä½¿ç”¨æ ‡å‡†èšåˆç­–ç•¥")
        '''è¿™ä¸€æ­¥ä¼šæŠŠæ‰€æœ‰æœ¬åœ°æƒé‡(å¦‚w2, w5, w7, w1, w9)åšå¹³å‡,å¾—åˆ°æ–°çš„å…¨å±€æƒé‡'''

        # update global model with new weights
        global_model.load_state_dict(global_weights) # å°†èšåˆåçš„å¹³å‡æƒé‡åŠ è½½åˆ°å…¨å±€æ¨¡å‹ä¸­
        
        # SWAæƒé‡æ”¶é›†
        if enable_swa and epoch >= swa_start_epoch:
            if swa_model is None:
                # åˆå§‹åŒ–SWAæ¨¡å‹
                swa_model = copy.deepcopy(global_model.state_dict())
                swa_n = 1
                print(f"ğŸ”„ SWA: å¼€å§‹æ”¶é›†æƒé‡ (è½®æ¬¡ {epoch+1})")
            else:
                # æ›´æ–°SWAæƒé‡: Î¸_SWA = (Î¸_SWA * n + Î¸_current) / (n + 1)
                swa_n += 1
                for key in swa_model.keys():
                    swa_model[key] = (swa_model[key] * (swa_n - 1) + global_weights[key]) / swa_n
                print(f"ğŸ”„ SWA: æ›´æ–°æƒé‡ (æ”¶é›†äº† {swa_n} è½®)")

        
        loss_avg = sum(local_losses) / len(local_losses) # è®¡ç®—æœ¬è½®æ‰€æœ‰å‚ä¸å®¢æˆ·ç«¯çš„å¹³å‡æœ¬åœ°æŸå¤±
        train_loss.append(loss_avg) # è®°å½•

        # --- æ¯è½®æ€§èƒ½è¯„ä¼° ---
        print(f'\nğŸ§ª è¯„ä¼°è½®æ¬¡ {epoch+1} çš„å…¨å±€æ¨¡å‹æ€§èƒ½...', flush=True)
        list_acc, list_loss = [], []
        global_model.eval()
        for c in range(args.num_users):
            local_model = LocalUpdate(args=args, dataset=train_dataset,
                                      idxs=user_groups[c], client_id=c)
            acc, loss = local_model.inference(model=global_model)
            list_acc.append(acc)
            list_loss.append(loss)
        local_test_accuracy.append(sum(list_acc)/len(list_acc))

        current_acc_for_stopping = local_test_accuracy[-1]
        
        # ä½¿ç”¨æ¨¡å—åŒ–å‡½æ•°æ‰“å°é€šä¿¡ç»Ÿè®¡
        epoch_comm_cost = print_enhanced_communication_stats(epoch, idxs_users, local_weights, enable_compression=False)
        
        # è®°å½•æœ¬è½®é€šä¿¡å¼€é”€  
        communication_cost.append(epoch_comm_cost)
        
        # æ›´æ–°EMAå¹³æ»‘å‡†ç¡®ç‡ï¼ˆä¸æ®‹å·®è”é‚¦å­¦ä¹ å¯¹é½ï¼‰
        if ema_acc is None:
            ema_acc = current_acc_for_stopping
        else:
            ema_acc = ema_alpha * current_acc_for_stopping + (1 - ema_alpha) * ema_acc

        # ä½¿ç”¨æ¨¡å—åŒ–å‡½æ•°æ‰“å°è½®æ¬¡ç»“æœ
        print_enhanced_epoch_results(epoch, loss_avg, current_acc_for_stopping*100, 
                                   current_acc_for_stopping*100, ema_acc*100, 
                                   epoch_comm_cost, best_ema_acc*100)
        
        # æ—©åœæœºåˆ¶æ£€æŸ¥ï¼ˆä½¿ç”¨EMAå¹³æ»‘å‡†ç¡®ç‡ï¼‰
        eps = 1e-4  # å°é˜ˆå€¼é˜²æ­¢æŠ–åŠ¨
        if ema_acc > best_ema_acc + eps:
            best_ema_acc = ema_acc
            patience_counter = 0
            best_global_weights = copy.deepcopy(global_model.state_dict())
            print(f'âœ… æ–°çš„æœ€ä½³EMAå¹³æ»‘å‡†ç¡®ç‡: {100*best_ema_acc:.2f}%')
        else:
            patience_counter += 1
            print(f'âš ï¸ EMAå¹³æ»‘å‡†ç¡®ç‡æœªæ”¹å–„. è€å¿ƒå€¼: {patience_counter}/{patience}')

        # å®šæœŸä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆæ¯10è½®ä¿å­˜ä¸€æ¬¡ï¼‰
        if (epoch+1) % 10 == 0:
            save_enhanced_model_checkpoint(global_model, epoch, './save/', f'{args.dataset}_{args.model}_fedavg')

        # print global training loss after every 'print_every' rounds
        if (epoch+1) % print_every == 0:
            print(f'\nğŸ“Š {epoch+1} è½®è®­ç»ƒç»Ÿè®¡:')
            print(f'ğŸ“ˆ å¹³å‡è®­ç»ƒæŸå¤±: {np.mean(np.array(train_loss)):.6f}')
            print(f'ğŸ¯ è®­ç»ƒå‡†ç¡®ç‡: {100*local_test_accuracy[-1]:.2f}%\n')

        # --- ä¿®æ”¹å¼€å§‹ï¼šè®°å½•æœ¬è½®æ¬¡çš„æŒ‡æ ‡ ---
        # é‡æ–°è®¡ç®—å½“å‰å­¦ä¹ ç‡ä»¥ä¾¿è®°å½•
        import math
        current_lr = args.lr
        if args.lr_scheduler == 'cosine':
            total_rounds = args.epochs
            min_lr = args.lr * 0.05
            warmup_rounds = min(5, total_rounds // 10)
            if epoch < warmup_rounds:
                current_lr = args.lr * (epoch + 1) / warmup_rounds
            else:
                effective_round = epoch - warmup_rounds
                effective_total = total_rounds - warmup_rounds
                cosine_factor = 0.5 * (1 + math.cos(math.pi * effective_round / effective_total))
                current_lr = min_lr + (args.lr - min_lr) * cosine_factor

        loss_avg = np.mean(np.array(train_loss))
        history['epoch'].append(epoch + 1)
        history['test_accuracy'].append(local_test_accuracy[-1])
        history['ema_accuracy'].append(ema_acc)  # è®°å½•EMAå¹³æ»‘å‡†ç¡®ç‡
        history['avg_train_loss'].append(loss_avg)
        history['learning_rate'].append(current_lr)
        history['communication_cost'].append(epoch_comm_cost)  # æ·»åŠ é€šä¿¡å¼€é”€è®°å½•
        # --- ä¿®æ”¹ç»“æŸ ---
            
        # æ—©åœæ£€æŸ¥
        if patience_counter >= patience:
            print(f'Early stopping triggered after {epoch+1} global rounds')
            if best_global_weights is not None:
                global_model.load_state_dict(best_global_weights)
                print("Loaded best model weights for final testing.")
            break

    # --- MODIFICATION START: Final Reporting ---
    # åº”ç”¨SWAæƒé‡ (å¦‚æœå¯ç”¨)
    final_model_name = "æœ€ä½³EMAæ¨¡å‹"
    if enable_swa and swa_model is not None:
        print(f"\nğŸ”„ åº”ç”¨SWAæƒé‡ (æ”¶é›†äº† {swa_n} è½®)...")
        # é¦–å…ˆè¯„ä¼°å½“å‰æœ€ä½³æ¨¡å‹
        if best_global_weights is not None:
            global_model.load_state_dict(best_global_weights)
        current_test_acc, _ = test_inference(args, global_model, test_dataset)
        
        # ç„¶åè¯„ä¼°SWAæ¨¡å‹
        global_model.load_state_dict(swa_model)
        swa_test_acc, _ = test_inference(args, global_model, test_dataset)
        
        print(f"ğŸ” æœ€ä½³EMAæ¨¡å‹å‡†ç¡®ç‡: {current_test_acc*100:.2f}%")
        print(f"ğŸ” SWAæ¨¡å‹å‡†ç¡®ç‡: {swa_test_acc*100:.2f}%")
        
        # é€‰æ‹©æ›´å¥½çš„æ¨¡å‹
        if swa_test_acc > current_test_acc:
            print("âœ… SWAæ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œä½¿ç”¨SWAæƒé‡")
            final_model_name = "SWAæ¨¡å‹"
        else:
            print("âœ… æœ€ä½³EMAæ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œä¿æŒåŸæƒé‡")
            if best_global_weights is not None:
                global_model.load_state_dict(best_global_weights)
    else:
        if best_global_weights is not None:
            global_model.load_state_dict(best_global_weights)
    
    print(f"\nè¯„ä¼°æœ€ç»ˆæ¨¡å‹æ€§èƒ½ ({final_model_name})...")
    # æ³¨æ„ï¼šæ­¤æ—¶çš„global_modelå·²ç»æ˜¯é€‰æ‹©çš„æœ€ä½³æ¨¡å‹
    test_acc, test_loss = test_inference(args, global_model, test_dataset)

    print("ä½¿ç”¨æœ€ä½³æ¨¡å‹è¯„ä¼°å¹³å‡æœ¬åœ°æµ‹è¯•æ€§èƒ½...")
    list_acc_best_model = []
    global_model.eval()
    for c in range(args.num_users):
        local_model = LocalUpdate(args=args, dataset=train_dataset,
                                  idxs=user_groups[c], client_id=c)
        acc, _ = local_model.inference(model=global_model)
        list_acc_best_model.append(acc)
    avg_local_test_acc_best_model = sum(list_acc_best_model) / len(list_acc_best_model)

    # è®­ç»ƒå®Œæˆï¼Œæ‰“å°æœ€ç»ˆæ€»ç»“
    total_training_time = time.time()-start_time
    print(f"\nğŸ‰ è”é‚¦å­¦ä¹ è®­ç»ƒå®Œæˆ!")
    print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_training_time:.2f}ç§’")
    print(f"ğŸ”„ å®é™…è®­ç»ƒè½®æ•°: {epoch+1}/{args.epochs}")
    print(f"ğŸ“¡ æ€»é€šä¿¡æˆæœ¬: {sum(communication_cost):,} å‚æ•°")
    
    # æœ€ç»ˆæ¨¡å‹è¯„ä¼°
    print(f"\nğŸ æœ€ç»ˆç»“æœ:")
    print("|---- Avg Local Test Accuracy (Best Model): {:.2f}%".format(100*avg_local_test_acc_best_model))
    print("|---- Global Test Accuracy (Best Model): {:.2f}%".format(100*test_acc))
    print("|---- Best EMA Smoothed Accuracy: {:.2f}%".format(100*best_ema_acc))
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_enhanced_model_checkpoint(global_model, epoch, './save/', f'{args.dataset}_{args.model}_fedavg_final')

    # --- ä¿®æ”¹å¼€å§‹ï¼šå°†å†å²è®°å½•ä¿å­˜ä¸ºCSVæ–‡ä»¶ ---
    # åˆ›å»ºæ—¶é—´æˆ³å‘½åçš„æ–‡ä»¶å¤¹
    current_time = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = os.path.join('./save/logs', current_time)
    os.makedirs(log_dir, exist_ok=True)
    
    # CSVæ–‡ä»¶åä¸æ–‡ä»¶å¤¹åä¿æŒä¸€è‡´ï¼ˆéƒ½ä½¿ç”¨æ—¶é—´æˆ³ï¼‰
    log_filename = f'{current_time}.csv'
    log_path = os.path.join(log_dir, log_filename)
    
    # ä¿å­˜å®éªŒè¯¦æƒ…åˆ°åŒä¸€æ–‡ä»¶å¤¹
    iid_str = 'iid' if args.iid else 'noniid'
    details_content = f"""å®éªŒæ—¶é—´: {current_time}
å®éªŒç±»å‹: Federated Learning
æ•°æ®é›†: {args.dataset.upper()}
æ¨¡å‹: {args.model.upper()}
è®­ç»ƒè½®æ•°: {args.epochs}
æ•°æ®åˆ†å¸ƒ: {iid_str.upper()}
å­¦ä¹ ç‡: {args.lr}
æœ¬åœ°è®­ç»ƒè½®æ•°: {args.local_ep}
å‚ä¸å®¢æˆ·ç«¯æ•°: {args.num_users}
å‚ä¸æ¯”ä¾‹: {args.frac}
"""
    details_path = os.path.join(log_dir, 'experiment_details.txt')
    with open(details_path, 'w', encoding='utf-8') as f:
        f.write(details_content)
    
    df = pd.DataFrame(history)
    df.to_csv(log_path, index=False)
    print(f"ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {log_path}")
    print(f"ğŸ“‹ å®éªŒè¯¦æƒ…å·²ä¿å­˜åˆ°: {details_path}")
    
    # è‡ªåŠ¨ç”Ÿæˆå›¾åƒ
    try:
        from visualize_results import plot_single_experiment
        plots_dir = './save/plots'
        plot_result = plot_single_experiment(log_path, plots_dir)
        if plot_result:
            print(f"ğŸ“Š å®éªŒå›¾åƒå·²è‡ªåŠ¨ç”Ÿæˆåˆ°: {plot_result}")
        else:
            print("âš ï¸ å›¾åƒç”Ÿæˆå¤±è´¥")
    except Exception as e:
        print(f"âš ï¸ è‡ªåŠ¨ç”Ÿæˆå›¾åƒæ—¶å‡ºé”™: {e}")
        print("ğŸ’¡ ä½ å¯ä»¥æ‰‹åŠ¨è¿è¡Œ: python visualize_results.py --single " + log_path)
    # --- ä¿®æ”¹ç»“æŸ ---

    # ä¿å­˜ç»“æœ
    import pickle
    save_dir = './save/objects'
    os.makedirs(save_dir, exist_ok=True)

    file_name = './save/objects/federated_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}].pkl'.\
        format(args.dataset, args.model, args.epochs, args.frac, args.iid,
                args.local_ep, args.local_bs)

    with open(file_name, 'wb') as f:
        pickle.dump([train_loss, local_test_accuracy, communication_cost], f) # MODIFIED

    print(f'è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {file_name}')
    # --- MODIFICATION END ---