#!/usr/bin/env python
# -*- coding: utf-8 -*-
# æ®‹å·®è”é‚¦å­¦ä¹ å·¥å…·å‡½æ•° - ç®€åŒ–ç‰ˆ

import torch
import copy
import time
import math
import numpy as np
from torch import nn
from torch.utils.data import DataLoader, Dataset
from torch.optim.lr_scheduler import CosineAnnealingLR
from sampling import DatasetSplit

# è®¾ç½®cuDNNé€‰é¡¹ä»¥è§£å†³ç®—æ³•é€‰æ‹©é—®é¢˜
torch.backends.cudnn.enabled = True
torch.backends.cudnn.benchmark = True  # å¯ç”¨benchmarkä»¥è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•
torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§ç®—æ³•è·å¾—æ›´å¥½æ€§èƒ½


def calculate_diversity_scores_residual(local_residuals, client_data_sizes, server_model_template, args=None):
    """
    è®¡ç®—å®¢æˆ·ç«¯æ®‹å·®çš„å¤šæ ·æ€§åˆ†æ•° - æ®‹å·®è”é‚¦å­¦ä¹ ç‰ˆæœ¬
    åŸºäºæ®‹å·®æƒé‡å·®å¼‚å’Œæ•°æ®åˆ†å¸ƒä¸å¹³è¡¡ç¨‹åº¦
    æ”¯æŒæ‰“åŒ…æ ¼å¼å’Œå¯†é›†æ ¼å¼çš„æ®‹å·®æ•°æ®
    """
    num_clients = len(local_residuals)
    diversity_scores = []
    #num_clients å­˜å‚¨å‚ä¸æœ¬è½®çš„å®¢æˆ·ç«¯æ€»æ•°
    #diversity_scores æ˜¯ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œåç»­ä¼šç”¨æ¥å­˜æ”¾æˆ‘ä»¬ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯è®¡ç®—å‡ºçš„æœ€ç»ˆå¤šæ ·æ€§åˆ†æ•°

    # æ£€æŸ¥ä¸Šè¡Œé“¾è·¯æ˜¯å¦å‹ç¼©
    is_compressed = (args is not None and args.uplink_compression != 'none')

    # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯ç›¸å¯¹äºå…¶ä»–å®¢æˆ·ç«¯çš„æ®‹å·®æƒé‡å·®å¼‚
    for i in range(num_clients):
        total_distance = 0.0
        #åœ¨ä¸ºå®¢æˆ·ç«¯ i è®¡ç®—åˆ†æ•°ä¹‹å‰ï¼Œå…ˆå°†å®ƒçš„â€œæ€»è·ç¦»â€(total_distance)æ¸…é›¶ã€‚
        #è¿™ä¸ªå˜é‡å°†ç”¨æ¥ç´¯åŠ å®ƒä¸å…¶ä»–æ‰€æœ‰å®¢æˆ·ç«¯çš„å·®å¼‚ç¨‹åº¦ã€‚
        weight_count = 0
        
        for j in range(num_clients):
            if i != j:
                # è®¡ç®—ä¸¤ä¸ªæ®‹å·®æƒé‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
                distance = 0.0
                
                # æ ¹æ®å‹ç¼©çŠ¶æ€å†³å®šæ˜¯å¦è§£åŒ…
                if is_compressed and server_model_template is not None:
                    # å‹ç¼©æ ¼å¼ï¼Œéœ€è¦è§£åŒ…
                    dense_residual_i = unpack_sparse_residual(local_residuals[i], template=server_model_template)
                    dense_residual_j = unpack_sparse_residual(local_residuals[j], template=server_model_template)
                else:
                    # æœªå‹ç¼©æ ¼å¼ï¼Œlocal_residuals å·²ç»æ˜¯å¯†é›†çš„
                    dense_residual_i = local_residuals[i]
                    dense_residual_j = local_residuals[j]
                
                for key in dense_residual_i.keys():
                    if key in dense_residual_j:
                        w1 = dense_residual_i[key].flatten().float()  # ç¡®ä¿æ˜¯æµ®ç‚¹å‹
                        w2 = dense_residual_j[key].flatten().float()  # ç¡®ä¿æ˜¯æµ®ç‚¹å‹
                        '''
                        .flatten() å°†æŸä¸€å±‚çš„æ‰€æœ‰å‚æ•°ï¼ˆæ— è®ºåŸæ¥æ˜¯çŸ©é˜µè¿˜æ˜¯æ›´é«˜ç»´çš„å¼ é‡ï¼‰â€œå‹å¹³â€æˆä¸€ä¸ªé•¿é•¿çš„ä¸€ç»´å‘é‡
                        .float() ç¡®ä¿æ•°æ®ç±»å‹æ˜¯æµ®ç‚¹æ•°ï¼Œä¾¿äºåç»­è®¡ç®—
                        ç°åœ¨ w1 å’Œ w2 åˆ†åˆ«ä»£è¡¨äº†å®¢æˆ·ç«¯ i å’Œ j åœ¨åŒä¸€å±‚ä¸Šçš„æ›´æ–°å‘é‡
                        '''

                        # è®¡ç®—ä½™å¼¦è·ç¦» (1 - cosine_similarity)
                        norm1 = torch.norm(w1)
                        norm2 = torch.norm(w2)
                        '''
                        norm1 å’Œ norm2 æ˜¯å‘é‡çš„é•¿åº¦ï¼ˆæ¨¡ï¼‰
                        '''
                        
                        if norm1 > 0 and norm2 > 0:
                            cosine_sim = torch.dot(w1, w2) / (norm1 * norm2)#torch.dot(w1, w2) æ˜¯å‘é‡çš„ç‚¹ç§¯
                            cosine_distance = 1.0 - cosine_sim.item()
                            '''
                            1.0 - cosine_sim å°†ç›¸ä¼¼åº¦è½¬æ¢æˆäº†è·ç¦»
                            æ–¹å‘è¶Šä¸€è‡´ï¼ˆç›¸ä¼¼åº¦æ¥è¿‘1ï¼‰ï¼Œè·ç¦»å°±è¶Šæ¥è¿‘0
                            æ–¹å‘è¶Šç›¸åï¼ˆç›¸ä¼¼åº¦æ¥è¿‘-1ï¼‰ï¼Œè·ç¦»å°±è¶Šæ¥è¿‘2ã€‚
                            '''
                            distance += cosine_distance
                            weight_count += 1
                            #å½“ for key ... è¿™ä¸ªæœ€å†…å±‚å¾ªç¯å…¨éƒ¨æ‰§è¡Œå®Œæ¯•åï¼Œweight_count çš„å€¼å·²ç»ä¸å†æ˜¯0äº†
                            #å®ƒç­‰äºå®¢æˆ·ç«¯ i å’Œ j ä¹‹é—´å…±åŒæ‹¥æœ‰çš„ã€è¢«æˆåŠŸæ¯”è¾ƒçš„æ€»å±‚æ•°
                
                if weight_count > 0:
                    total_distance += distance / weight_count
                    #total_distance å°±ä»£è¡¨äº†å®¢æˆ·ç«¯ i ä¸å…¶ä»–æ‰€æœ‰å®¢æˆ·ç«¯çš„å¹³å‡è·ç¦»ä¹‹å’Œ
        
        #è®¡ç®—å¹³å‡è·ç¦»
        avg_distance = total_distance / max(1, num_clients - 1)
        
        # ç»“åˆæ•°æ®é‡ä¸å¹³è¡¡å› å­
        total_samples = sum(client_data_sizes)
        data_imbalance = abs(client_data_sizes[i] / total_samples - 1.0 / num_clients)
        '''
        1.0 / num_clients æ˜¯åœ¨æ•°æ®å®Œå…¨å‡åŒ€åˆ†å¸ƒæ—¶ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯åº”å çš„æ•°æ®æ¯”ä¾‹
        client_data_sizes[i] / total_samples æ˜¯å®¢æˆ·ç«¯ i å®é™…çš„æ•°æ®å æ¯”
        è¿™ä¸¤è€…ä¹‹å·®çš„ç»å¯¹å€¼ï¼Œå°±è¡¡é‡äº†å®¢æˆ·ç«¯ i çš„æ•°æ®é‡åç¦»â€œå¹³å‡æ°´å¹³â€çš„ç¨‹åº¦
        æ•°æ®é‡ç‰¹åˆ«å¤šæˆ–ç‰¹åˆ«å°‘çš„å®¢æˆ·ç«¯ï¼Œè¿™ä¸ªå€¼éƒ½ä¼šæ¯”è¾ƒå¤§
        '''
        
        # ç»¼åˆå¤šæ ·æ€§åˆ†æ•° (æ®‹å·®å·®å¼‚ + æ•°æ®ä¸å¹³è¡¡)
        diversity_score = 0.7 * avg_distance + 0.3 * data_imbalance
        diversity_scores.append(diversity_score)
    
    return diversity_scores

class DatasetSplit(Dataset):
    """å°†æ•°æ®é›†åˆ†å‰²ç»™ä¸åŒå®¢æˆ·ç«¯çš„ç±»"""
    
    def __init__(self, dataset, idxs):
        self.dataset = dataset
        self.idxs = [int(i) for i in idxs]

    def __len__(self):
        return len(self.idxs)

    def __getitem__(self, item):
        image, label = self.dataset[self.idxs[item]]
        # ä½¿ç”¨clone().detach()ä»£æ›¿torch.tensor()æ¥é¿å…è­¦å‘Š
        if isinstance(image, torch.Tensor):
            image = image.clone().detach()
        else:
            image = torch.tensor(image)
            
        if isinstance(label, torch.Tensor):
            label = label.clone().detach()
        else:
            label = torch.tensor(label)
            
        return image, label

class LocalUpdateResidual(object):
    """æ®‹å·®è”é‚¦å­¦ä¹ çš„å®¢æˆ·ç«¯æœ¬åœ°æ›´æ–°ç±»"""
    
    def __init__(self, args, dataset, idxs, client_id=None):
        self.args = args
        self.client_id = client_id if client_id is not None else "Unknown"
        self.trainloader, self.validloader, self.testloader = self.train_val_test(
            dataset, list(idxs))
        
        # è®¾å¤‡è®¾ç½®
        try:
            gpu_id = int(args.gpu) if args.gpu is not None else -1
            self.device = 'cuda' if gpu_id >= 0 and torch.cuda.is_available() else 'cpu'
        except (ValueError, TypeError):
            self.device = 'cpu'
            
        self.criterion = nn.CrossEntropyLoss().to(self.device)
    
    def train_val_test(self, dataset, idxs):
        """å°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†"""
        idxs_train = idxs[:int(0.8*len(idxs))]
        idxs_val = idxs[int(0.8*len(idxs)):int(0.9*len(idxs))]
        idxs_test = idxs[int(0.9*len(idxs)):]

        trainloader = DataLoader(DatasetSplit(dataset, idxs_train),
                                 batch_size=self.args.local_bs, shuffle=True)
        validloader = DataLoader(DatasetSplit(dataset, idxs_val),
                                 batch_size=int(len(idxs_val)/10), shuffle=False)
        testloader = DataLoader(DatasetSplit(dataset, idxs_test),
                                batch_size=int(len(idxs_test)/10), shuffle=False)
        return trainloader, validloader, testloader

    def _get_model_features(self, model, x):
        """ä»æ¨¡å‹ä¸­æå–ç‰¹å¾ï¼Œæ”¯æŒä¸åŒçš„æ¨¡å‹ç»“æ„"""
        if hasattr(model, 'forward_features') and hasattr(model, 'classifier'):
            # æ–°çš„æ¨¡å‹ç»“æ„ï¼Œæ”¯æŒç‰¹å¾æå–
            features = model.forward_features(x)
            return features, model.classifier
        elif hasattr(model, 'features') and hasattr(model, 'classifier'):
            # ç±»ä¼¼VGGç»“æ„ï¼Œæœ‰featureså’Œclassifier
            features = model.features(x)
            features = features.view(features.size(0), -1)  # flatten
            return features, model.classifier
        elif hasattr(model, 'conv_layers') and hasattr(model, 'fc'):
            # è‡ªå®šä¹‰CNNç»“æ„
            features = x
            for layer in model.conv_layers:
                features = layer(features)
            # å‡è®¾æœ‰avgpool
            if hasattr(model, 'avgpool'):
                features = model.avgpool(features)
            features = features.view(features.size(0), -1)  # flatten
            return features, model.fc
        else:
            # ç®€å•ç»“æ„ï¼Œå°è¯•æ‰¾åˆ°æœ€åçš„å…¨è¿æ¥å±‚
            if hasattr(model, 'fc'):
                # é€šè¿‡forwardåˆ°fcå±‚ä¹‹å‰è·å–ç‰¹å¾
                features = model.forward_features(x) if hasattr(model, 'forward_features') else None
                if features is None:
                    # å¦‚æœæ²¡æœ‰forward_featuresæ–¹æ³•ï¼Œä½¿ç”¨å®Œæ•´çš„forward
                    return None, None
                return features, model.fc
            elif hasattr(model, 'classifier'):
                features = model.forward_features(x) if hasattr(model, 'forward_features') else None
                if features is None:
                    return None, None
                return features, model.classifier
            else:
                return None, None

    def update_weights(self, model, global_round, global_weights=None):
        """æ›´æ–°æ¨¡å‹æƒé‡"""
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()
        epoch_loss = []

        # è®¾ç½®ä¼˜åŒ–å™¨
        trainable_params = list(model.parameters())

        if self.args.optimizer == 'sgd':
            optimizer = torch.optim.SGD(trainable_params, lr=self.args.lr,
                                        momentum=getattr(self.args, 'momentum', 0.5), 
                                        weight_decay=getattr(self.args, 'weight_decay', 1e-4))
        elif self.args.optimizer == 'adam':
            optimizer = torch.optim.Adam(trainable_params, lr=self.args.lr,
                                         betas=(getattr(self.args, 'adam_beta1', 0.9), 
                                               getattr(self.args, 'adam_beta2', 0.999)),
                                         eps=getattr(self.args, 'adam_eps', 1e-8),
                                         weight_decay=getattr(self.args, 'weight_decay', 1e-4))
        elif self.args.optimizer == 'adamw':
            optimizer = torch.optim.AdamW(model.parameters(), lr=self.args.lr,
                                          betas=(getattr(self.args, 'adam_beta1', 0.9), 
                                                getattr(self.args, 'adam_beta2', 0.999)),
                                          eps=getattr(self.args, 'adam_eps', 1e-8),
                                          weight_decay=getattr(self.args, 'weight_decay', 0.01))
        else:
            # é»˜è®¤ä½¿ç”¨SGD
            optimizer = torch.optim.SGD(model.parameters(), lr=self.args.lr,
                                        momentum=0.5, weight_decay=1e-4)

        # å­¦ä¹ ç‡è°ƒåº¦
        lr_scheduler_type = getattr(self.args, 'lr_scheduler', 'none')
        
        if lr_scheduler_type == 'cosine':
            # æ”¹è¿›çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒæ•´
            total_rounds = getattr(self.args, 'epochs', 50)
            min_lr = self.args.lr * 0.05  # é™ä½æœ€å°å­¦ä¹ ç‡ï¼Œå¢å¼ºåæœŸå¾®è°ƒ
            
            # ä½¿ç”¨warmup + cosineç­–ç•¥
            warmup_rounds = min(5, total_rounds // 10)  # å‰10%è½®æ¬¡è¿›è¡Œwarmup
            if global_round < warmup_rounds:
                # Warmupé˜¶æ®µï¼šçº¿æ€§å¢é•¿åˆ°ç›®æ ‡å­¦ä¹ ç‡
                current_lr = self.args.lr * (global_round + 1) / warmup_rounds
            else:
                # Cosineé€€ç«é˜¶æ®µ
                effective_round = global_round - warmup_rounds
                effective_total = total_rounds - warmup_rounds
                cosine_factor = 0.5 * (1 + math.cos(math.pi * effective_round / effective_total))
                current_lr = min_lr + (self.args.lr - min_lr) * cosine_factor
        elif lr_scheduler_type == 'step':
            # æ­¥é•¿è°ƒåº¦å™¨
            step_size = getattr(self.args, 'lr_step_size', 20)
            gamma = getattr(self.args, 'lr_gamma', 0.1)
            current_lr = self.args.lr * (gamma ** (global_round // step_size))
        elif lr_scheduler_type == 'exp':
            # æŒ‡æ•°è¡°å‡è°ƒåº¦å™¨
            gamma = getattr(self.args, 'lr_gamma', 0.95)
            current_lr = self.args.lr * (gamma ** global_round)
        else:
            # å›ºå®šå­¦ä¹ ç‡æˆ–æ— è°ƒåº¦å™¨
            current_lr = self.args.lr
        
        # åº”ç”¨å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            param_group['lr'] = current_lr
        
        print(f"[CLIENT {self.client_id}] è½®æ¬¡ {global_round}: å­¦ä¹ ç‡ = {current_lr:.6f}")

        for iter_epoch in range(self.args.local_ep):
            batch_loss = []
            for batch_idx, (images, labels) in enumerate(self.trainloader):
                images, labels = images.to(self.device), labels.to(self.device)

                model.zero_grad()
                if self.personalization_layer is not None:
                    self.personalization_layer.zero_grad()
                
                log_probs = model(images)
                
                # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«NaN
                if torch.isnan(log_probs).any():
                    print(f"è­¦å‘Š: æ¨¡å‹è¾“å‡ºåŒ…å«NaNå€¼ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
                
                # è®¡ç®—æ ‡å‡†äº¤å‰ç†µæŸå¤±
                ce_loss = self.criterion(log_probs, labels)
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
                if torch.isnan(ce_loss):
                    print(f"è­¦å‘Š: äº¤å‰ç†µæŸå¤±ä¸ºNaNï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
                
                # Non-IIDåœºæ™¯ä¸‹æ·»åŠ çŸ¥è¯†è’¸é¦ç­–ç•¥
                total_loss = ce_loss
                if (hasattr(self.args, 'iid') and self.args.iid == 0 and global_round > 0 
                    and getattr(self.args, 'enable_knowledge_distillation', 1) == 1
                    and global_weights is not None):
                    try:
                        # --- ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨å…¨å±€æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼ˆevalæ¨¡å¼ï¼‰ ---
                        # é¿å…é‡å¤åˆ›å»ºæ•™å¸ˆæ¨¡å‹ï¼Œæé«˜æ•ˆç‡
                        with torch.no_grad():
                            # åˆ›å»ºæ•™å¸ˆæ¨¡å‹çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå¤ç”¨å½“å‰modelç»“æ„
                            teacher_logits = None
                            
                            # ä¸´æ—¶ä¿å­˜å½“å‰æ¨¡å‹çŠ¶æ€
                            current_state = {k: v.clone() for k, v in model.state_dict().items()}
                            
                            # ä¸´æ—¶åŠ è½½å…¨å±€æƒé‡åˆ°å½“å‰æ¨¡å‹
                            temp_global_weights = {k: v.to(self.device) for k, v in global_weights.items()}
                            model.load_state_dict(temp_global_weights)
                            model.eval()
                            
                            # è·å–æ•™å¸ˆè¾“å‡º
                            teacher_logits = model(images)
                            
                            # æ¢å¤å­¦ç”Ÿæ¨¡å‹çŠ¶æ€
                            model.load_state_dict(current_state)
                            model.train()
                        
                        # æ£€æŸ¥teacherè¾“å‡ºæ˜¯å¦æœ‰æ•ˆ
                        if teacher_logits is not None and not torch.isnan(teacher_logits).any() and not torch.isinf(teacher_logits).any():
                            # è®¡ç®—çŸ¥è¯†è’¸é¦æŸå¤±
                            T = getattr(self.args, 'distill_temperature', 3.0)
                            alpha = getattr(self.args, 'distill_alpha', 0.3)
                            
                            student_soft = torch.log_softmax(log_probs / T, dim=1)
                            teacher_soft = torch.softmax(teacher_logits / T, dim=1)
                            distill_loss = torch.nn.functional.kl_div(student_soft, teacher_soft, reduction='batchmean') * (T * T)
                            
                            # æ£€æŸ¥è’¸é¦æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                            if not torch.isnan(distill_loss) and not torch.isinf(distill_loss):
                                total_loss = (1 - alpha) * ce_loss + alpha * distill_loss
                    except Exception as e:
                        print(f"çŸ¥è¯†è’¸é¦è®¡ç®—å‡ºé”™ï¼Œä½¿ç”¨æ ‡å‡†æŸå¤±: {str(e)}")
                        total_loss = ce_loss
                
                loss = total_loss
                
                # æœ€ç»ˆæ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
                if torch.isnan(loss):
                    print(f"è­¦å‘Š: æœ€ç»ˆæŸå¤±ä¸ºNaNï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
                
                # --- MODIFICATION START: Conditionally apply FedProx ---
                # åªæœ‰åœ¨Non-IIDåœºæ™¯ä¸‹ï¼Œå¹¶ä¸”mu>0æ—¶ï¼Œæ‰åº”ç”¨FedProxè¿‘ç«¯é¡¹
                if getattr(self.args, 'iid', 1) == 0 and getattr(self.args, 'mu', 0.0) > 0 and global_weights is not None:
                    prox_term = 0.0
                    # global_weights æ˜¯æœ¬è½®å¼€å§‹æ—¶çš„å…¨å±€æ¨¡å‹æƒé‡
                    for name, param in model.named_parameters():
                        if name in global_weights:
                            # ç¡®ä¿å…¨å±€å‚æ•°å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                            # global_weights[name] æ˜¯å…¨å±€æ¨¡å‹çš„å‚æ•°
                            # param æ˜¯å½“å‰å®¢æˆ·ç«¯æ¨¡å‹çš„å‚æ•°
                            # è¿™é‡Œå°†å…¨å±€å‚æ•°ç§»åŠ¨åˆ°å½“å‰å®¢æˆ·ç«¯å‚æ•°æ‰€åœ¨çš„è®¾å¤‡ï¼Œå¹¶ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                            global_param_tensor = global_weights[name].detach().to(param.device, dtype=param.dtype)
                            
                            # è®¡ç®—å½“å‰å®¢æˆ·ç«¯å‚æ•°ä¸å…¨å±€å‚æ•°ä¹‹é—´çš„å¹³æ–¹å·®
                            # torch.pow(param - global_param_tensor, 2) è®¡ç®—æ¯ä¸ªå‚æ•°çš„å¹³æ–¹å·®
                            # torch.sum(...) å¯¹æ‰€æœ‰å‚æ•°çš„å¹³æ–¹å·®æ±‚å’Œï¼Œå¾—åˆ°ä¸€ä¸ªæ ‡é‡
                            prox_term += torch.sum(torch.pow(param - global_param_tensor, 2))
                    
                    # å°†FedProxçš„è¿‘ç«¯é¡¹åŠ å…¥åˆ°æ€»æŸå¤±ä¸­
                    # self.args.mu æ˜¯FedProxçš„æ­£åˆ™åŒ–å¼ºåº¦è¶…å‚æ•°
                    # prox_term æ˜¯æ‰€æœ‰å‚æ•°å¹³æ–¹å·®çš„æ€»å’Œ
                    # (self.args.mu / 2) * prox_term æ˜¯FedProxçš„æ­£åˆ™åŒ–é¡¹
                    loss += (self.args.mu / 2) * prox_term
                # --- MODIFICATION END ---
                    
                loss.backward()
                
                # æ·»åŠ æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()

                if self.args.verbose and (batch_idx % 10 == 0):
                    current_samples = (batch_idx + 1) * len(images)  # å·²å¤„ç†çš„æ ·æœ¬æ•°
                    total_samples = len(self.trainloader.dataset)
                    progress_percent = 100. * current_samples / total_samples  # ä¿®å¤ï¼šåŸºäºæ ·æœ¬æ•°è®¡ç®—è¿›åº¦
                    print('| Global Round : {} | Client : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                        global_round, self.client_id, iter_epoch, 
                        min(current_samples, total_samples),  # ç¡®ä¿ä¸è¶…è¿‡æ€»æ ·æœ¬æ•°
                        total_samples,
                        min(progress_percent, 100.0),  # ç¡®ä¿ä¸è¶…è¿‡100%
                        loss.item()))
                # self.logger.add_scalar('loss', loss.item())  # æ³¨é‡Šæ‰loggerè°ƒç”¨
                batch_loss.append(loss.item())
            epoch_loss.append(sum(batch_loss)/len(batch_loss))

        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)

    def inference(self, model):
        """æ¨ç†å‡½æ•°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        model.eval()
        loss, total, correct = 0.0, 0.0, 0.0

        with torch.no_grad():
            for batch_idx, (images, labels) in enumerate(self.testloader):
                images, labels = images.to(self.device), labels.to(self.device)

                # æ¨ç†
                outputs = model(images)
                batch_loss = self.criterion(outputs, labels)
                loss += batch_loss.item()

                # é¢„æµ‹
                _, pred_labels = torch.max(outputs, 1)
                pred_labels = pred_labels.view(-1)
                correct += torch.sum(torch.eq(pred_labels, labels)).item()
                total += len(labels)

        accuracy = correct/total
        return accuracy, loss/len(self.testloader)

    def update_weights_memory_efficient(self, global_weights, global_round, device):
        """å†…å­˜é«˜æ•ˆçš„æƒé‡æ›´æ–°æ–¹æ³•"""
        from model_factory import get_model
        
        #åˆ›å»ºæ¨¡å‹
        model = get_model(self.args.dataset, self.args.model)  # æ ¹æ®æ•°æ®é›†å’Œæ¨¡å‹åç§°åˆ›å»ºæ¨¡å‹å®ä¾‹
        model.load_state_dict(global_weights)  # åŠ è½½å…¨å±€æ¨¡å‹æƒé‡
        model = model.to(device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚GPUæˆ–CPUï¼‰
        model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        
        #åœ¨è¿™é‡Œä¸€æ¬¡æ€§åˆ›å»ºå¥½æ•™å¸ˆæ¨¡å‹ï¼ˆä¼˜åŒ–åçš„å®ç°ï¼‰
        teacher_model = None  # åˆå§‹åŒ–æ•™å¸ˆæ¨¡å‹ä¸ºNone
        # å¢åŠ ä¸€ä¸ª distill_warmup_rounds å‚æ•°ï¼Œé»˜è®¤ä¸º3ï¼Œç”¨äºæ§åˆ¶çŸ¥è¯†è’¸é¦çš„é¢„çƒ­è½®æ¬¡
        distill_warmup_rounds = getattr(self.args, 'distill_warmup_rounds', 3) 

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³å¯ç”¨çŸ¥è¯†è’¸é¦çš„æ¡ä»¶
        if (hasattr(self.args, 'iid') and self.args.iid == 0  # ç¡®ä¿æ˜¯Non-IIDåœºæ™¯
            and getattr(self.args, 'enable_knowledge_distillation', 1) == 1  # ç¡®ä¿å¯ç”¨äº†çŸ¥è¯†è’¸é¦
            and global_weights is not None  # ç¡®ä¿å…¨å±€æƒé‡å¯ç”¨
            and global_round > distill_warmup_rounds):  # ç¡®ä¿å½“å‰è½®æ¬¡è¶…è¿‡é¢„çƒ­è½®æ¬¡

            print(f"[CLIENT {self.client_id}] å¯ç”¨çŸ¥è¯†è’¸é¦ (è½®æ¬¡ > {distill_warmup_rounds})")
            
            # --- ä¼˜åŒ–ï¼šå¤ç”¨å…¨å±€æ¨¡å‹è€Œéé‡æ–°åˆ›å»º ---
            # åˆ›å»ºæ•™å¸ˆæ¨¡å‹çš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥åŸºäºå½“å‰å…¨å±€æƒé‡
            teacher_model = get_model(self.args.dataset, self.args.model)  # åˆ›å»ºæ•™å¸ˆæ¨¡å‹å®ä¾‹

            # å°†å…¨å±€æƒé‡ç§»åŠ¨åˆ°å½“å‰è®¾å¤‡å†åŠ è½½ï¼Œç¡®ä¿æƒé‡å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š
            temp_weights = {k: v.to(device) for k, v in global_weights.items()} #temp_weightsæ˜¯ä¸€ä¸ªæ–°çš„å­—å…¸ï¼Œå­˜å‚¨äº†ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡ä¸Šçš„å…¨å±€æƒé‡
            teacher_model.load_state_dict(temp_weights)  # åŠ è½½å…¨å±€æƒé‡åˆ°æ•™å¸ˆæ¨¡å‹
            teacher_model = teacher_model.to(device)  # å°†æ•™å¸ˆæ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            teacher_model.eval()  # è®¾ç½®æ•™å¸ˆæ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼

            # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœæ˜¾å­˜å’Œè®¡ç®—èµ„æº
            for param in teacher_model.parameters():
              param.requires_grad = False
        
        epoch_loss = []  # åˆå§‹åŒ–ç”¨äºå­˜å‚¨æ¯è½®è®­ç»ƒæŸå¤±çš„åˆ—è¡¨

        # è®¾ç½®ä¼˜åŒ–å™¨
        if self.args.optimizer == 'sgd':
            optimizer = torch.optim.SGD(model.parameters(), lr=self.args.lr,
                                        momentum=getattr(self.args, 'momentum', 0.5), 
                                        weight_decay=getattr(self.args, 'weight_decay', 1e-4))
        elif self.args.optimizer == 'adam':
            optimizer = torch.optim.Adam(model.parameters(), lr=self.args.lr,
                                         betas=(getattr(self.args, 'adam_beta1', 0.9), 
                                               getattr(self.args, 'adam_beta2', 0.999)),
                                         eps=getattr(self.args, 'adam_eps', 1e-8),
                                         weight_decay=getattr(self.args, 'weight_decay', 1e-4))
        elif self.args.optimizer == 'adamw':
            optimizer = torch.optim.AdamW(model.parameters(), lr=self.args.lr,
                                          betas=(getattr(self.args, 'adam_beta1', 0.9), 
                                                getattr(self.args, 'adam_beta2', 0.999)),
                                          eps=getattr(self.args, 'adam_eps', 1e-8),
                                          weight_decay=getattr(self.args, 'weight_decay', 1e-4))

        # ä¼˜åŒ–çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
        lr_scheduler_type = getattr(self.args, 'lr_scheduler', 'none')
        current_lr = self.args.lr
        
        if lr_scheduler_type == 'cosine':
            # æ”¹è¿›çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒæ•´
            total_rounds = getattr(self.args, 'epochs', 50)  # è·å–æ€»è®­ç»ƒè½®æ¬¡ï¼Œé»˜è®¤ä¸º50
            min_lr = self.args.lr * 0.05  # è®¾ç½®æœ€å°å­¦ä¹ ç‡ä¸ºåˆå§‹å­¦ä¹ ç‡çš„5%ï¼Œç”¨äºåæœŸå¾®è°ƒ
            
            # ä½¿ç”¨warmup + cosineç­–ç•¥
            warmup_rounds = min(5, total_rounds // 10)  # å‰10%çš„è½®æ¬¡ç”¨äºwarmupï¼Œæœ€å¤š5è½®
            if global_round < warmup_rounds:
            # Warmupé˜¶æ®µï¼šå­¦ä¹ ç‡ä»0çº¿æ€§å¢é•¿åˆ°ç›®æ ‡å­¦ä¹ ç‡
              current_lr = self.args.lr * (global_round + 1) / warmup_rounds
            else:
            # Cosineé€€ç«é˜¶æ®µï¼šå­¦ä¹ ç‡æŒ‰ç…§ä½™å¼¦å‡½æ•°é€æ¸å‡å°
              effective_round = global_round - warmup_rounds  # å½“å‰è½®æ¬¡å‡å»warmupè½®æ¬¡
              effective_total = total_rounds - warmup_rounds  # æ€»è½®æ¬¡å‡å»warmupè½®æ¬¡
              cosine_factor = 0.5 * (1 + math.cos(math.pi * effective_round / effective_total))  # è®¡ç®—ä½™å¼¦å› å­
              current_lr = min_lr + (self.args.lr - min_lr) * cosine_factor  # æ ¹æ®ä½™å¼¦å› å­è°ƒæ•´å­¦ä¹ ç‡
            
            # é’ˆå¯¹Non-IIDå’ŒGroupNormçš„é€‚åº”æ€§è°ƒæ•´
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šå®Œå…¨æ³¨é‡Šæ‰é¢å¤–è¡°å‡ï¼Œè®©cosineè°ƒåº¦å™¨å…¨æƒè´Ÿè´£ ---
            # if hasattr(self.args, 'iid') and self.args.iid == 0:
            #     # æ–¹æ¡ˆ (æ¨è): è®©è¡°å‡æ›´æ¸©å’Œ
            #     current_lr *= 0.9  # <--- å°†ä¹‹å‰çš„è¡°å‡å› å­ (ä¾‹å¦‚0.7) è°ƒæ•´ä¸ºæ›´å¹³ç¼“çš„ 0.9
            # if hasattr(self.args, 'model') and 'gn' in str(self.args.model).lower():
            #     current_lr *= 0.9  # GroupNormæ¨¡å‹å¾®è°ƒ
            
            # å°†è®¡ç®—å‡ºçš„å­¦ä¹ ç‡åº”ç”¨åˆ°ä¼˜åŒ–å™¨çš„å‚æ•°ç»„ä¸­
            for param_group in optimizer.param_groups:
              param_group['lr'] = current_lr
                
        print(f"[CLIENT {self.client_id}] è½®æ¬¡ {global_round}: å­¦ä¹ ç‡ = {current_lr:.6f}")

        # æœ¬åœ°è®­ç»ƒ
        for iter_epoch in range(self.args.local_ep):
            batch_loss = []
            for batch_idx, (images, labels) in enumerate(self.trainloader):
                images, labels = images.to(device), labels.to(device)

                model.zero_grad()
                log_probs = model(images)
                
                # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«NaN
                if torch.isnan(log_probs).any():
                    print(f"è­¦å‘Š: æ¨¡å‹è¾“å‡ºåŒ…å«NaNå€¼ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
                
                # è®¡ç®—æ ‡å‡†äº¤å‰ç†µæŸå¤±
                ce_loss = self.criterion(log_probs, labels)
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
                if torch.isnan(ce_loss):
                    print(f"è­¦å‘Š: äº¤å‰ç†µæŸå¤±ä¸ºNaNï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
                
                # Non-IIDåœºæ™¯ä¸‹æ·»åŠ çŸ¥è¯†è’¸é¦ç­–ç•¥
                total_loss = ce_loss
                if teacher_model is not None: # <--- ç›´æ¥åˆ¤æ–­æ•™å¸ˆæ¨¡å‹æ˜¯å¦å­˜åœ¨
                    try:
                        # ä½¿ç”¨é¢„å…ˆåˆ›å»ºçš„æ•™å¸ˆæ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦
                        with torch.no_grad():
                            teacher_logits = teacher_model(images) # <--- ç›´æ¥å¤ç”¨ï¼Œä¸å†é‡æ–°åˆ›å»º
                        
                        # æ£€æŸ¥teacherè¾“å‡ºæ˜¯å¦æœ‰æ•ˆ
                        if not torch.isnan(teacher_logits).any() and not torch.isinf(teacher_logits).any():
                            # è®¡ç®—çŸ¥è¯†è’¸é¦æŸå¤±
                            T = getattr(self.args, 'distill_temperature', 3.0)
                            alpha = getattr(self.args, 'distill_alpha', 0.3)
                            
                            student_soft = torch.log_softmax(log_probs / T, dim=1)
                            teacher_soft = torch.softmax(teacher_logits / T, dim=1)
                            distill_loss = torch.nn.functional.kl_div(student_soft, teacher_soft, reduction='batchmean') * (T * T)
                            
                            # æ£€æŸ¥è’¸é¦æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                            if not torch.isnan(distill_loss) and not torch.isinf(distill_loss):
                                total_loss = (1 - alpha) * ce_loss + alpha * distill_loss
                    except Exception as e:
                        print(f"çŸ¥è¯†è’¸é¦è®¡ç®—å‡ºé”™ï¼Œä½¿ç”¨æ ‡å‡†æŸå¤±: {str(e)}")
                        total_loss = ce_loss
                
                loss = total_loss
                
                # æœ€ç»ˆæ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN
                if torch.isnan(loss):
                    print(f"è­¦å‘Š: æœ€ç»ˆæŸå¤±ä¸ºNaN,è·³è¿‡æ­¤æ‰¹æ¬¡")
                    continue
                
                # åªæœ‰åœ¨Non-IIDåœºæ™¯ä¸‹ï¼Œå¹¶ä¸”mu>0æ—¶ï¼Œæ‰åº”ç”¨FedProxè¿‘ç«¯é¡¹
                if getattr(self.args, 'iid', 1) == 0 and getattr(self.args, 'mu', 0.0) > 0 and global_weights is not None:
                    prox_term = 0.0
                    # global_weights æ˜¯æœ¬è½®å¼€å§‹æ—¶çš„å…¨å±€æ¨¡å‹æƒé‡
                    for name, param in model.named_parameters():
                        if name in global_weights:
                            # ç¡®ä¿å…¨å±€å‚æ•°å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                            global_param_tensor = global_weights[name].detach().to(param.device)
                            prox_term += torch.sum(torch.pow(param - global_param_tensor, 2))
                    
                    loss += (self.args.mu / 2) * prox_term
                
                loss.backward()
                
                # æ·»åŠ æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()

                if self.args.verbose and (batch_idx % 10 == 0):
                    current_samples = (batch_idx + 1) * len(images)  # å·²å¤„ç†çš„æ ·æœ¬æ•°
                    total_samples = len(self.trainloader.dataset)
                    progress_percent = 100. * current_samples / total_samples  # ä¿®å¤ï¼šåŸºäºæ ·æœ¬æ•°è®¡ç®—è¿›åº¦
                    print('| Global Round : {} | Client : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                        global_round, self.client_id, iter_epoch, 
                        min(current_samples, total_samples),  # ç¡®ä¿ä¸è¶…è¿‡æ€»æ ·æœ¬æ•°
                        total_samples,
                        min(progress_percent, 100.0),  # ç¡®ä¿ä¸è¶…è¿‡100%
                        loss.item()))
                # self.logger.add_scalar('loss', loss.item())  # æ³¨é‡Šæ‰loggerè°ƒç”¨
                batch_loss.append(loss.item())
            epoch_loss.append(sum(batch_loss)/len(batch_loss))

        # è·å–æ›´æ–°åçš„æƒé‡ï¼ˆåœ¨CPUä¸Šï¼‰
        updated_weights = {k: v.cpu() for k, v in model.state_dict().items()}
            
        return updated_weights, sum(epoch_loss) / len(epoch_loss)

def model_subtract(dict1, dict2):
    """è®¡ç®—ä¸¤ä¸ªæ¨¡å‹å‚æ•°å­—å…¸çš„å·®å€¼ï¼ˆæ®‹å·®ï¼‰ï¼Œç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
    
    Args:
        dict1: è¢«å‡æ•°å‚æ•°å­—å…¸ï¼ˆé€šå¸¸æ˜¯æ›´æ–°åçš„æœ¬åœ°æƒé‡ï¼‰
        dict2: å‡æ•°å‚æ•°å­—å…¸ï¼ˆé€šå¸¸æ˜¯å…¨å±€æ¨¡å‹æƒé‡ï¼‰
        
    Returns:
        result: å·®å€¼å­—å…¸ï¼ˆæ®‹å·®ï¼‰
    """
    result = {}
    for key in dict1.keys():
        # ç¡®ä¿ä¸¤ä¸ªå­—å…¸éƒ½åŒ…å«è¯¥é”®
        if key not in dict2:
            continue  # è·³è¿‡dict2ä¸­ä¸å­˜åœ¨çš„é”®
            
        # ç¡®ä¿ä¸¤ä¸ªå¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
        tensor1 = dict1[key]
        tensor2 = dict2[key]
        
        # å¦‚æœè®¾å¤‡ä¸åŒï¼Œå°†tensor2ç§»åŠ¨åˆ°tensor1çš„è®¾å¤‡
        if tensor1.device != tensor2.device:
            tensor2 = tensor2.to(tensor1.device)
        
        result[key] = tensor1 - tensor2
    return result

def model_add(dict1, dict2):
    """å°†ä¸¤ä¸ªæ¨¡å‹å‚æ•°å­—å…¸ç›¸åŠ ï¼Œç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
    
    Args:
        dict1: åŸºç¡€æ¨¡å‹å‚æ•°å­—å…¸ï¼ˆé€šå¸¸æ˜¯å…¨å±€æ¨¡å‹æƒé‡ï¼‰
        dict2: è¦æ·»åŠ çš„å‚æ•°å­—å…¸ï¼ˆé€šå¸¸æ˜¯æ®‹å·®ï¼‰
        
    Returns:
        result: ç›¸åŠ åçš„å‚æ•°å­—å…¸
    """
    result = {}
    for key in dict1.keys():
        # å¦‚æœæ®‹å·®å­—å…¸ä¸­ä¸åŒ…å«è¯¥é”®ï¼ˆæ¯”å¦‚è¢«è¿‡æ»¤æ‰çš„å½’ä¸€åŒ–å±‚å‚æ•°ï¼‰ï¼Œ
        # åˆ™ä¿æŒåŸå€¼ä¸å˜
        if key not in dict2:
            result[key] = dict1[key].clone()
            continue
            
        # ç¡®ä¿ä¸¤ä¸ªå¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
        tensor1 = dict1[key]
        tensor2 = dict2[key]
        
        # å¦‚æœè®¾å¤‡ä¸åŒï¼Œå°†tensor2ç§»åŠ¨åˆ°tensor1çš„è®¾å¤‡
        if tensor1.device != tensor2.device:
            tensor2 = tensor2.to(tensor1.device)
        
        result[key] = tensor1 + tensor2
    return result

def average_weights_residual(w, template=None, enable_timing=False):
    """
    å¹³å‡å¤šä¸ªæ®‹å·®å­—å…¸ (æ”¯æŒæ©ç +å€¼æ ¼å¼å’Œå¯†é›†æ ¼å¼)ã€‚
    
    Args:
        w: æ®‹å·®å­—å…¸åˆ—è¡¨ï¼Œå¯ä»¥æ˜¯æ‰“åŒ…åçš„ (mask, values) æ ¼å¼æˆ–å¯†é›†å¼ é‡æ ¼å¼ã€‚
        template (dict): æ¨¡æ¿å­—å…¸ï¼Œæä¾›ç›®æ ‡å½¢çŠ¶ã€è®¾å¤‡å’Œæ•°æ®ç±»å‹ï¼ˆä»…å‹ç¼©æ¨¡å¼éœ€è¦ï¼‰ã€‚
        enable_timing (bool): æ˜¯å¦å¯ç”¨æ—¶é—´ç»Ÿè®¡
        
    Returns:
        w_avg: å¹³å‡åçš„æ®‹å·®å­—å…¸ï¼ˆå¯†é›†Tensoræ ¼å¼ï¼‰ã€‚
        æˆ– (w_avg, total_unpack_time): å¦‚æœå¯ç”¨æ—¶é—´ç»Ÿè®¡ï¼Œè¿”å›å…ƒç»„
    """
    if not w or len(w) == 0:
        return {}
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ®‹å·®å­—å…¸ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ çš„æ ¼å¼
    first_residual = w[0]
    if not first_residual:
        return {}
    
    first_key = next(iter(first_residual))
    first_value = first_residual[first_key]
    
    # åˆ¤æ–­æ˜¯æ–°æ ¼å¼ï¼ˆpackedï¼‰è¿˜æ˜¯æ—§æ ¼å¼ï¼ˆdenseï¼‰
    # éœ€è¦æ£€æŸ¥ä½æ‰“åŒ…æ ¼å¼æˆ–ä¼ ç»Ÿæ ¼å¼
    is_packed_format = (isinstance(first_value, dict) and 
                       (('mask' in first_value and 'values' in first_value) or 
                        ('mask_data' in first_value and 'is_bit_packed' in first_value)))
    
    if is_packed_format and template is not None:
        # æ–°æ ¼å¼ï¼šå¤„ç†æ‰“åŒ…çš„ (mask, values) æ•°æ®ï¼Œæ”¯æŒä½çº§å’Œä¼ ç»Ÿæ ¼å¼
        w_avg = {}
        total_unpack_time = 0.0  # ç»Ÿè®¡è§£åŒ…æ—¶é—´
        unpack_count = 0
        
        # --- æ–°å¢ä»£ç ï¼šç›´æ¥ä»æ¨¡æ¿è§£åŒ…æ‰€æœ‰æ®‹å·® ---
        dense_residuals = []
        for res in w:
            unpack_result = unpack_sparse_residual(res, template=template, enable_timing=enable_timing)
            if enable_timing and isinstance(unpack_result, tuple):
                dense_res, unpack_time = unpack_result
                total_unpack_time += unpack_time
            else:
                dense_res = unpack_result
            dense_residuals.append(dense_res)
        # ----------------------------------------

        # --- ä¿®æ”¹ä»£ç ï¼šä½¿ç”¨è§£åŒ…åçš„ dense_residuals è¿›è¡Œå¹³å‡ ---
        if not dense_residuals:
            return {}
            
        w_avg = copy.deepcopy(dense_residuals[0])
        for key in w_avg.keys():
            # ä»ç¬¬äºŒä¸ªæ®‹å·®å¼€å§‹ç´¯åŠ 
            for i in range(1, len(dense_residuals)):
                # å› ä¸ºå…¨éƒ¨åœ¨CPUä¸Šï¼Œæ— éœ€æ£€æŸ¥è®¾å¤‡
                w_avg[key] += dense_residuals[i][key]
            
            # é™¤ä»¥å®¢æˆ·ç«¯æ•°é‡å¾—åˆ°å¹³å‡å€¼
            w_avg[key] = torch.div(w_avg[key], len(dense_residuals))
        # ----------------------------------------------------
        
        # å¦‚æœå¯ç”¨äº†æ—¶é—´ç»Ÿè®¡ï¼Œæ‰“å°å¹¶è¿”å›æ—¶é—´ä¿¡æ¯
        if enable_timing and total_unpack_time > 0:
            print(f"   ğŸ“¦ æœåŠ¡å™¨æ ‡å‡†èšåˆè§£åŒ…è€—æ—¶: {total_unpack_time:.2f}ms")
            return w_avg, total_unpack_time
            
        return w_avg
        

    elif template is None:
        # å…¨é‡æ¨¡å¼ï¼šç›´æ¥å¤„ç†å¯†é›†å¼ é‡æ•°æ®ï¼Œæ— éœ€è§£åŒ…
        print("   ğŸš€ å…¨é‡æ¨¡å¼èšåˆï¼šç›´æ¥å¤„ç†å¯†é›†å¼ é‡ï¼Œè·³è¿‡è§£åŒ…æ­¥éª¤")
        
        # ç›´æ¥ä½¿ç”¨å¯†é›†å¼ é‡è¿›è¡Œå¹³å‡èšåˆ
        w_avg = copy.deepcopy(w[0])
        
        for key in w_avg.keys():
            # ä»ç¬¬äºŒä¸ªæ®‹å·®å¼€å§‹ç´¯åŠ 
            for i in range(1, len(w)):
                # ç¡®ä¿å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Šå†ç›¸åŠ 
                if w[i][key].device != w_avg[key].device:
                    w[i][key] = w[i][key].to(w_avg[key].device)
                w_avg[key] = torch.add(w_avg[key], w[i][key])
            
            # é™¤ä»¥å®¢æˆ·ç«¯æ•°é‡å¾—åˆ°å¹³å‡å€¼
            w_avg[key] = torch.div(w_avg[key], len(w))
        
        return w_avg
        
    else:
        # æ—§æ ¼å¼ï¼šå¤„ç†å¯†é›†å¼ é‡æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰
        dense_residuals = []
        for residual_dict in w:
            dense_w = {}
            for key, tensor in residual_dict.items():
                if hasattr(tensor, 'is_sparse') and tensor.is_sparse:
                    # PyTorchç¨€ç–å¼ é‡è½¬å¯†é›†
                    dense_w[key] = tensor.to_dense()
                else:
                    # å·²ç»æ˜¯å¯†é›†å¼ é‡
                    dense_w[key] = tensor
            dense_residuals.append(dense_w)
            
        # ä½¿ç”¨å¯†é›†å¼ é‡è¿›è¡Œå¹³å‡èšåˆ
        w_avg = copy.deepcopy(dense_residuals[0])
        
        for key in w_avg.keys():
            # ä»ç¬¬äºŒä¸ªæ®‹å·®å¼€å§‹ç´¯åŠ 
            for i in range(1, len(dense_residuals)):
                # ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
                if w_avg[key].device != dense_residuals[i][key].device:
                    dense_residuals[i][key] = dense_residuals[i][key].to(w_avg[key].device)
                
                # å¯†é›†å¼ é‡çš„åŠ æ³•
                w_avg[key] += dense_residuals[i][key]
            
            # é™¤ä»¥å®¢æˆ·ç«¯æ•°é‡å¾—åˆ°å¹³å‡å€¼
            w_avg[key] = torch.div(w_avg[key], len(dense_residuals))
        
        return w_avg

def apply_residual_compression_fast(residual, compression_ratio=0.1):
    """
    ä¼˜åŒ–çš„æ®‹å·®å‹ç¼©å‡½æ•°ï¼Œä½¿ç”¨Top-Kç¨€ç–åŒ–å‡å°‘é€šä¿¡å¼€é”€
    
    Args:
        residual: æ®‹å·®å‚æ•°å­—å…¸ï¼ŒåŒ…å«æ¯ä¸€å±‚çš„å‚æ•°æ›´æ–°å€¼
        compression_ratio: å‹ç¼©æ¯”ä¾‹ (0~1)ï¼Œè¡¨ç¤ºä¿ç•™çš„å‚æ•°æ¯”ä¾‹ï¼Œå€¼è¶Šå°å‹ç¼©è¶Šå¼º
    
    Returns:
        compressed_residual: å‹ç¼©åçš„æ®‹å·®å­—å…¸ï¼Œä¿ç•™é‡è¦å‚æ•°ï¼Œå…¶ä½™ç½®é›¶
    """
    compressed_residual = {}  # å­˜å‚¨å‹ç¼©åçš„æ®‹å·®

    for key, param in residual.items():
        # å¦‚æœå‚æ•°ä¸æ˜¯æµ®ç‚¹ç±»å‹ï¼ˆä¾‹å¦‚æ•´æ•°ç±»å‹ï¼‰ï¼Œç›´æ¥å…‹éš†ä¿å­˜ï¼Œä¸è¿›è¡Œå‹ç¼©
        if not param.dtype.is_floating_point:
            compressed_residual[key] = param.clone()
            continue

        # ä½¿ç”¨torch.no_grad()é¿å…è®¡ç®—å›¾çš„æ„å»ºï¼Œå‡å°‘å†…å­˜å’Œè®¡ç®—å¼€é”€
        with torch.no_grad():
            # è°ƒç”¨ä¼˜åŒ–çš„Top-Kå‹ç¼©å‡½æ•°ï¼Œå¯¹å½“å‰å±‚çš„å‚æ•°è¿›è¡Œç¨€ç–åŒ–
            compressed_residual[key] = fast_topk_compression(param, compression_ratio)

    return compressed_residual

def fast_topk_compression(param, compression_ratio):
    """
    ä¼˜åŒ–çš„Top-Kå‹ç¼©å®ç°ï¼Œç”¨äºç¨€ç–åŒ–å¼ é‡ä»¥å‡å°‘é€šä¿¡å¼€é”€ã€‚
    
    Args:
        param: è¾“å…¥çš„å¼ é‡ï¼ˆé€šå¸¸æ˜¯æ¨¡å‹çš„å‚æ•°æˆ–æ®‹å·®ï¼‰ã€‚
        compression_ratio: å‹ç¼©æ¯”ä¾‹ (0~1)ï¼Œè¡¨ç¤ºä¿ç•™çš„å‚æ•°æ¯”ä¾‹ï¼Œå€¼è¶Šå°å‹ç¼©è¶Šå¼ºã€‚
    
    Returns:
        å‹ç¼©åçš„å¼ é‡ï¼Œä»…ä¿ç•™é‡è¦å‚æ•°ï¼Œå…¶ä½™ç½®é›¶ã€‚
    """
    # å°†å¼ é‡å±•å¹³ä¸ºä¸€ç»´å‘é‡ï¼Œä½¿ç”¨viewä»£æ›¿flattenä»¥æé«˜æ•ˆç‡
    flat_param = param.view(-1)
    
    # è®¡ç®—éœ€è¦ä¿ç•™çš„å‚æ•°æ•°é‡ k
    k = max(1, int(len(flat_param) * compression_ratio))
    
    # å¦‚æœä¿ç•™çš„å‚æ•°æ•°é‡å¤§äºç­‰äºæ€»å‚æ•°æ•°é‡ï¼Œåˆ™æ— éœ€å‹ç¼©ï¼Œç›´æ¥è¿”å›åŸå¼ é‡çš„å‰¯æœ¬
    if k >= len(flat_param):
        return param.clone()
    
    # ä½¿ç”¨ kthvalue æ–¹æ³•è®¡ç®—é˜ˆå€¼ï¼Œæ¯” topk æ–¹æ³•æ›´é«˜æ•ˆ
    try:
        # æ‰¾åˆ°ç¬¬ (len(flat_param) - k) å°çš„ç»å¯¹å€¼ä½œä¸ºé˜ˆå€¼
        threshold = torch.kthvalue(torch.abs(flat_param), len(flat_param) - k)[0]
        
        # åˆ›å»ºæ©ç ï¼Œæ ‡è®°å¤§äºç­‰äºé˜ˆå€¼çš„å…ƒç´ 
        mask = torch.abs(flat_param) >= threshold
        
        # åˆ›å»ºä¸€ä¸ªä¸åŸå¼ é‡å½¢çŠ¶ç›¸åŒçš„é›¶å¼ é‡
        compressed = torch.zeros_like(flat_param)
        
        # ä½¿ç”¨æ©ç ä¿ç•™é‡è¦å‚æ•°ï¼Œå…¶ä½™ç½®é›¶
        compressed[mask] = flat_param[mask]
        
        # å°†å‹ç¼©åçš„å¼ é‡æ¢å¤ä¸ºåŸå§‹å½¢çŠ¶å¹¶è¿”å›
        return compressed.view(param.shape)
    except:
        # å¦‚æœ kthvalue æ–¹æ³•å¤±è´¥ï¼ˆä¾‹å¦‚åœ¨æŸäº›ç‰¹æ®Šæƒ…å†µä¸‹ï¼‰ï¼Œå›é€€åˆ°æ ‡å‡†çš„ topk æ–¹æ³•
        # ä½¿ç”¨ topk æ‰¾åˆ°ç»å¯¹å€¼æœ€å¤§çš„ k ä¸ªå…ƒç´ çš„ç´¢å¼•
        _, top_k_indices = torch.topk(torch.abs(flat_param), k)
        
        # åˆ›å»ºä¸€ä¸ªä¸åŸå¼ é‡å½¢çŠ¶ç›¸åŒçš„é›¶å¼ é‡
        compressed = torch.zeros_like(flat_param)
        
        # æ ¹æ®ç´¢å¼•ä¿ç•™é‡è¦å‚æ•°ï¼Œå…¶ä½™ç½®é›¶
        compressed[top_k_indices] = flat_param[top_k_indices]
        
        # å°†å‹ç¼©åçš„å¼ é‡æ¢å¤ä¸ºåŸå§‹å½¢çŠ¶å¹¶è¿”å›
        return compressed.view(param.shape)

def adaptive_client_aggregation(local_residuals, client_data_sizes, client_losses, 
                               server_model_template, diversity_scores=None, 
                               aggregation_method='weighted_avg', enable_timing=False):
    #FedAC(Federated Averaging with Adaptive Client Weighting)
    """
    è‡ªé€‚åº”å®¢æˆ·ç«¯èšåˆç­–ç•¥
    æ ¹æ®æ•°æ®é‡ã€æŸå¤±å’Œå¤šæ ·æ€§åŠ¨æ€è°ƒæ•´èšåˆæƒé‡
    """
    if not local_residuals:
        return {}
    
    num_clients = len(local_residuals)
    '''
    å…³äºlocal_residualsçš„ç¤ºä¾‹:
    local_residuals = [
    {"layer1": torch.tensor([1.0, 2.0]), "layer2": torch.tensor([3.0, 4.0])},
    {"layer1": torch.tensor([0.5, 1.5]), "layer2": torch.tensor([2.5, 3.5])}
    ]
    '''
    
    # è®¡ç®—åŸºç¡€æƒé‡ï¼ˆæ•°æ®é‡ï¼‰
    total_samples = sum(client_data_sizes)
    data_weights = [size / total_samples for size in client_data_sizes]
    
    # å¤„ç†NaNæˆ–æ— ç©·æŸå¤±å€¼
    safe_losses = []
    for loss in client_losses:
        #æ¯ä¸ªå®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒåè®¡ç®—å¾—åˆ°çš„å¹³å‡æŸå¤±
        if torch.isnan(torch.tensor(loss)) or torch.isinf(torch.tensor(loss)):
            safe_losses.append(1.0)  # ä½¿ç”¨é»˜è®¤æŸå¤±å€¼
        else:
            safe_losses.append(float(loss))
    
    if aggregation_method == 'weighted_avg':
        # æ ‡å‡†åŠ æƒå¹³å‡
        weights = data_weights
    elif aggregation_method == 'diversity_aware' and diversity_scores is not None:
        # åŸºäºå¤šæ ·æ€§çš„æƒé‡è°ƒæ•´
        # å¤šæ ·æ€§è¶Šé«˜ï¼Œæƒé‡é€‚å½“å¢åŠ ï¼ˆä½†ä¸è¿‡åº¦ï¼‰
        
        # 1. å…ˆè®¡ç®—æŸå¤±æƒé‡
        loss_weights = [1.0 / (1.0 + loss) for loss in safe_losses]
        total_loss_weight = sum(loss_weights)
        if total_loss_weight > 0:
            # å½’ä¸€åŒ–æŸå¤±æƒé‡
            loss_weights = [w / total_loss_weight for w in loss_weights]
        else:
            # å¦‚æœæ€»æŸå¤±æƒé‡ä¸º0ï¼Œå‡åŒ€åˆ†é…æƒé‡
            loss_weights = [1.0 / num_clients] * num_clients

        # 2. è®¡ç®—å¤šæ ·æ€§æƒé‡
        # å¤šæ ·æ€§è¶Šé«˜ï¼Œæƒé‡é€‚å½“å¢åŠ ï¼ˆä½†ä¸è¿‡åº¦ï¼‰
        div_weights = [min(1.5, 1.0 + 0.3 * score) for score in diversity_scores]
        # æ³¨æ„ï¼šè¿™é‡Œçš„ div_weights æ˜¯ä¸€ä¸ª"è°ƒæ•´å› å­"ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå½’ä¸€åŒ–çš„æƒé‡

        # 3. ç»“åˆæ•°æ®é‡ã€æŸå¤±å’Œå¤šæ ·æ€§æƒé‡
        # åŸºç¡€æƒé‡ç»“åˆæ•°æ®é‡å’ŒæŸå¤±ï¼Œå¤šæ ·æ€§ä½œä¸ºä¹˜æ³•è°ƒæ•´é¡¹
        base_weights = [0.6 * dw + 0.4 * lw for dw, lw in zip(data_weights, loss_weights)]

        # ç”¨å¤šæ ·æ€§å› å­æ¥è°ƒæ•´åŸºç¡€æƒé‡
        weights = [bw * divw for bw, divw in zip(base_weights, div_weights)]
    else:
        # é»˜è®¤ä½¿ç”¨æ•°æ®é‡æƒé‡
        weights = data_weights
    
    # å½’ä¸€åŒ–æƒé‡å¹¶ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
    total_weight = sum(weights)
    if total_weight > 0:
        # å½’ä¸€åŒ–æƒé‡
        weights = [w / total_weight for w in weights]
    else:
        # å¦‚æœæ€»æƒé‡ä¸º0ï¼Œå‡åŒ€åˆ†é…æƒé‡
        weights = [1.0 / num_clients] * num_clients
    
    # ç¡®ä¿æƒé‡èŒƒå›´åˆç†
    weights = [max(min(w, 1.0), 0.0) for w in weights]
    
    # å†æ¬¡å½’ä¸€åŒ–æƒé‡
    total_weight = sum(weights)
    if total_weight > 0:
        weights = [w / total_weight for w in weights]
    else:
        weights = [1.0 / num_clients] * num_clients
    '''
    è™½ç„¶ç¬¬ä¸€æ¬¡å½’ä¸€åŒ–ç†è®ºä¸Šåº”è¯¥èƒ½ç¡®ä¿æƒé‡åœ¨åˆç†èŒƒå›´å†…
    ä½†åœ¨å¤æ‚çš„è®¡ç®—ä¸­ï¼Œå¯èƒ½ä¼šå› ä¸ºæµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜æˆ–æŸäº›æç«¯æƒ…å†µï¼ˆä¾‹å¦‚æŸä¸ªæƒé‡å› å¤šæ ·æ€§å¥–åŠ±ä¹˜æ•°å˜å¾—ç•¥å¤§äº1ï¼‰
    å¯¼è‡´ä¸ªåˆ«æƒé‡å€¼å‡ºç°å¾®å°çš„åå·®ï¼Œæ¯”å¦‚å˜æˆ -0.0000001 æˆ–è€… 1.0000001
    è¿™ä¸€æ­¥ max(min(w, 1.0), 0.0) ä¼šéå¸¸ä¸¥æ ¼åœ°å°†æ‰€æœ‰è¿™äº›æ„å¤–å€¼â€œè£å‰ªâ€å› [0.0, 1.0] çš„åŒºé—´å†…
    '''

    # æ‰§è¡ŒåŠ æƒèšåˆ
    aggregated_residual = {}  # åˆå§‹åŒ–èšåˆåçš„æ®‹å·®å­—å…¸
    first_residual = local_residuals[0]  # è·å–ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„æ®‹å·®ä½œä¸ºæ¨¡æ¿
    total_unpack_time = 0.0  # åˆå§‹åŒ–è®¡æ—¶å™¨ï¼Œç”¨äºç»Ÿè®¡è§£åŒ…æ—¶é—´
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ‰“åŒ…æ ¼å¼çš„æ•°æ®
    first_value = next(iter(first_residual.values())) if first_residual else None
    is_packed_format = (isinstance(first_value, dict) and 
                       (('mask' in first_value and 'values' in first_value) or 
                        ('mask_data' in first_value and 'is_bit_packed' in first_value)))
    
    if is_packed_format and server_model_template is not None:
        # isinstanceæ£€æŸ¥ç¬¬ä¸€ä¸ªå€¼æ˜¯å­—å…¸è¿˜æ˜¯å¼ é‡,å¯¹åº”ç€æ˜¯æ‰“åŒ…æ®‹å·®å­—å…¸è¿˜æ˜¯æ²¡æœ‰æ‰“åŒ…çš„å¼ é‡æ®‹å·®å­—å…¸
        # æ‰“åŒ…æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æ¨¡æ¿è¿›è¡Œè§£åŒ…
        
        # è§£åŒ…æ‰€æœ‰å®¢æˆ·ç«¯çš„æ®‹å·®ï¼ˆä½¿ç”¨CPUæ¨¡æ¿ï¼‰
        dense_residuals = []  # ç”¨äºå­˜å‚¨è§£åŒ…åçš„å¯†é›†æ ¼å¼æ®‹å·®
        for residual in local_residuals:
            unpack_result = unpack_sparse_residual(residual, 
                                                   template=server_model_template, 
                                                   enable_timing=enable_timing)
            
            if enable_timing and isinstance(unpack_result, tuple):
                dense_residual, unpack_time = unpack_result
                total_unpack_time += unpack_time
            else:
                dense_residual = unpack_result
            
            dense_residuals.append(dense_residual)
        
        # ä½¿ç”¨å¯†é›†æ ¼å¼è¿›è¡ŒåŠ æƒèšåˆï¼ˆåœ¨CPUä¸Šå®Œæˆï¼‰
        first_tensor_dict = dense_residuals[0]  # è·å–ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„å¯†é›†æ®‹å·®ä½œä¸ºæ¨¡æ¿
        for key in first_tensor_dict.keys():
            first_tensor = first_tensor_dict[key]
            
            # åˆå§‹åŒ–å½“å‰å±‚çš„èšåˆå¼ é‡ï¼ˆåœ¨CPUä¸Šï¼‰
            aggregated_residual[key] = torch.zeros_like(first_tensor, 
                                                       dtype=first_tensor.dtype,
                                                       device=first_tensor.device)  # åº”è¯¥åœ¨CPUä¸Š
            
            # éå†æ¯ä¸ªå®¢æˆ·ç«¯çš„æ®‹å·®ï¼Œè¿›è¡ŒåŠ æƒç´¯åŠ 
            for i, residual in enumerate(dense_residuals):
                # æ£€æŸ¥å½“å‰å®¢æˆ·ç«¯çš„æ®‹å·®æ˜¯å¦åŒ…å«NaNæˆ–Infå€¼
                if torch.isnan(residual[key]).any() or torch.isinf(residual[key]).any():
                    print(f"è­¦å‘Š: å®¢æˆ·ç«¯ {i} çš„æ®‹å·®åŒ…å«NaN/Infå€¼ï¼Œè·³è¿‡æ­¤å®¢æˆ·ç«¯")
                    continue
                
                # å°†æƒé‡è½¬æ¢ä¸ºå¼ é‡ï¼Œç¡®ä¿ç±»å‹å’Œè®¾å¤‡ä¸€è‡´ï¼ˆCPUä¸Šï¼‰
                weight_scalar = torch.tensor(float(weights[i]), dtype=torch.float32, 
                                           device=residual[key].device)  # åº”è¯¥åœ¨CPUä¸Š
                
                if residual[key].dtype != torch.float32:
                    weight_scalar = weight_scalar.to(residual[key].dtype)
                
                # åŠ æƒç´¯åŠ æ®‹å·®ï¼ˆCPUä¸Šçš„è®¡ç®—ï¼‰
                aggregated_residual[key] += weight_scalar * residual[key]
    
    else:
        # --- æ–°å¢ä»£ç å—ï¼šå¤„ç†å…¨é‡(dense)æ¨¡å¼ ---
        # local_residuals å·²ç»æ˜¯ [dense_dict_1, dense_dict_2, ...]
        
        first_tensor_dict = local_residuals[0]
        for key in first_tensor_dict.keys():
            
            first_tensor = first_tensor_dict[key]
            
            # åˆå§‹åŒ–å½“å‰å±‚çš„èšåˆå¼ é‡
            aggregated_residual[key] = torch.zeros_like(first_tensor, 
                                                       dtype=first_tensor.dtype,
                                                       device=first_tensor.device)
            
            # éå†æ¯ä¸ªå®¢æˆ·ç«¯çš„æ®‹å·®ï¼Œè¿›è¡ŒåŠ æƒç´¯åŠ 
            for i, residual in enumerate(local_residuals):
                # æ£€æŸ¥å½“å‰å®¢æˆ·ç«¯çš„æ®‹å·®æ˜¯å¦åŒ…å«NaNæˆ–Infå€¼
                if key not in residual or torch.isnan(residual[key]).any() or torch.isinf(residual[key]).any():
                    print(f"è­¦å‘Š: å®¢æˆ·ç«¯ {i} çš„æ®‹å·® {key} åŒ…å«æ— æ•ˆå€¼ï¼Œè·³è¿‡æ­¤å®¢æˆ·ç«¯")
                    continue
                
                # ç¡®ä¿è®¾å¤‡ä¸€è‡´
                if residual[key].device != aggregated_residual[key].device:
                    residual[key] = residual[key].to(aggregated_residual[key].device)
                
                # å°†æƒé‡è½¬æ¢ä¸ºå¼ é‡ï¼Œç¡®ä¿ç±»å‹å’Œè®¾å¤‡ä¸€è‡´
                weight_scalar = torch.tensor(float(weights[i]), dtype=torch.float32, 
                                           device=residual[key].device)
                
                if residual[key].dtype != torch.float32:
                    weight_scalar = weight_scalar.to(residual[key].dtype)
                
                # åŠ æƒç´¯åŠ æ®‹å·®
                aggregated_residual[key] += weight_scalar * residual[key]
        # --- æ–°å¢ä»£ç å—ç»“æŸ ---

    # åœ¨å‡½æ•°æœ«å°¾è¿”å›è®¡æ—¶ç»“æœ
    if enable_timing:
        print(f"   ğŸ“¦ æœåŠ¡å™¨è‡ªé€‚åº”èšåˆè§£åŒ…è€—æ—¶: {total_unpack_time:.2f}ms")
        return aggregated_residual, total_unpack_time
    else:
        return aggregated_residual

def pack_bool_mask_to_bits(bool_mask):
    """
    ä½çº§æ‰“åŒ…å‡½æ•°
    å°†å¸ƒå°”æ©ç æ‰“åŒ…ä¸ºä½çº§å­˜å‚¨,8ä¸ªå¸ƒå°”å€¼æ‰“åŒ…ä¸º1ä¸ªå­—èŠ‚,èŠ‚çœ87.5%å­˜å‚¨ç©ºé—´
    
    Args:
        bool_mask: torch.BoolTensor,å¸ƒå°”æ©ç 
        
    Returns:
        tuple: (packed_bytes, original_shape, num_bits)
            - packed_bytes: np.ndarray (uint8)ï¼Œæ‰“åŒ…åçš„å­—èŠ‚æ•°ç»„  
            - original_shape: tuple,åŸå§‹æ©ç çš„å½¢çŠ¶
            - num_bits: int,åŸå§‹æ©ç çš„æ€»ä½æ•°,ç”¨äºè§£å‹ç¼©æ—¶æˆªå–æœ‰æ•ˆä½
    """
    # è·å–å¸ƒå°”æ©ç çš„åŸå§‹å½¢çŠ¶ï¼Œç”¨äºè§£å‹ç¼©æ—¶æ¢å¤å½¢çŠ¶
    original_shape = bool_mask.shape
    
    # å°†å¸ƒå°”æ©ç å±•å¹³ä¸ºä¸€ç»´æ•°ç»„ï¼Œå¹¶è½¬æ¢ä¸ºNumPyæ•°ç»„
    # .flatten() å°†å¼ é‡å±•å¹³ä¸ºä¸€ç»´
    # .cpu() å°†å¼ é‡ç§»åŠ¨åˆ°CPUä¸Šï¼ˆå¦‚æœåœ¨GPUä¸Šï¼‰
    # .numpy() è½¬æ¢ä¸ºNumPyæ•°ç»„
    # .astype(np.uint8) å°†å¸ƒå°”å€¼è½¬æ¢ä¸ºæ— ç¬¦å·8ä½æ•´æ•°ï¼ˆ0æˆ–1ï¼‰
    flat_mask = bool_mask.flatten().cpu().numpy().astype(np.uint8)
    
    # è®¡ç®—å¸ƒå°”æ©ç çš„æ€»ä½æ•°ï¼ˆå³å±•å¹³åçš„ä¸€ç»´æ•°ç»„é•¿åº¦ï¼‰
    num_bits = len(flat_mask)
    
    # ä½¿ç”¨NumPyçš„packbitså‡½æ•°å°†å¸ƒå°”å€¼æ‰“åŒ…ä¸ºå­—èŠ‚
    # æ¯8ä¸ªå¸ƒå°”å€¼ï¼ˆ0æˆ–1ï¼‰æ‰“åŒ…ä¸º1ä¸ªå­—èŠ‚ï¼Œå®ç°8:1çš„å‹ç¼©
    packed_bytes = np.packbits(flat_mask)
    
    # è¿”å›æ‰“åŒ…åçš„å­—èŠ‚æ•°ç»„ã€åŸå§‹å½¢çŠ¶å’Œæ€»ä½æ•°
    return packed_bytes, original_shape, num_bits

def unpack_bits_to_bool_mask(packed_bytes, original_shape, num_bits, device='cpu'):
    """
    ä½çº§è§£åŒ…å‡½æ•°
    
    Args:
        packed_bytes: np.ndarray (uint8)ï¼Œæ‰“åŒ…åçš„å­—èŠ‚æ•°ç»„
        original_shape: tuple,ç›®æ ‡æ©ç å½¢çŠ¶
        num_bits: int,æœ‰æ•ˆä½æ•°
        device: str,ç›®æ ‡è®¾å¤‡
        
    Returns:
        torch.BoolTensor,æ¢å¤çš„å¸ƒå°”æ©ç 
    """
    # è§£åŒ…å­—èŠ‚ä¸ºä½
    unpacked_bits = np.unpackbits(packed_bytes)
    
    # æˆªå–å®é™…éœ€è¦çš„ä½æ•°ï¼ˆå› ä¸ºæœ€åä¸€ä¸ªå­—èŠ‚å¯èƒ½æœ‰å¡«å……ï¼‰
    unpacked_bits = unpacked_bits[:num_bits]
    
    # è½¬æ¢ä¸ºå¸ƒå°”å¼ é‡å¹¶æ¢å¤å½¢çŠ¶
    bool_mask = torch.from_numpy(unpacked_bits.astype(bool)).reshape(original_shape)
    
    return bool_mask.to(device)

def pack_sparse_residual(compressed_residual, enable_timing=True, use_bit_packing=True):
    """
    å°†ç¨€ç–æ®‹å·®Tensoræ‰“åŒ…æˆ (mask, values) çš„æ ¼å¼ï¼Œä»¥ä¾¿é«˜æ•ˆä¼ è¾“ã€‚
    æ”¯æŒä½çº§æ©ç æ‰“åŒ…

    Args:
        compressed_residual (dict): ç»è¿‡Top-Kå‹ç¼©çš„æ®‹å·®å­—å…¸,å€¼æ˜¯åŒ…å«å¤§é‡0çš„Tensorã€‚
        enable_timing (bool): æ˜¯å¦å¯ç”¨è®¡æ—¶åŠŸèƒ½
        use_bit_packing (bool): æ˜¯å¦ä½¿ç”¨ä½çº§æ©ç æ‰“åŒ…(é»˜è®¤å¯ç”¨)

    Returns:
        dict: æ‰“åŒ…åçš„æ®‹å·®å­—å…¸ã€‚
              æ ¼å¼ä¸º: { 'layer_name': {'mask': packed_data, 'values': torch.FloatTensor}, ... }
    """
    import time
    start_time = time.time() if enable_timing else None
    
    packed_residual = {}
    
    for key, sparse_tensor in compressed_residual.items():
        # åªå¤„ç†æµ®ç‚¹ç±»å‹çš„å‚æ•°
        if not sparse_tensor.dtype.is_floating_point:
            continue
        
        # 1. åˆ›å»º mask,å½¢çŠ¶ä¸ sparse_tensor å®Œå…¨ç›¸åŒ,æ¯ä¸ªä½ç½®çš„å€¼è¡¨ç¤º sparse_tensor ä¸­å¯¹åº”ä½ç½®æ˜¯å¦ä¸ºéé›¶
        mask = (sparse_tensor != 0)
        
        # 2. æå–éé›¶ values
        values = sparse_tensor[mask]
        #è¿™é‡Œå»é™¤æ‰€æœ‰çš„0å€¼ï¼Œvaluesæ˜¯ä¸€ä¸ªä¸€ç»´å¼ é‡ï¼ŒåŒ…å«æ‰€æœ‰éé›¶å…ƒç´ 
        
        # 3. å¦‚æœç¡®å®æœ‰éé›¶å€¼ï¼Œåˆ™æ‰“åŒ…
        if values.numel() > 0:
            #values.numel()è¿”å›å¼ é‡ä¸­å…ƒç´ çš„æ€»æ•°
            if use_bit_packing:
                #ä½¿ç”¨ä½çº§æ‰“åŒ…
                packed_mask, original_shape, num_bits = pack_bool_mask_to_bits(mask)
                
                packed_residual[key] = {
                    'mask_data': packed_mask,           # æ‰“åŒ…çš„å­—èŠ‚æ•°ç»„
                    'mask_shape': original_shape,       # åŸå§‹æ©ç å½¢çŠ¶
                    'mask_bits': num_bits,              # æœ‰æ•ˆä½æ•°
                    'values': values.cpu(),             # éé›¶å€¼æ•°ç»„
                    'is_bit_packed': True               # æ ‡è¯†ä½¿ç”¨äº†ä½æ‰“åŒ…
                }
            else:
                # ä¼ ç»Ÿæ–¹å¼ï¼Œæ¯ä¸ªå¸ƒå°”å€¼å 1å­—èŠ‚
                packed_residual[key] = {
                    'mask': mask.cpu(),                 # å¸ƒå°”æ©ç 
                    'values': values.cpu(),             # éé›¶å€¼æ•°ç»„
                    'is_bit_packed': False              # æ ‡è¯†æœªä½¿ç”¨ä½æ‰“åŒ…
                }
    
    if enable_timing and start_time is not None:
        pack_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        total_params = sum(torch.numel(tensor) for tensor in compressed_residual.values())
        
        return packed_residual, pack_time
    
    return packed_residual

def unpack_sparse_residual(packed_residual, template, enable_timing=False):
    """
    å°†æ‰“åŒ…åçš„ç¨€ç–æ®‹å·®æ•°æ®è§£åŒ…æˆåŸæ¥çš„æ ¼å¼ã€‚
    æ”¯æŒä½çº§æ©ç è§£åŒ…ï¼Œå…¼å®¹ä¼ ç»Ÿå¸ƒå°”æ©ç æ ¼å¼ã€‚

    Args:
        packed_residual (dict): å·²æ‰“åŒ…çš„æ®‹å·®å­—å…¸ã€‚æ”¯æŒä½çº§å’Œä¼ ç»Ÿæ ¼å¼
        template (dict): æ¨¡æ¿å­—å…¸ï¼Œæä¾›åŸå§‹å¼ é‡çš„å½¢çŠ¶å’Œè®¾å¤‡ä¿¡æ¯
        enable_timing (bool): æ˜¯å¦å¯ç”¨è®¡æ—¶åŠŸèƒ½

    Returns:
        dict: è§£åŒ…åçš„æ®‹å·®å­—å…¸ï¼Œæ ¼å¼ä¸åŸå§‹æ®‹å·®ç›¸åŒã€‚
    """
    import time
    start_time = time.time() if enable_timing else None
    
    unpacked_residual = {}
    #é¦–å…ˆç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½è¢«åˆå§‹åŒ–ä¸ºé›¶
    for key, template_tensor in template.items():
        unpacked_residual[key] = torch.zeros_like(template_tensor)
    
    # ç„¶ååªæ›´æ–°é‚£äº›åœ¨ packed_residual ä¸­å­˜åœ¨çš„å‚æ•°
    for key, packed_data in packed_residual.items():
        if key in template:
            target_device = template[key].device
            
            # é¦–å…ˆæ£€æŸ¥ packed_data æ˜¯å¦æ˜¯å­—å…¸ç±»å‹
            if isinstance(packed_data, dict):
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ä½çº§æ‰“åŒ…
                if packed_data.get('is_bit_packed', False):
                    # ä½çº§è§£åŒ…
                    mask = unpack_bits_to_bool_mask(
                        packed_data['mask_data'],
                        packed_data['mask_shape'],
                        packed_data['mask_bits'],
                        device=target_device
                    )
                    values = packed_data['values'].to(target_device)
                else:
                    # ä¼ ç»Ÿè§£åŒ…ï¼ˆå‘åå…¼å®¹ï¼‰
                    if 'mask' in packed_data:
                        mask = packed_data['mask'].to(target_device)
                        values = packed_data['values'].to(target_device)
                    else:
                        # å…¼å®¹æ›´æ—§çš„æ ¼å¼
                        continue
                
                # é‡æ„åŸå§‹å¼ é‡
                unpacked_residual[key][mask] = values
            else:
                # packed_data æ˜¯ Tensorï¼Œè¯´æ˜æ˜¯å¯†é›†æ ¼å¼ï¼Œç›´æ¥å¤åˆ¶
                # è¿™ç§æƒ…å†µå®é™…ä¸Šä¸åº”è¯¥å‡ºç°åœ¨ unpack_sparse_residual ä¸­
                # ä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬ç›´æ¥å¤åˆ¶å€¼
                unpacked_residual[key] = packed_data.to(target_device)
    
    if enable_timing and start_time is not None:
        unpack_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        total_params = sum(torch.numel(tensor) for tensor in unpacked_residual.values())
        print(f"   ğŸ“¦ è§£åŒ…è€—æ—¶: {unpack_time:.2f}ms (é‡å»º {total_params:,} å‚æ•°)")
        return unpacked_residual, unpack_time
    
    return unpacked_residual

def calculate_communication_cost_dict(packed_residual):
    """
    è®¡ç®— (mask, values) æ–¹å¼çš„é€šä¿¡æˆæœ¬ï¼Œæ”¯æŒä½çº§æ©ç ä¼˜åŒ–ã€‚

    Args:
        packed_residual (dict): æ‰“åŒ…åçš„æ®‹å·®å­—å…¸ï¼Œæ”¯æŒä½çº§å’Œä¼ ç»Ÿæ ¼å¼
        
    Returns:
        tuple: (comm_cost, client_transmitted_bytes, layer_details)
    """
    client_transmitted_bytes = 0
    layer_details = []
    
    # å¤„ç† (mask, values) æ•°æ®
    for key, data in packed_residual.items():
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ä½çº§æ‰“åŒ…
            if data.get('is_bit_packed', False):
                # ä½çº§æ‰“åŒ…æ ¼å¼
                mask_data = data['mask_data']
                mask_shape = data['mask_shape']
                mask_bits = data['mask_bits']
                values = data['values']
                
                total_params = mask_bits  # æ€»å‚æ•°æ•°é‡ç­‰äºä½æ•°
                nonzero_params = torch.numel(values)
                
                # ä½çº§æ©ç çš„å­—èŠ‚æ•°ï¼ˆå·²ç»æ˜¯å®é™…ä¼ è¾“çš„å­—èŠ‚æ•°ï¼‰
                mask_bytes = len(mask_data)  # numpyæ•°ç»„çš„å®é™…å­—èŠ‚æ•°
                
                # æ ¹æ®å®é™…æ•°æ®ç±»å‹è®¡ç®—éé›¶å€¼çš„å­—èŠ‚æ•°
                dtype_size = values.element_size()
                values_bytes = nonzero_params * dtype_size
                
                layer_bytes = mask_bytes + values_bytes
                client_transmitted_bytes += layer_bytes
                
                sparsity = (total_params - nonzero_params) / total_params * 100 if total_params > 0 else 0
                
                layer_details.append({
                    'layer_name': key,
                    'total_params': total_params,
                    'nonzero_params': nonzero_params,
                    'sparsity': sparsity,
                    'transmitted_bytes': layer_bytes,
                    'mask_bytes': mask_bytes,
                    'values_bytes': values_bytes,
                    'bit_packed': True  # æ ‡è®°ä½¿ç”¨äº†ä½çº§ä¼˜åŒ–
                })
                
            elif 'mask' in data and 'values' in data:
                # ä¼ ç»Ÿæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                mask = data['mask']
                values = data['values']
                
                total_params = torch.numel(mask)
                nonzero_params = torch.numel(values)
                
                # ä¼ ç»Ÿæ–¹å¼ï¼šæ¯ä¸ªå¸ƒå°”å€¼1å­—èŠ‚
                mask_bytes = total_params * mask.element_size()  # é€šå¸¸æ˜¯1å­—èŠ‚
                
                # æ ¹æ®å®é™…æ•°æ®ç±»å‹è®¡ç®—éé›¶å€¼çš„å­—èŠ‚æ•°
                dtype_size = values.element_size()
                values_bytes = nonzero_params * dtype_size
                
                layer_bytes = mask_bytes + values_bytes
                client_transmitted_bytes += layer_bytes
                
                sparsity = (total_params - nonzero_params) / total_params * 100 if total_params > 0 else 0
                
                layer_details.append({
                    'layer_name': key,
                    'total_params': total_params,
                    'nonzero_params': nonzero_params,
                    'sparsity': sparsity,
                    'transmitted_bytes': layer_bytes,
                    'mask_bytes': mask_bytes,
                    'values_bytes': values_bytes,
                    'bit_packed': False  # æ ‡è®°æœªä½¿ç”¨ä½çº§ä¼˜åŒ–
                })
    
    # ä¸ºäº†ä¸ä¹‹å‰ä»£ç å…¼å®¹ï¼Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªç­‰æ•ˆçš„"å‚æ•°é‡"
    # è¿™é‡Œæˆ‘ä»¬å®šä¹‰1ä¸ª"å‚æ•°å•ä½"= 4å­—èŠ‚ (float32)
    comm_cost_in_params = client_transmitted_bytes / 4.0
    
    return comm_cost_in_params, client_transmitted_bytes, layer_details

def print_round_communication_stats(enable_compression, epoch_comm_cost, total_nonzero_values, 
                                   total_original_params, num_selected_clients, 
                                   single_model_params, current_compression_ratio):
    """
    æ‰“å°è½®æ¬¡é€šä¿¡é‡ç»Ÿè®¡ä¿¡æ¯ - ç®€åŒ–ç‰ˆ
    
    Args:
        enable_compression (bool): æ˜¯å¦å¯ç”¨å‹ç¼©
        epoch_comm_cost (int): è½®æ¬¡é€šä¿¡æˆæœ¬
        total_nonzero_values (int): æ€»éé›¶å‚æ•°æ•°é‡
        total_original_params (int): æ€»åŸå§‹å‚æ•°æ•°é‡
        num_selected_clients (int): é€‰ä¸­çš„å®¢æˆ·ç«¯æ•°é‡
        single_model_params (int): å•ä¸ªæ¨¡å‹å‚æ•°æ•°é‡
        current_compression_ratio (float): å½“å‰å‹ç¼©ç‡
    """
    if enable_compression:
        # è®¡ç®—å‹ç¼©æ•ˆæœç»Ÿè®¡
        total_baseline_params = single_model_params * num_selected_clients
        compression_effectiveness = (1 - epoch_comm_cost / total_baseline_params) * 100
        
        # å­—èŠ‚çº§ç»Ÿè®¡
        total_param_bytes = total_nonzero_values * 4     # float32å‚æ•°å€¼å­—èŠ‚æ•°
        baseline_bytes = total_baseline_params * 4  # åŸºå‡†å­—èŠ‚æ•°
        bytes_compression_effectiveness = (1 - total_param_bytes / baseline_bytes) * 100
        
        print(f"   ğŸ“¡ åŒå‘uniformå‹ç¼©é€šä¿¡é‡:")
        print(f"      â€¢ ä¼ é€’å‚æ•°æ€»æ•°: {total_nonzero_values:,} å‚æ•° ({total_param_bytes:,} å­—èŠ‚)")
        print(f"   ğŸ—œï¸ å‹ç¼©æ•ˆæœ: å‡å°‘ {compression_effectiveness:.1f}% é€šä¿¡é‡")
        print(f"   ğŸ“Š åŸºå‡†é€šä¿¡é‡: {total_baseline_params:,} å‚æ•° = {baseline_bytes:,} å­—èŠ‚")
        
        # å®é™…å‹ç¼©ç‡åˆ†æ
        actual_compression_ratio = total_nonzero_values / total_baseline_params
        print(f"   ğŸ” å®é™…å‹ç¼©ç‡: {actual_compression_ratio:.3f} (è®¾å®š: {current_compression_ratio:.3f})")
    else:
        dense_bytes = epoch_comm_cost * 4  # å¯†é›†ä¼ è¾“
        print(f"   ğŸ“¡ å¯†é›†é€šä¿¡é‡: {epoch_comm_cost:,} å‚æ•° = {dense_bytes:,} å­—èŠ‚")


def print_final_compression_stats(enable_compression, total_comm_cost, total_rounds, 
                                   single_model_params, avg_selected_clients, compression_ratio):
    """
    æ‰“å°æœ€ç»ˆçš„åŒå‘uniformå‹ç¼©ç»Ÿè®¡ä¿¡æ¯ - ç®€åŒ–ç‰ˆ
    """
    print("\nğŸ“Š æœ€ç»ˆå‹ç¼©ç»Ÿè®¡:")
    if enable_compression:
        theoretical_baseline_total = int(single_model_params * avg_selected_clients * total_rounds)
        compression_effectiveness = (1 - total_comm_cost / theoretical_baseline_total) * 100
        actual_compression_ratio = total_comm_cost / theoretical_baseline_total

        print(f"   â€¢ ç†è®ºåŸºå‡†é€šä¿¡é‡: {theoretical_baseline_total:,} å‚æ•°")
        print(f"   â€¢ å®é™…æ€»é€šä¿¡é‡: {total_comm_cost:,} å‚æ•°")
        print(f"   â€¢ åŒå‘uniformå‹ç¼©æ•ˆæœ: å‡å°‘ {compression_effectiveness:.2f}% é€šä¿¡é‡")
        print(f"   â€¢ å®é™…å¹³å‡å‹ç¼©ç‡: {actual_compression_ratio:.3f}")
        print(f"   â€¢ è®¾å®šåŒå‘å‹ç¼©ç‡: {compression_ratio:.3f}")
    else:
        print(f"   â€¢ æ€»é€šä¿¡é‡: {total_comm_cost:,} å‚æ•° (æœªå¯ç”¨å‹ç¼©)")