#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6  # æŒ‡å®šPythonè§£é‡Šå™¨åŠç¼–ç æ ¼å¼ï¼ˆå…¼å®¹ä¸­æ–‡æ³¨é‡Šï¼‰

# è®¾ç½®ç¼–ç 
import sys
import locale
import os

# å¼ºåˆ¶è®¾ç½®UTF-8ç¼–ç 
if sys.platform.startswith('win'):
    os.system('chcp 65001 > nul')  # è®¾ç½®Windowsæ§åˆ¶å°ä¸ºUTF-8
    
# ç¡®ä¿printè¾“å‡ºä½¿ç”¨UTF-8
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')

# å¯¼å…¥ä¾èµ–åº“
from tqdm import tqdm       # è¿›åº¦æ¡æ˜¾ç¤ºå·¥å…·
import matplotlib.pyplot as plt  # å¯è§†åŒ–ç»˜å›¾åº“
import pandas as pd         # æ•°æ®å¤„ç†å’Œä¿å­˜CSV
import torch
import sys
from datetime import datetime

# è®¾ç½®è¾“å‡ºç¼–ç 
if hasattr(sys.stdout, 'reconfigure'):
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

from torch.utils.data import DataLoader  # æ•°æ®æ‰¹é‡åŠ è½½å·¥å…·

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from utils import get_dataset    # æ•°æ®é›†åŠ è½½å·¥å…·å‡½æ•°
from options import args_parser # å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
from update import test_inference  # æµ‹è¯•æ¨ç†å‡½æ•°
from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar, CNNCifar100, ResNet18Fed, replace_bn_with_gn  # è‡ªå®šä¹‰æ¨¡å‹å®šä¹‰
from model_factory import get_model  # æ–°çš„æ¨¡å‹å·¥å‚

'''
è¯¥æ–‡ä»¶å®ç°äº†ä¸€ä¸ªä¼ ç»Ÿçš„ã€éè”é‚¦çš„ï¼ˆä¸­å¿ƒåŒ–ï¼‰æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæµç¨‹ã€‚å®ƒé€šå¸¸ç”¨ä½œè”é‚¦å­¦ä¹ æ€§èƒ½çš„åŸºå‡† (baseline)ã€‚
ä»£ç ä¼šåŠ è½½æ•°æ®ï¼Œæ„å»ºæ¨¡å‹ï¼Œåœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ã€‚
'''

if __name__ == '__main__':
    # å‚æ•°è§£æä¸è®¾å¤‡é…ç½®
    args = args_parser()  # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆå¦‚æ¨¡å‹ç±»å‹ã€æ•°æ®é›†ã€epochæ•°ç­‰ï¼‰
    if args.gpu:          # è‹¥å¯ç”¨GPUåŠ é€Ÿ
        torch.cuda.set_device(int(args.gpu))  # æŒ‡å®šGPUè®¾å¤‡ç¼–å·
        #åŠ äº†intä¹‹åå°±ä¸ç”¨åƒä¹‹å‰ä¸€æ ·åœ¨å‘½ä»¤è¡Œè¾“å…¥gpu=cuda:0äº†,åªç”¨å†™gpu=0
    device = 'cuda' if args.gpu else 'cpu'  # ç¡®å®šè®¡ç®—è®¾å¤‡ï¼ˆGPU/CPUï¼‰

    # åŠ è½½æ•°æ®é›†
    train_dataset, test_dataset, _ = get_dataset(args)  # è·å–è®­ç»ƒé›†ã€æµ‹è¯•é›†åŠå¯èƒ½çš„é¢å¤–ä¿¡æ¯
    
    # æ„å»ºæ¨¡å‹
    print(f"æ­£åœ¨æ„å»ºåŸºçº¿æ¨¡å‹: {args.model} for dataset: {args.dataset}")
    
    # é¦–å…ˆå°è¯•ä½¿ç”¨æ–°çš„ä¼˜åŒ–æ¨¡å‹
    try:
        if args.model in ['cnn']:
            # ä½¿ç”¨æ–°çš„æ ‡å‡†CNNæ¨¡å‹
            if args.dataset == 'mnist':
                global_model = get_model('mnist', 'cnn')  # CNN_MNIST
            elif args.dataset == 'cifar':
                global_model = get_model('cifar10', 'cnn')  # CNNCifar
            else:
                raise ValueError(f"æ ‡å‡†CNNä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
                
        elif args.model in ['optimized', 'cnn_optimized']:
            # ä½¿ç”¨æ–°çš„ä¼˜åŒ–CNNæ¨¡å‹
            if args.dataset == 'mnist':
                global_model = get_model('mnist', 'optimized')  # CNN_MNIST_Optimized
            else:
                raise ValueError(f"ä¼˜åŒ–CNNç›®å‰ä»…æ”¯æŒMNISTæ•°æ®é›†")
                
        elif args.model in ['resnet18', 'resnet']:
            # ä½¿ç”¨ResNetæ¨¡å‹
            if args.dataset == 'cifar':
                global_model = get_model('cifar10', 'resnet18_fed')  # ResNet18_CIFAR10_Fed 
            elif args.dataset == 'cifar100':
                global_model = get_model('cifar100', 'resnet18_fed')  # ResNet18_CIFAR100_Fed
            else:
                raise ValueError(f"ResNet18ä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
                
        elif args.model in ['efficientnet']:
            # ä½¿ç”¨EfficientNetæ¨¡å‹
            if args.dataset == 'cifar':
                global_model = get_model('cifar10', 'efficientnet')  # EfficientNet_CIFAR10
            elif args.dataset == 'cifar100':
                global_model = get_model('cifar100', 'efficientnet')  # EfficientNet_CIFAR100
            else:
                raise ValueError(f"EfficientNetä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
                
        elif args.model == 'densenet':
            # ä½¿ç”¨DenseNetæ¨¡å‹
            if args.dataset == 'cifar100':
                global_model = get_model('cifar100', 'densenet')  # DenseNet_CIFAR100
            else:
                raise ValueError(f"DenseNetä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
        else:
            raise ValueError("å°è¯•åŸæœ‰æ¨¡å‹")
            
        print(f"[SUCCESS] æˆåŠŸåŠ è½½ä¼˜åŒ–æ¨¡å‹: {global_model.__class__.__name__}")
        
    except Exception as e:
        print(f"[WARNING] ä¼˜åŒ–æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"[INFO] å›é€€åˆ°åŸæœ‰æ¨¡å‹...")
        
        # å›é€€åˆ°åŸæœ‰æ¨¡å‹æ„å»ºé€»è¾‘
        if args.model == 'cnn':  # å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹åˆ†æ”¯
            # æ ¹æ®æ•°æ®é›†é€‰æ‹©ä¸åŒCNNç»“æ„
            if args.dataset == 'mnist':      # MNISTæ‰‹å†™æ•°å­—è¯†åˆ«
                global_model = CNNMnist(args=args)  # 28x28ç°åº¦å›¾è¾“å…¥çš„ç½‘ç»œ
            elif args.dataset == 'fmnist':   # Fashion-MNISTæœè£…åˆ†ç±»
                global_model = CNNFashion_Mnist(args=args)
            elif args.dataset == 'cifar':    # CIFAR-10å›¾åƒåˆ†ç±»
                global_model = CNNCifar(args=args)  # é€‚ç”¨äº32x32å½©è‰²å›¾çš„CNN
            elif args.dataset == 'cifar100': # CIFAR-100å›¾åƒåˆ†ç±»
                global_model = CNNCifar100(args=args)  # é€‚ç”¨äº32x32å½©è‰²å›¾çš„æ·±åº¦CNN
        elif args.model == 'resnet':  # ResNet18Fedæ¨¡å‹åˆ†æ”¯
            # ResNet for better performance
            if args.dataset == 'cifar':
                global_model = ResNet18Fed(num_classes=args.num_classes)
                global_model = replace_bn_with_gn(global_model)  # ä½¿ç”¨GroupNorm
            elif args.dataset == 'cifar100':
                global_model = ResNet18Fed(num_classes=100)
                global_model = replace_bn_with_gn(global_model)
            else:
                print(f"ResNet not implemented for dataset {args.dataset}, using CNN instead")
                if args.dataset == 'mnist':
                    global_model = CNNMnist(args=args)
                elif args.dataset == 'fmnist':
                    global_model = CNNFashion_Mnist(args=args)
        elif args.model == 'mlp':  # å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹åˆ†æ”¯
            img_size = train_dataset[0][0].shape  # è·å–è¾“å…¥å›¾åƒå°ºå¯¸
            len_in = 1
            for x in img_size:  # è®¡ç®—è¾“å…¥å±‚ç»´åº¦ï¼ˆå±•å¹³åçš„åƒç´ æ€»æ•°ï¼‰
                len_in *= x
            global_model = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes)
        else:
            exit('Error: unrecognized model')  # æ¨¡å‹ç±»å‹é”™è¯¯å¤„ç†
    
    # æ¨¡å‹é…ç½®
    global_model.to(device)    # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰
    global_model.train()       # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨BN/Dropoutç­‰å±‚ï¼‰
    print(global_model)        # æ‰“å°æ¨¡å‹ç»“æ„
    
    # è®­ç»ƒé…ç½®
    # ä¼˜åŒ–å™¨é€‰æ‹©ï¼ˆæ”¯æŒSGDã€Adamã€AdamWï¼‰
    if args.optimizer == 'sgd':
        optimizer = torch.optim.SGD(global_model.parameters(), 
                                  lr=args.lr, 
                                  momentum=args.momentum, 
                                  weight_decay=args.weight_decay)
    elif args.optimizer == 'adam':
        optimizer = torch.optim.Adam(global_model.parameters(), 
                                   lr=args.lr, 
                                   betas=(args.adam_beta1, args.adam_beta2),
                                   eps=args.adam_eps,
                                   weight_decay=args.weight_decay)
    elif args.optimizer == 'adamw':
        optimizer = torch.optim.AdamW(global_model.parameters(), 
                                    lr=args.lr, 
                                    betas=(args.adam_beta1, args.adam_beta2),
                                    eps=args.adam_eps,
                                    weight_decay=args.weight_decay)
    else:
        raise ValueError(f"Unsupported optimizer: {args.optimizer}")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None
    if args.lr_scheduler in ['none', 'fixed']:
        scheduler = None  # å›ºå®šå­¦ä¹ ç‡ï¼Œä¸ä½¿ç”¨è°ƒåº¦å™¨
    elif args.lr_scheduler == 'step':
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 
                                                  step_size=args.lr_step_size, 
                                                  gamma=args.lr_gamma)
    elif args.lr_scheduler == 'exp':
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 
                                                         gamma=args.lr_gamma)
    elif args.lr_scheduler == 'cosine':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 
                                                             T_max=args.cosine_t_max,
                                                             eta_min=getattr(args, 'eta_min', 1e-6))
    else:
        print(f"è­¦å‘Š: æœªçŸ¥çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ '{args.lr_scheduler}'ï¼Œå°†ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡")
        scheduler = None
    
    # æ•°æ®åŠ è½½å™¨ï¼ˆæ‰¹å¤„ç†+éšæœºæ‰“ä¹±ï¼‰
    trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    # éªŒè¯é›†æ•°æ®åŠ è½½å™¨ - ç”¨äºæ—©åœ
    val_size = int(0.1 * len(train_dataset))
    train_size = len(train_dataset) - val_size
    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])
    trainloader = DataLoader(train_subset, batch_size=64, shuffle=True)
    valloader = DataLoader(val_subset, batch_size=64, shuffle=False)
    
    # æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µæŸå¤±ï¼ˆé€‚ç”¨äºåŸå§‹logitsè¾“å‡ºï¼‰
    criterion = torch.nn.CrossEntropyLoss().to(device)
    epoch_loss = []  # è®°å½•æ¯è½®å¹³å‡æŸå¤±
    val_losses = []  # è®°å½•éªŒè¯æŸå¤±
    
    # --- ä¿®æ”¹å¼€å§‹ï¼šåˆå§‹åŒ–å†å²è®°å½• ---
    history = {
        'epoch': [],
        'test_accuracy': [],
        'train_loss': []
    }
    # --- ä¿®æ”¹ç»“æŸ ---
    
    # æ—©åœæœºåˆ¶å‚æ•°
    best_val_loss = float('inf')
    patience = 5  # è¿ç»­å¤šå°‘ä¸ªepochéªŒè¯æŸå¤±ä¸ä¸‹é™å°±åœæ­¢
    patience_counter = 0

    # è®­ç»ƒå¾ªç¯[6,8](@ref)
    for epoch in range(args.epochs):  # ç§»é™¤tqdmï¼Œä½¿ç”¨è¯¦ç»†è¾“å‡º
        batch_loss = []
        
        # è®­ç»ƒé˜¶æ®µ
        global_model.train()
        # éå†è®­ç»ƒæ•°æ®æ‰¹æ¬¡
        for batch_idx, (images, labels) in enumerate(trainloader):
            images, labels = images.to(device), labels.to(device)  # æ•°æ®é€è®¾å¤‡
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()       # æ¸…ç©ºæ¢¯åº¦ï¼ˆé¿å…ç´¯ç§¯ï¼‰
            outputs = global_model(images)  # æ¨¡å‹æ¨ç†
            loss = criterion(outputs, labels)  # è®¡ç®—æŸå¤±
            
            # åå‘ä¼ æ’­ä¸ä¼˜åŒ–
            loss.backward()             # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            optimizer.step()            # æ›´æ–°æ¨¡å‹å‚æ•°
            
            # æ¯50ä¸ªæ‰¹æ¬¡æ‰“å°è®­ç»ƒçŠ¶æ€ - ä½¿ç”¨æ‚¨è¦æ±‚çš„è¯¦ç»†æ ¼å¼
            if batch_idx % 50 == 0:
                print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                    epoch + 1, 1, batch_idx * len(images), len(trainloader.dataset),
                    100. * batch_idx / len(trainloader), loss.item()))
            batch_loss.append(loss.item())  # è®°å½•å½“å‰æ‰¹æ¬¡æŸå¤±
        
        # è®¡ç®—å¹¶è®°å½•å½“å‰epochå¹³å‡è®­ç»ƒæŸå¤±
        loss_avg = sum(batch_loss) / len(batch_loss)
        print(f'\nTrain loss: {loss_avg}')
        epoch_loss.append(loss_avg)
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        if scheduler is not None:
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]
            print(f'Learning rate updated to: {current_lr:.6f}')
        
        # éªŒè¯é˜¶æ®µ
        global_model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for images, labels in valloader:
                images, labels = images.to(device), labels.to(device)
                outputs = global_model(images)
                val_loss += criterion(outputs, labels).item()
        
        val_loss /= len(valloader)
        val_losses.append(val_loss)
        print(f'Validation loss: {val_loss:.6f}')
        
        # è®¡ç®—æµ‹è¯•å‡†ç¡®ç‡ç”¨äºè®°å½•
        test_acc, test_loss = test_inference(args, global_model, test_dataset)
        
        # --- ä¿®æ”¹å¼€å§‹ï¼šè®°å½•æœ¬è½®æ¬¡çš„æŒ‡æ ‡ ---
        history['epoch'].append(epoch + 1)
        history['test_accuracy'].append(test_acc)
        history['train_loss'].append(loss_avg)
        # --- ä¿®æ”¹ç»“æŸ ---
        
        # æ—©åœæ£€æŸ¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            print(f'New best validation loss: {val_loss:.6f}')
        else:
            patience_counter += 1
            print(f'Validation loss did not improve. Patience: {patience_counter}/{patience}')
            
        if patience_counter >= patience:
            print(f'Early stopping triggered after {epoch+1} epochs')
            break

    # å¯è§†åŒ–è®­ç»ƒæŸå¤±æ›²çº¿
    print(f'\nTraining completed after {len(epoch_loss)} epochs')
    print(f'Final training loss: {epoch_loss[-1]:.6f}')
    print(f'Final validation loss: {val_losses[-1]:.6f}')
    print(f'Best validation loss: {best_val_loss:.6f}')
    
    # æ¨¡å‹æµ‹è¯•è¯„ä¼°
    test_acc, test_loss = test_inference(args, global_model, test_dataset)
    print(f'Test on {len(test_dataset)} samples')
    print(f"Test Accuracy: {100*test_acc:.2f}%")  # è¾“å‡ºæµ‹è¯•å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰

    # --- ä¿®æ”¹å¼€å§‹ï¼šå°†å†å²è®°å½•ä¿å­˜ä¸ºCSVæ–‡ä»¶ ---
    # åˆ›å»ºæ—¶é—´æˆ³å‘½åçš„æ–‡ä»¶å¤¹
    current_time = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = os.path.join('./save/logs', current_time)
    os.makedirs(log_dir, exist_ok=True)

    # CSVæ–‡ä»¶åä¸æ–‡ä»¶å¤¹åä¿æŒä¸€è‡´ï¼ˆéƒ½ä½¿ç”¨æ—¶é—´æˆ³ï¼‰
    log_filename = f'{current_time}.csv'
    log_path = os.path.join(log_dir, log_filename)
    
    # ä¿å­˜å®éªŒè¯¦æƒ…åˆ°åŒä¸€æ–‡ä»¶å¤¹
    iid_str = 'iid' if args.iid else 'noniid'
    details_content = f"""å®éªŒæ—¶é—´: {current_time}
å®éªŒç±»å‹: Baseline Centralized Learning
æ•°æ®é›†: {args.dataset.upper()}
æ¨¡å‹: {args.model.upper()}
è®­ç»ƒè½®æ•°: {args.epochs}
æ•°æ®åˆ†å¸ƒ: {iid_str.upper()}
å­¦ä¹ ç‡: {args.lr}
æ‰¹æ¬¡å¤§å°: {args.local_bs}
"""
    details_path = os.path.join(log_dir, 'experiment_details.txt')
    with open(details_path, 'w', encoding='utf-8') as f:
        f.write(details_content)

    df = pd.DataFrame(history)
    df.to_csv(log_path, index=False)
    print(f"ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {log_path}")
    print(f"ğŸ“‹ å®éªŒè¯¦æƒ…å·²ä¿å­˜åˆ°: {details_path}")
    
    # è‡ªåŠ¨ç”Ÿæˆå›¾åƒ
    try:
        from visualize_results import plot_single_experiment
        plots_dir = './save/plots'
        plot_result = plot_single_experiment(log_path, plots_dir)
        if plot_result:
            print(f"ğŸ“Š å®éªŒå›¾åƒå·²è‡ªåŠ¨ç”Ÿæˆåˆ°: {plot_result}")
        else:
            print("âš ï¸ å›¾åƒç”Ÿæˆå¤±è´¥")
    except Exception as e:
        print(f"âš ï¸ è‡ªåŠ¨ç”Ÿæˆå›¾åƒæ—¶å‡ºé”™: {e}")
        print("ğŸ’¡ ä½ å¯ä»¥æ‰‹åŠ¨è¿è¡Œ: python visualize_results.py --single " + log_path)
    # --- ä¿®æ”¹ç»“æŸ ---

