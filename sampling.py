#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6

'''
è¿™ä¸ªæ–‡ä»¶è´Ÿè´£å°†æ•°æ®é›†åˆ’åˆ†ç»™ä¸åŒçš„å®¢æˆ·ç«¯ï¼Œæ¨¡æ‹Ÿè”é‚¦å­¦ä¹ ä¸­çš„æ•°æ®åˆ†å¸ƒæƒ…å†µã€‚
å®ƒåŒ…å«äº†ä¸º MNIST å’Œ CIFAR-10 æ•°æ®é›†ç”Ÿæˆç‹¬ç«‹åŒåˆ†å¸ƒ (IID) å’Œéç‹¬ç«‹åŒåˆ†å¸ƒ (Non-IID) æ•°æ®çš„å‡½æ•°ã€‚
Non-IID çš„æƒ…å†µè¿›ä¸€æ­¥åŒºåˆ†ä¸ºæ•°æ®é‡å‡è¡¡å’Œä¸å‡è¡¡çš„åœºæ™¯ã€‚
'''

import numpy as np # å¯¼å…¥ NumPy ç”¨äºæ•°å€¼è®¡ç®—ï¼Œç‰¹åˆ«æ˜¯æ•°ç»„æ“ä½œ
from torchvision import datasets, transforms # ä» torchvision å¯¼å…¥æ•°æ®é›†å’Œå›¾åƒå˜æ¢å·¥å…·
import torch
from torch.utils.data import Dataset

class DatasetSplit(Dataset):
    """æ•°æ®é›†åˆ†å‰²ç±»"""
    def __init__(self, dataset, idxs):
        self.dataset = dataset
        self.idxs = [int(i) for i in idxs]

    def __len__(self):
        return len(self.idxs)

    def __getitem__(self, item):
        image, label = self.dataset[self.idxs[item]]
        # ä½¿ç”¨clone().detach()ä»£æ›¿torch.tensor()æ¥é¿å…è­¦å‘Š
        if isinstance(image, torch.Tensor):
            image = image.clone().detach()
        else:
            image = torch.tensor(image)
            
        if isinstance(label, torch.Tensor):
            label = label.clone().detach()
        else:
            label = torch.tensor(label)
            
        return image, label

def mnist_iid(dataset, num_users): # ä¸º MNIST æ•°æ®é›†è¿›è¡Œ IID åˆ’åˆ†
    """
    Sample I.I.D. client data from MNIST dataset
    :param dataset: æ•´ä¸ªè®­ç»ƒæ•°æ®é›†
    :param num_users: å®¢æˆ·ç«¯æ•°é‡
    :return: dict of image index, å­—å…¸,é”®æ˜¯ç”¨æˆ·ID,å€¼æ˜¯è¯¥ç”¨æˆ·æ‹¥æœ‰çš„æ•°æ®æ ·æœ¬ç´¢å¼•é›†åˆ
    """
    num_items = int(len(dataset)/num_users) # è®¡ç®—æ¯ä¸ªç”¨æˆ·å¹³å‡åˆ†é…åˆ°çš„æ•°æ®é¡¹æ•°é‡
    dict_users, all_idxs = {}, [i for i in range(len(dataset))] # åˆå§‹åŒ–ç”¨æˆ·å­—å…¸å’Œæ‰€æœ‰æ•°æ®æ ·æœ¬çš„ç´¢å¼•åˆ—è¡¨
    for i in range(num_users):
        # np.random.choice ä» all_idxs ä¸­ä¸é‡å¤åœ°éšæœºé€‰æ‹© num_items ä¸ªç´¢å¼•
        dict_users[i] = set(np.random.choice(all_idxs, num_items,
                                             replace=False))
        #replaceè¡¨ç¤ºéšæœºé€‰æ‹©æ—¶ä¸å…è®¸é‡å¤é€‰æ‹©ã€‚æ¢å¥è¯è¯´ï¼ŒåŒä¸€ä¸ªç´¢å¼•ä¸èƒ½è¢«åŒä¸€ä¸ªç”¨æˆ·é€‰æ‹©ä¸¤æ¬¡ã€‚è¿™ç¡®ä¿äº†åˆ†é…ç»™æ¯ä¸ªç”¨æˆ·çš„ç´¢å¼•æ˜¯å”¯ä¸€çš„ã€‚
        # ä» all_idxs ä¸­ç§»é™¤å·²ç»åˆ†é…ç»™ç”¨æˆ·çš„ç´¢å¼•
        all_idxs = list(set(all_idxs) - dict_users[i])
    return dict_users
'''
æœ€ç»ˆæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ç»“æ„å¦‚ä¸‹ï¼š
é”®(key):ç”¨æˆ·ç¼–å·int,0 ~ num_users-1
å€¼(value):è¯¥ç”¨æˆ·æ‹¥æœ‰çš„æ•°æ®æ ·æœ¬ç´¢å¼•çš„é›†åˆ(set,é‡Œé¢æ˜¯ int)
å¯èƒ½æ˜¯
{
    0: {2, 5, 8},
    1: {0, 3, 7},
    2: {1, 4, 6}
}
'''


def mnist_noniid(dataset, num_users): # ä¸º MNIST æ•°æ®é›†è¿›è¡Œ Non-IID åˆ’åˆ† (æ¯ä¸ªç”¨æˆ·å›ºå®šåˆ†ç‰‡æ•°)
    """
    Sample non-I.I.D client data from MNIST dataset (Robust Version)
    
    ä½¿ç”¨"æ´—ç‰Œåè½®æµåˆ†å‘"ç­–ç•¥ï¼Œç¡®ä¿åˆ†ç‰‡åˆ†é…çš„å…¬å¹³æ€§å’Œå”¯ä¸€æ€§ã€‚
    
    :param dataset: MNISTæ•°æ®é›†
    :param num_users: å®¢æˆ·ç«¯æ•°é‡
    :return: å®¢æˆ·ç«¯æ•°æ®ç´¢å¼•å­—å…¸
    """
    # MNIST è®­ç»ƒé›† 60,000 å¼ å›¾ç‰‡ --> å‡è®¾æ¯ä¸ªåˆ†ç‰‡ 300 å¼ å›¾ç‰‡ï¼Œå…± 200 ä¸ªåˆ†ç‰‡
    # "shard" æŒ‡çš„æ˜¯å°†æ•°æ®åº“æ•°æ®åˆ†å‰²æˆå¤šä¸ªç‰‡æ®µæˆ–å­é›†ã€‚
    # æ¯ç‰‡å›¾ç‰‡çš„æ ‡ç­¾æ˜¯ç›¸é‚»çš„ï¼ˆå³åŒä¸€ç‰‡å¤§æ¦‚ç‡å±äºåŒä¸€ç±»åˆ«ï¼‰ï¼Œè¿™æ ·æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®åˆ†å¸ƒå°±ä¸æ˜¯IIDï¼Œè€Œæ˜¯åå‘æŸå‡ ä¸ªç±»åˆ«
    num_shards, num_imgs = 200, 300
    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}
    idxs = np.arange(num_shards * num_imgs)  # æ‰€æœ‰æ•°æ®æ ·æœ¬çš„åŸå§‹ç´¢å¼• (0 åˆ° 59999)
    
    # è·å–è®­ç»ƒæ•°æ®çš„æ ‡ç­¾ (å…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬)
    # ä¼˜å…ˆä½¿ç”¨ .targets å±æ€§ (æ–°ç‰ˆtorchvision)ï¼Œå¦åˆ™å›é€€åˆ° .train_labels
    labels = np.array(dataset.targets if hasattr(dataset, 'targets') else dataset.train_labels)
    
    # sort labels (å…³é”®æ­¥éª¤ï¼šæŒ‰æ ‡ç­¾æ’åºæ•°æ®ï¼Œä»¥åˆ›å»º Non-IID åˆ†å¸ƒ)
    idxs_labels = np.vstack((idxs, labels))  # å°†ç´¢å¼•å’Œæ ‡ç­¾å‚ç›´å †å 
    # argsort() è¿”å›æ’åºåçš„ç´¢å¼•ï¼Œè¿™é‡ŒæŒ‰ç¬¬äºŒè¡Œ (æ ‡ç­¾) æ’åº
    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]
    # æŒ‰æ ‡ç­¾æ’åºï¼Œä¿è¯åŒä¸€ç±»åˆ«çš„å›¾ç‰‡ç´¢å¼•æ’åœ¨ä¸€èµ·
    idxs = idxs_labels[0, :]  # è·å–æ’åºåçš„æ•°æ®æ ·æœ¬ç´¢å¼•

    # --- BEGIN: Recommended Change ---
    
    # åˆ›å»ºæ‰€æœ‰åˆ†ç‰‡çš„ç´¢å¼•åˆ—è¡¨
    shard_idxs = list(range(num_shards))
    
    # éšæœºæ‰“ä¹±åˆ†ç‰‡ç´¢å¼•
    np.random.shuffle(shard_idxs)
    
    # ä½¿ç”¨è½®æµåˆ†å‘ (round-robin) çš„æ–¹å¼å°†æ‰“ä¹±åçš„åˆ†ç‰‡åˆ†é…ç»™å®¢æˆ·ç«¯
    for i in range(num_users):
        # è®¡ç®—å½“å‰ç”¨æˆ·åº”è¯¥åˆ†é…åˆ°çš„åˆ†ç‰‡
        assigned_shards = shard_idxs[i::num_users]  # ä»ç¬¬iä¸ªå¼€å§‹ï¼Œæ¯éš”num_usersä¸ªå–ä¸€ä¸ª
        
        # å°†è¿™äº›åˆ†ç‰‡ä¸­çš„æ•°æ®ç´¢å¼•åˆ†é…ç»™ç”¨æˆ·
        for shard_id in assigned_shards:
            start_idx = shard_id * num_imgs
            end_idx = start_idx + num_imgs
            dict_users[i] = np.concatenate(
                (dict_users[i], idxs[start_idx:end_idx]), axis=0)

    # æ‰“å°åˆ†é…ä¿¡æ¯
    shards_per_client_list = [len(shard_idxs[i::num_users]) for i in range(num_users)]
    print(f"ğŸ“Š æ•°æ®åˆ†é…ä¿¡æ¯: {num_users}ä¸ªå®¢æˆ·ç«¯ï¼Œ"
          f"æ¯å®¢æˆ·ç«¯åˆ†é… {min(shards_per_client_list)}-{max(shards_per_client_list)} ä¸ªåˆ†ç‰‡")
          
    # --- END: Recommended Change ---
    
    return dict_users
'''
å‡è®¾ num_users = 3,æ¯ä¸ªç”¨æˆ·åˆ†é…åˆ°çš„å›¾ç‰‡ç´¢å¼•å¦‚ä¸‹
{
    0: array([   0.,    1.,    2., ...,  899.,  900., 1199.]),  # ç”¨æˆ·0æ‹¥æœ‰ç¬¬0~1199å·å›¾ç‰‡
    1: array([1200., 1201., ..., 1799.]),                       # ç”¨æˆ·1æ‹¥æœ‰ç¬¬1200~1799å·å›¾ç‰‡
    2: array([1800., 1801., ..., 2399.])                        # ç”¨æˆ·2æ‹¥æœ‰ç¬¬1800~2399å·å›¾ç‰‡
}
'''

def mnist_noniid_unequal(dataset, num_users): # ä¸º MNIST æ•°æ®é›†è¿›è¡Œ Non-IID ä¸”æ•°æ®é‡ä¸å‡è¡¡çš„åˆ’åˆ†
    """
    Sample non-I.I.D client data from MNIST dataset s.t clients
    have unequal amount of data
    :param dataset:
    :param num_users:
    :returns a dict of clients with each clients assigned certain
    number of training imgs
    """
    # MNIST è®­ç»ƒé›† 60,000 å¼ å›¾ç‰‡ --> å‡è®¾æ¯ä¸ªåˆ†ç‰‡ 50 å¼ å›¾ç‰‡ï¼Œå…± 1200 ä¸ªåˆ†ç‰‡
    num_shards, num_imgs = 1200, 50
    idx_shard = [i for i in range(num_shards)]
    dict_users = {i: np.array([]) for i in range(num_users)}
    idxs = np.arange(num_shards*num_imgs)
    
    # è·å–è®­ç»ƒæ•°æ®çš„æ ‡ç­¾ (å…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬)
    # ä¼˜å…ˆä½¿ç”¨ .targets å±æ€§ (æ–°ç‰ˆtorchvision)ï¼Œå¦åˆ™å›é€€åˆ° .train_labels
    labels = np.array(dataset.targets if hasattr(dataset, 'targets') else dataset.train_labels)

    # sort labels (åŒä¸Šï¼ŒæŒ‰æ ‡ç­¾æ’åº)
    idxs_labels = np.vstack((idxs, labels))
    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]
    idxs = idxs_labels[0, :]

    # Minimum and maximum shards assigned per client:
    min_shard = 1 # æ¯ä¸ªå®¢æˆ·ç«¯æœ€å°‘åˆ†é…çš„åˆ†ç‰‡æ•°
    max_shard = 30 # æ¯ä¸ªå®¢æˆ·ç«¯æœ€å¤šåˆ†é…çš„åˆ†ç‰‡æ•°

    # Divide the shards into random chunks for every client
    # s.t the sum of these chunks = num_shards
    # ä¸ºæ¯ä¸ªç”¨æˆ·éšæœºç”Ÿæˆä¸€ä¸ªä»‹äº min_shard å’Œ max_shard ä¹‹é—´çš„åˆ†ç‰‡æ•°é‡
    random_shard_size = np.random.randint(min_shard, max_shard+1,
                                          size=num_users)
    '''æ¯”å¦‚ np.random.randint(1, 31, size=4) å¾—åˆ° [10, 25, 5, 20],å…¶å®ä½œä¸ºä¸€ä¸ªå æ¯”,åé¢ä¸åˆ†ç‰‡æ€»æ•°ç›¸ä¹˜'''
    # å°†è¿™äº›éšæœºç”Ÿæˆçš„åˆ†ç‰‡æ•°é‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶æ€»å’Œçº¦ç­‰äºæ€»åˆ†ç‰‡æ•° num_shards
    random_shard_size = np.around(random_shard_size /
                                  sum(random_shard_size) * num_shards)
    random_shard_size = random_shard_size.astype(int) # è½¬æ¢ä¸ºæ•´æ•°
    '''å½’ä¸€åŒ–åï¼Œå‡è®¾å¾—åˆ° [200, 400, 100, 500],å³4ä¸ªç”¨æˆ·åˆ†åˆ«åˆ†é…200ã€400ã€100ã€500ä¸ªåˆ†ç‰‡(æ€»å’Œçº¦ç­‰äº1200)ã€‚'''

    # Assign the shards randomly to each client
    # å¤„ç†éšæœºåˆ†é…ååˆ†ç‰‡æ€»æ•°å¯èƒ½ç•¥å¤§äºæˆ–å°äº num_shards çš„æƒ…å†µ
    if sum(random_shard_size) > num_shards:
        # å¦‚æœåˆ†é…çš„æ€»åˆ†ç‰‡æ•°è¶…è¿‡äº†å®é™…æ‹¥æœ‰çš„åˆ†ç‰‡æ•°
        for i in range(num_users):
            # First assign each client 1 shard to ensure every client has
            # atleast one shard of data (å…ˆç»™æ¯ä¸ªç”¨æˆ·åˆ†é…ä¸€ä¸ªåŸºç¡€åˆ†ç‰‡)
            rand_set = set(np.random.choice(idx_shard, 1, replace=False))
            idx_shard = list(set(idx_shard) - rand_set)
            for rand in rand_set:
                dict_users[i] = np.concatenate(
                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),
                    axis=0)

        random_shard_size = random_shard_size-1 # å‡å»å·²åˆ†é…çš„åŸºç¡€åˆ†ç‰‡
        '''å…ˆåˆ†åŸºç¡€åˆ†ç‰‡æ˜¯ä¸ºäº†é˜²æ­¢æœ‰å®¢æˆ·ç«¯åˆ†ä¸åˆ°æ•°æ®ï¼Œåé¢è¿™æ®µä»£ç æ˜¯æŠŠå‰©ä½™åˆ†ç‰‡æŒ‰å½’ä¸€åŒ–åçš„ç›®æ ‡æ•°é‡ç»§ç»­åˆ†é…ï¼Œç›´åˆ°åˆ†ç‰‡åˆ†å®Œã€‚'''
        # Next, randomly assign the remaining shards (å†åˆ†é…å‰©ä½™çš„)
        for i in range(num_users):
            if len(idx_shard) == 0: # å¦‚æœæ²¡æœ‰å‰©ä½™åˆ†ç‰‡äº†ï¼Œåˆ™è·³è¿‡
                continue
            shard_size = random_shard_size[i]
            if shard_size > len(idx_shard): # å¦‚æœæœŸæœ›åˆ†é…æ•°å¤§äºå‰©ä½™æ•°ï¼Œåˆ™å–å‰©ä½™æ•°
                shard_size = len(idx_shard)
            rand_set = set(np.random.choice(idx_shard, shard_size,
                                            replace=False))
            idx_shard = list(set(idx_shard) - rand_set)
            for rand in rand_set:
                dict_users[i] = np.concatenate(
                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),
                    axis=0)
    else: # å¦‚æœåˆ†é…çš„æ€»åˆ†ç‰‡æ•°å°äºç­‰äºå®é™…æ‹¥æœ‰çš„åˆ†ç‰‡æ•°
        '''
        å¦‚æœåˆ†é…å®Œæ¯ä¸ªç”¨æˆ·çš„ç›®æ ‡åˆ†ç‰‡æ•°åè¿˜æœ‰å‰©ä½™çš„åˆ†ç‰‡(len(idx_shard) > 0),å°±æŠŠè¿™äº›å‰©ä½™çš„åˆ†ç‰‡å…¨éƒ¨åˆ†é…ç»™å½“å‰æ‹¥æœ‰æ•°æ®æœ€å°‘çš„é‚£ä¸ªç”¨æˆ·ã€‚
        '''
        for i in range(num_users):
            shard_size = random_shard_size[i]
            rand_set = set(np.random.choice(idx_shard, shard_size,
                                            replace=False))
            idx_shard = list(set(idx_shard) - rand_set)
            for rand in rand_set:
                dict_users[i] = np.concatenate(
                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),
                    axis=0)

        if len(idx_shard) > 0: # å¦‚æœè¿˜æœ‰å‰©ä½™çš„åˆ†ç‰‡
            # Add the leftover shards to the client with minimum images:
            # å°†å‰©ä½™çš„åˆ†ç‰‡åˆ†é…ç»™å½“å‰æ‹¥æœ‰æ•°æ®æœ€å°‘çš„å®¢æˆ·ç«¯
            shard_size = len(idx_shard)
            # æ‰¾åˆ°æ‹¥æœ‰æ•°æ®æœ€å°‘çš„å®¢æˆ·ç«¯ k
            k = min(dict_users, key=lambda x: len(dict_users.get(x)))
            rand_set = set(np.random.choice(idx_shard, shard_size,
                                            replace=False))
            idx_shard = list(set(idx_shard) - rand_set)
            for rand in rand_set:
                dict_users[k] = np.concatenate(
                    (dict_users[k], idxs[rand*num_imgs:(rand+1)*num_imgs]),
                    axis=0)
    return dict_users


def cifar_iid(dataset, num_users): # ä¸º CIFAR-10 æ•°æ®é›†è¿›è¡Œ IID åˆ’åˆ†
    """
    Sample I.I.D. client data from CIFAR10 dataset
    :param dataset:
    :param num_users:
    :return: dict of image index
    """
    # é€»è¾‘ä¸ mnist_iid å®Œå…¨ç›¸åŒ
    num_items = int(len(dataset)/num_users)
    dict_users, all_idxs = {}, [i for i in range(len(dataset))]
    for i in range(num_users):
        dict_users[i] = set(np.random.choice(all_idxs, num_items,
                                             replace=False))
        all_idxs = list(set(all_idxs) - dict_users[i])
    return dict_users


def cifar_noniid(dataset, num_users): # ä¸º CIFAR-10 æ•°æ®é›†è¿›è¡Œ Non-IID åˆ’åˆ†
    """
    Sample non-I.I.D client data from CIFAR10 dataset
    :param dataset:
    :param num_users:
    :return:
    """
    # CIFAR-10 è®­ç»ƒé›† 50,000 å¼ å›¾ç‰‡ --> å‡è®¾æ¯ä¸ªåˆ†ç‰‡ 250 å¼ å›¾ç‰‡ï¼Œå…± 200 ä¸ªåˆ†ç‰‡
    num_shards, num_imgs = 200, 250
    idx_shard = [i for i in range(num_shards)]
    dict_users = {i: np.array([]) for i in range(num_users)}
    idxs = np.arange(num_shards*num_imgs)
    # labels = dataset.train_labels.numpy() # torchvision 0.9.1 ä¹‹å‰ç‰ˆæœ¬
    labels = np.array(dataset.targets) # torchvision 0.9.1 åŠä¹‹åç‰ˆæœ¬ï¼ŒCIFAR10 çš„æ ‡ç­¾å±æ€§åä¸º targets

    # sort labels (åŒä¸Š)
    idxs_labels = np.vstack((idxs, labels))
    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]
    idxs = idxs_labels[0, :]

    # divide and assign (æ¯ä¸ªå®¢æˆ·ç«¯åˆ†é… 2 ä¸ªä¸»è¦ç±»åˆ«çš„åˆ†ç‰‡)
    for i in range(num_users):
        rand_set = set(np.random.choice(idx_shard, 2, replace=False))
        idx_shard = list(set(idx_shard) - rand_set)
        for rand in rand_set:
            dict_users[i] = np.concatenate(
                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)
    return dict_users

def cifar_noniid_dirichlet(dataset, num_users, alpha=0.5):
    """
    ä½¿ç”¨ Dirichlet åˆ†å¸ƒè¿›è¡Œ CIFAR æ•°æ®é›†çš„ Non-IID åˆ’åˆ†
    
    Args:
        dataset: CIFAR æ•°æ®é›† (æ”¯æŒ CIFAR-10 å’Œ CIFAR-100)
        num_users: å®¢æˆ·ç«¯æ•°é‡
        alpha: Dirichlet åˆ†å¸ƒçš„æµ“åº¦å‚æ•°ï¼Œè¶Šå°æ•°æ®è¶Šä¸å‡åŒ€
        
    Returns:
        dict_users: å­—å…¸ï¼Œé”®ä¸ºå®¢æˆ·ç«¯IDï¼Œå€¼ä¸ºè¯¥å®¢æˆ·ç«¯çš„æ•°æ®ç´¢å¼•æ•°ç»„
    """
    # è‡ªåŠ¨æ£€æµ‹ç±»åˆ«æ•°é‡
    labels = np.array(dataset.targets)
    num_classes = len(np.unique(labels))  # åŠ¨æ€æ£€æµ‹ç±»åˆ«æ•°é‡
    dict_users = {i: np.array([]) for i in range(num_users)}
    
    # æŒ‰ç±»åˆ«ç»„ç»‡æ•°æ®ç´¢å¼•
    labels = np.array(dataset.targets)
    label_distribution = [[] for _ in range(num_classes)]
    for idx, label in enumerate(labels):
        label_distribution[label].append(idx)
    
    # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯ç”Ÿæˆç±»åˆ«åˆ†å¸ƒ
    for i in range(num_users):
        # ä» Dirichlet åˆ†å¸ƒé‡‡æ ·ç±»åˆ«æƒé‡
        proportions = np.random.dirichlet(np.repeat(alpha, num_classes))
        
        # å°†æƒé‡è½¬æ¢ä¸ºæ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
        proportions = np.array([p * len(label_distribution[j]) 
                               for j, p in enumerate(proportions)])
        proportions = proportions.astype(int)
        
        # ç¡®ä¿æ¯ä¸ªå®¢æˆ·ç«¯è‡³å°‘æœ‰ä¸€äº›æ•°æ®
        if proportions.sum() == 0:
            proportions[np.random.randint(0, num_classes)] = 1
            
        # ä¸ºå½“å‰å®¢æˆ·ç«¯åˆ†é…æ•°æ®
        client_data = []
        for j in range(num_classes):
            if proportions[j] > 0:
                # ä»è¯¥ç±»åˆ«ä¸­éšæœºé€‰æ‹©ç›¸åº”æ•°é‡çš„æ ·æœ¬
                available_samples = len(label_distribution[j])
                take_samples = min(proportions[j], available_samples)
                
                if take_samples > 0:
                    selected = np.random.choice(
                        label_distribution[j], 
                        take_samples, 
                        replace=False
                    )
                    client_data.extend(selected)
                    
                    # ä»å¯ç”¨æ ·æœ¬ä¸­ç§»é™¤å·²é€‰æ‹©çš„æ ·æœ¬
                    label_distribution[j] = list(
                        set(label_distribution[j]) - set(selected)
                    )
        
        dict_users[i] = np.array(client_data)
    
    # æ‰“å°æ•°æ®åˆ†å¸ƒä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    print(f"[INFO] Dirichlet åˆ’åˆ†å®Œæˆ (alpha={alpha}):")
    total_samples = 0
    for i in range(min(5, num_users)):  # åªæ˜¾ç¤ºå‰5ä¸ªå®¢æˆ·ç«¯çš„åˆ†å¸ƒ
        client_labels = [labels[idx] for idx in dict_users[i]]
        label_counts = np.bincount(client_labels, minlength=num_classes)
        total_samples += len(dict_users[i])
        print(f"  å®¢æˆ·ç«¯ {i}: {len(dict_users[i])} æ ·æœ¬, ç±»åˆ«åˆ†å¸ƒ: {label_counts}")
    print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
    
    return dict_users


if __name__ == '__main__': # æµ‹è¯•ä»£ç å—
    # åŠ è½½ MNIST è®­ç»ƒæ•°æ®é›†
    dataset_train = datasets.MNIST('../data/mnist/', train=True, download=False,
                                   transform=transforms.Compose([
                                       transforms.ToTensor(), # å°† PIL Image æˆ– numpy.ndarray è½¬æ¢ä¸º FloatTensorï¼Œå¹¶å°†å›¾åƒçš„åƒç´ èŒƒå›´ä» [0, 255] å½’ä¸€åŒ–åˆ° [0, 1]
                                       transforms.Normalize((0.1307,), (0.3081,)) # ç”¨å‡å€¼å’Œæ ‡å‡†å·®å¯¹å¼ é‡å›¾åƒè¿›è¡Œæ ‡å‡†åŒ–
                                   ]))
    num = 100 # å‡è®¾æœ‰ 100 ä¸ªç”¨æˆ·
    d = mnist_noniid(dataset_train, num) # æµ‹è¯• Non-IID åˆ’åˆ†å‡½æ•°