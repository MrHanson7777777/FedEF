#!/usr/bin/env python
# -*- coding: utf-8 -*-
# æ®‹å·®è”é‚¦å­¦ä¹ ä¸»ç¨‹åº

# è®¾ç½®ç¼–ç 
import sys

# å¼ºåˆ¶è®¾ç½®UTF-8ç¼–ç 
if sys.platform.startswith('win'):
    import os
    os.system('chcp 65001 > nul')  #è®¾ç½®Windowsæ§åˆ¶å°ä¸ºUTF-8
    
# ç¡®ä¿printè¾“å‡ºä½¿ç”¨UTF-8
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')

import os
import copy
import time
import pickle
import numpy as np
import pandas as pd
import torch
import math
import sys
from tqdm import tqdm
from datetime import timedelta, datetime

# è®¾ç½®PyTorchå’ŒcuDNNé€‰é¡¹ä»¥ä¼˜åŒ–æ€§èƒ½
torch.backends.cudnn.enabled = True #å¦‚æœè®¾ç½®ä¸º Trueï¼ŒPyTorch ä¼šè°ƒç”¨ cuDNN æ¥åŠ é€Ÿæ·±åº¦å­¦ä¹ ä¸­çš„å·ç§¯æ“ä½œå’Œå…¶ä»–ç›¸å…³æ“ä½œ
torch.backends.cudnn.benchmark = True  # å¯ç”¨benchmarkä»¥è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•
torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§ç®—æ³•è·å¾—æ›´å¥½æ€§èƒ½

# è®¾ç½®è¾“å‡ºç¼–ç 
if hasattr(sys.stdout, 'reconfigure'):
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

from tensorboardX import SummaryWriter

from options import args_parser
from update import test_inference
from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar, CNNCifar100, ResNet18Fed
from model_factory import get_model
from utils import get_dataset, exp_details
from residual_utils import LocalUpdateResidual, average_weights_residual, model_subtract, model_add, apply_residual_compression_fast, adaptive_client_aggregation, calculate_diversity_scores_residual, pack_sparse_residual, unpack_sparse_residual, calculate_communication_cost_dict, print_round_communication_stats, print_final_compression_stats



def main():
    start_time = time.time()
    
    print("[DEBUG] æ®‹å·®è”é‚¦å­¦ä¹ ç¨‹åºå¯åŠ¨")
    
    # è®¾ç½®æ—¥å¿—è·¯å¾„
    path_project = os.path.abspath('..')
    logger = SummaryWriter('../logs')
    
    print("[DEBUG] æ—¥å¿—è®¾ç½®å®Œæˆ")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = args_parser()
    exp_details(args)
    print("[DEBUG] å‚æ•°è§£æå®Œæˆ")
    
    # è®¾ç½®è®¾å¤‡
    device = setup_device(args)
    
    # åŠ è½½æ•°æ®é›†
    print("[DEBUG] å¼€å§‹åŠ è½½æ•°æ®é›†...")
    train_dataset, test_dataset, user_groups = get_dataset(args)
    print(f"[DEBUG] æ•°æ®é›†åŠ è½½å®Œæˆ: train={len(train_dataset)}, test={len(test_dataset)}")
    
    # æ„å»ºæ¨¡å‹
    global_model = build_model(args, train_dataset)
    global_model.to(device)
    global_model.train()

    # è®¾ç½®æ¨¡å‹åˆ°è®¾å¤‡
    global_model.to(device)
    global_model.train()
    
    # ä¿å­˜å…¨å±€æ¨¡å‹çš„æƒé‡
    global_weights = global_model.state_dict()
    
    # åˆå§‹åŒ–æ¯ä¸ªå®¢æˆ·ç«¯çš„æ®‹å·®çŠ¶æ€,åé¢ç”¨äºä¸Šä¼ 
    client_residuals = {}
    print(f"[DEBUG] åˆå§‹åŒ– {args.num_users} ä¸ªå®¢æˆ·ç«¯çš„æ®‹å·®çŠ¶æ€...")
    
    # å°†æ®‹å·®ä¿å­˜åœ¨CPUä¸Šï¼Œåªåœ¨éœ€è¦æ—¶ç§»åŠ¨åˆ°GPU
    for i in range(args.num_users):
        client_residuals[i] = {key: torch.zeros_like(param).cpu() 
                              for key, param in global_weights.items()}
    
    print(f"[DEBUG] ä¸Šè¡Œæ®‹å·®çŠ¶æ€åˆå§‹åŒ–å®Œæˆ,ä¿å­˜åœ¨CPUå†…å­˜ä¸­")
    
    # <--- åˆå§‹åŒ–æœåŠ¡å™¨ç«¯çš„ä¸‹æ”¾æ®‹å·®è¯¯å·®åé¦ˆçŠ¶æ€ --->
    server_downlink_error = {key: torch.zeros_like(param).cpu()
                             for key, param in global_weights.items()}
    print(f"[DEBUG] æœåŠ¡å™¨ä¸‹è¡Œè¯¯å·®åé¦ˆçŠ¶æ€åˆå§‹åŒ–å®Œæˆ")
    
    # <--- åˆå§‹åŒ–å®¢æˆ·ç«¯åŒæ­¥æ¨¡å‹çŠ¶æ€ (æ‰€æœ‰å®¢æˆ·ç«¯ä»é›¶æ¨¡å‹å¼€å§‹) --->
    zero_weights = {key: torch.zeros_like(param).cpu()
                    for key, param in global_weights.items()}
    client_synced_models = {i: copy.deepcopy(zero_weights) for i in range(args.num_users)}
    print(f"[DEBUG] å®¢æˆ·ç«¯åŒæ­¥æ¨¡å‹åˆå§‹åŒ–ä¸ºé›¶çŠ¶æ€")
    
    # ç»Ÿè®¡å˜é‡
    communication_cost = []
    print_every = 2
    
    # <--- ä¸‹è¡Œé€šä¿¡çš„å•ç‹¬è®¡æ—¶å˜é‡ --->
    total_downlink_pack_time = 0.0    # æœåŠ¡å™¨æ‰“åŒ…æ—¶é—´
    total_downlink_unpack_time = 0.0  # å®¢æˆ·ç«¯è§£åŒ…æ—¶é—´
    downlink_pack_count = 0
    downlink_unpack_count = 0
    
    # åˆå§‹åŒ–è®­ç»ƒç»Ÿè®¡å˜é‡
    train_loss = []  # ç”¨äºè®°å½•æ¯è½®çš„å¹³å‡è®­ç»ƒæŸå¤±
    global_test_accuracy = []  # ç”¨äºè®°å½•æ¯è½®çš„å…¨å±€æµ‹è¯•å‡†ç¡®ç‡
    communication_cost = []  # ç”¨äºè®°å½•æ¯è½®çš„é€šä¿¡å¼€é”€ï¼ˆå­—èŠ‚æ•°ï¼‰
    epoch_times = []  # ç”¨äºè®°å½•æ¯è½®çš„è®­ç»ƒè€—æ—¶ï¼ˆç§’ï¼‰
    improve_streak = 0  # åˆå§‹åŒ–è¿ç»­æå‡è®¡æ•°å™¨
    
    # æ—©åœæœºåˆ¶å‚æ•°
    patience = args.stopping_rounds
    
    #è‡ªé€‚åº”å‹ç¼©æ‰€éœ€çš„çŠ¶æ€å˜é‡
    ema_alpha = 0.4  # EMAå¹³æ»‘å› å­ï¼Œç”¨äºå¹³æ»‘å‡†ç¡®ç‡çš„å˜åŒ–ï¼Œé¿å…å› å•è½®æ³¢åŠ¨å¯¼è‡´è¯¯åˆ¤
    ema_acc = None   # å¹³æ»‘åçš„å‡†ç¡®ç‡ï¼Œåˆå§‹å€¼ä¸ºNoneï¼Œåç»­ä¼šåŠ¨æ€æ›´æ–°
    best_ema_acc = -1.0 # è®°å½•æœ€ä½³çš„å¹³æ»‘å‡†ç¡®ç‡ï¼Œç”¨äºåˆ¤æ–­æ¨¡å‹æ€§èƒ½æ˜¯å¦æå‡
    patience_counter = 0  # è€å¿ƒè®¡æ•°å™¨ï¼Œç”¨äºæ—©åœæœºåˆ¶ï¼Œè®°å½•è¿ç»­æœªæå‡çš„è½®æ¬¡æ•°
    best_global_weights = None # ç¡®ä¿best_global_weightsåœ¨è¿™é‡Œåˆå§‹åŒ–ï¼Œç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡

    #åˆå§‹åŒ–å†å²è®°å½•,ç”¨äºåç»­åˆ†æå’Œå¯è§†åŒ–
    history = {
        'epoch': [],
        'test_accuracy': [],
        'avg_train_loss': [],
        'learning_rate': [],
        'compression_ratio': [],
        'communication_cost': []  # æ·»åŠ é€šä¿¡å¼€é”€è®°å½•
    }
    
    # åˆå§‹åŒ–å‹ç¼©æ—¶é—´ç»Ÿè®¡å˜é‡
    total_pack_time = 0.0      # æ€»æ‰“åŒ…æ—¶é—´ (æ¯«ç§’)
    total_unpack_time = 0.0    # æ€»è§£åŒ…æ—¶é—´ (æ¯«ç§’)  
    pack_count = 0             # æ‰“åŒ…æ¬¡æ•°
    unpack_count = 0           # è§£åŒ…æ¬¡æ•°
    
    # åœ¨ä¸»å‡½æ•°ä¸­è°ƒç”¨å°è£…çš„æ‰“å°å‡½æ•°  
    print_training_details(args=args, ema_alpha=ema_alpha, patience=patience)
    
    # è®­ç»ƒå¾ªç¯ - ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    pbar = tqdm(range(args.epochs), desc="Training Progress", 
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]")
    
    #åœ¨è®­ç»ƒå¾ªç¯ for epoch in pbar: ä¹‹å‰ï¼Œåˆå§‹åŒ–ä¸€ä¸ªæ–°çš„å­—å…¸æ¥å­˜å‚¨å¹³æ»‘æŸå¤±
    client_ema_losses = {}
    EMA_ALPHA = 0.3 # å¹³æ»‘å› å­ï¼Œå¯ä»¥ä½œä¸ºè¶…å‚æ•°è°ƒæ•´
    #---------------------åˆå§‹åŒ–ç»“æŸ---------------------
    
    # <--- Phase 1 - åˆå§‹æ¨¡å‹å†·å¯åŠ¨ä¼ è¾“ --->
    initial_global_weights = global_model.state_dict()  # ç›®æ ‡G_0,æœåŠ¡å™¨éšæœºåˆå§‹åŒ–å¾—åˆ°çš„åˆå§‹åŒ–æ¨¡å‹
    server_side_synced_model = {key: torch.zeros_like(param).cpu() for key, param in initial_global_weights.items()}

    # åˆ¤æ–­æ˜¯å¦éœ€è¦å†·å¯åŠ¨æµå¼ä¼ è¾“ï¼šæœ‰ä¸‹è¡Œå‹ç¼©ä¸”ç”¨æˆ·æœªæ˜ç¡®ç¦ç”¨
    if args.downlink_compression == 'uniform':
        # æœ‰ä¸‹è¡Œå‹ç¼© - æ‰§è¡Œå†·å¯åŠ¨æµå¼ä¼ è¾“
        # --- ä¿ç•™åŸå§‹çš„æµå¼ä¼ è¾“é€»è¾‘ (ä½¿ç”¨æ–°çš„ä¸‹è¡Œå‹ç¼©æ¯”å‚æ•°) ---
        INITIALIZATION_ROUNDS = int(1 / args.downlink_compression_ratio + 1)

        print(f"\n{'='*60}\nğŸš€ å¼€å§‹åˆå§‹æ¨¡å‹æµå¼ä¼ è¾“ (å…± {INITIALIZATION_ROUNDS} è½®)\n{'='*60}")
        pbar_init = tqdm(range(INITIALIZATION_ROUNDS), desc="åˆå§‹æ¨¡å‹åŒæ­¥")

        for init_round in pbar_init:
            # 1. è®¡ç®—å½“å‰è½®çš„å®Œæ•´æ®‹å·®ï¼šç›®æ ‡æ¨¡å‹ - å½“å‰åŒæ­¥æ¨¡å‹
            total_residual_to_send = model_subtract(initial_global_weights, server_side_synced_model)
            
            # 2. å¯¹å®Œæ•´æ®‹å·®è¿›è¡ŒTop-Kå‹ç¼©ï¼ˆä½¿ç”¨æ–°çš„ä¸‹è¡Œå‹ç¼©æ¯”å‚æ•°ï¼‰
            compressed_residual = apply_residual_compression_fast(
                total_residual_to_send, args.downlink_compression_ratio
            )
            
            # å‹ç¼©æ¨¡å¼ï¼šæ‰“åŒ…å‹ç¼©åçš„æ®‹å·®ç”¨äºä¼ è¾“
            pack_result = pack_sparse_residual(compressed_residual, enable_timing=True)
            packed_residual, pack_time = pack_result
            total_downlink_pack_time += pack_time
            downlink_pack_count += 1
                
            print(f"[INIT {init_round+1}/{INITIALIZATION_ROUNDS}] å‹ç¼©ç‡: {args.downlink_compression_ratio:.1f}, æ‰“åŒ…è€—æ—¶: {pack_time:.2f}ms")

            # 4. æ›´æ–°æœåŠ¡å™¨ç«¯çš„å®¢æˆ·ç«¯æ¨¡æ‹Ÿæ¨¡å‹ï¼ˆä½¿ç”¨å‹ç¼©åçš„å‚æ•°ï¼‰
            server_side_synced_model = model_add(server_side_synced_model, compressed_residual)

            # 5. æ‰€æœ‰å®¢æˆ·ç«¯æ¥æ”¶å¹¶æ›´æ–°è‡ªå·±çš„åŒæ­¥æ¨¡å‹
            # æ¯ä¸ªå®¢æˆ·ç«¯éƒ½æ¥æ”¶æ‰“åŒ…åçš„æ•°æ®å¹¶ç‹¬ç«‹è¿›è¡Œè§£åŒ…
            # åˆå§‹åŒ–è§£åŒ…æ—¶é—´ä¸º0
            max_client_unpack_time = 0.0

            for i in range(args.num_users):
                # å‹ç¼©æ¨¡å¼ï¼šå®¢æˆ·ç«¯æ¥æ”¶æ‰“åŒ…æ•°æ®å¹¶è§£åŒ…
                client_received_packed = copy.deepcopy(packed_residual)
                    
                # å®¢æˆ·ç«¯ç‹¬ç«‹è§£åŒ…
                client_unpacked_params, client_unpack_time = unpack_sparse_residual(
                    client_received_packed, zero_weights, enable_timing=True
                )
                    
                # æ›´æ–°æœ€å¤§è§£åŒ…æ—¶é—´
                max_client_unpack_time = max(max_client_unpack_time, client_unpack_time)
                
                # å®¢æˆ·ç«¯æ›´æ–°è‡ªå·±çš„åŒæ­¥æ¨¡å‹
                client_synced_models[i] = model_add(client_synced_models[i], client_unpacked_params)            # ç»Ÿè®¡æ‰€æœ‰å®¢æˆ·ç«¯çš„è§£åŒ…æ—¶é—´ï¼ˆå¹¶è¡Œæƒ…å†µä¸‹å–æœ€å¤§å€¼ï¼‰
                total_downlink_unpack_time += max_client_unpack_time
                downlink_unpack_count += 1  # ä¸€è½®å¹¶è¡Œè§£åŒ…ç®—ä½œä¸€æ¬¡æ“ä½œ

            # æ›´æ–°è¿›åº¦æ¡
            pbar_init.set_postfix({
                'PackTime': f'{pack_time:.2f}ms', 
                'UnpackTime': f'{max_client_unpack_time:.2f}ms'
            })

        print(f"\nâœ… åˆå§‹æ¨¡å‹æµå¼ä¼ è¾“å®Œæˆ.\n{'='*60}")
    else:
        print(f"\n{'='*60}\nğŸš€ åˆå§‹æ¨¡å‹æµå¼ä¼ è¾“: å·²è·³è¿‡ (å› ä¸‹è¡Œæœªå‹ç¼©,æ— éœ€å†·å¯åŠ¨)")
        print(f"   æ‰€æœ‰ {args.num_users} ä¸ªå®¢æˆ·ç«¯å°†ç›´æ¥åŒæ­¥å®Œæ•´æ¨¡å‹.\n{'='*60}")
        
        # æ‰€æœ‰å®¢æˆ·ç«¯ç›´æ¥è·å¾—å®Œæ•´æ¨¡å‹
        client_synced_models = {i: copy.deepcopy(initial_global_weights) for i in range(args.num_users)}
        server_side_synced_model = copy.deepcopy(initial_global_weights)
    
    # <--- åœ¨ä¸»è®­ç»ƒå¾ªç¯å‰åˆå§‹åŒ–å¹¿æ’­å˜é‡ --->
    residual_to_broadcast_packed = None  # æŒä¹…åŒ–å­˜å‚¨æ¯è½®æœåŠ¡å™¨è®¡ç®—çš„å¾…å¹¿æ’­æ®‹å·®
    
    # ç°åœ¨ï¼Œä¸»è®­ç»ƒå¾ªç¯å¼€å§‹
    for epoch in pbar:
        epoch_start_time = time.time()
        
        print(f"\n{'='*60}\nğŸ”„ å…¨å±€è½®æ¬¡ {epoch+1}/{args.epochs}\n{'='*60}")
        
        # åˆå§‹åŒ–æœ¬åœ°æ®‹å·®å’ŒæŸå¤±åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ¯ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒç»“æœ
        local_residuals = [] # local_residualså­˜å‚¨æ¯ä¸ªå®¢æˆ·ç«¯è¦ä¸Šä¼ çš„æ®‹å·®å­—å…¸
        local_losses= [] # local_losseså­˜å‚¨æ¯ä¸ªå®¢æˆ·ç«¯çš„è®­ç»ƒæŸå¤±
        
        # åˆå§‹åŒ–æœ¬è½®é€šä¿¡å¼€é”€ç»Ÿè®¡å˜é‡
        epoch_comm_cost = 0  # epoch_comm_costç”¨äºè®°å½•å½“å‰è½®æ¬¡æ‰€æœ‰å®¢æˆ·ç«¯çš„é€šä¿¡å¼€é”€ï¼ˆå­—èŠ‚æ•°ï¼‰
        
        # åˆå§‹åŒ–é€šä¿¡é‡ç»Ÿè®¡å˜é‡
        total_nonzero_values = 0   # å‹ç¼©åçš„å‚æ•°æ€»é‡
        total_original_params = 0  # å‹ç¼©å‰çš„å‚æ•°æ€»é‡
        
        global_model.train()
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯çŠ¶æ€è·Ÿè¸ª
        if not hasattr(args, 'client_history'):
            args.client_history = {
                'losses': {},           # æ¯ä¸ªå®¢æˆ·ç«¯çš„å†å²æŸå¤±
                'last_selected': [],    # æ‰€æœ‰å†å²é€‰æ‹©çš„å®¢æˆ·ç«¯ï¼ˆç”¨äºç®€å•é¢‘ç‡ç»Ÿè®¡ï¼‰
                'round_selections': [], # æŒ‰è½®è®°å½•çš„é€‰æ‹©å†å²ï¼ˆç”¨äºæ›´ç²¾ç¡®çš„è½®çº§åˆ†æï¼‰
                'performance_scores': {}
            }
        
        # å®šä¹‰æ¯è½®é€‰æ‹©çš„å®¢æˆ·ç«¯æ•°é‡
        m = max(int(args.frac * args.num_users), 1)

        # <--- å¯¹æ‰€æœ‰å®¢æˆ·ç«¯æ‰§è¡Œä¸‹è¡Œæ›´æ–° --->
        if residual_to_broadcast_packed is not None:
            print(f"\n[DOWNLINK] å‘æ‰€æœ‰ {args.num_users} ä¸ªå®¢æˆ·ç«¯å¹¿æ’­ä¸‹è¡Œæ›´æ–°...")

            # æ¯ä¸ªå®¢æˆ·ç«¯å¹¶è¡Œæ¥æ”¶æœåŠ¡å™¨å¹¿æ’­çš„æ®‹å·®,æ”¯æŒå…¨é‡å’Œå‹ç¼©æ¨¡å¼
            max_client_unpack_time = 0.0  # è®°å½•æœ€å¤§è§£åŒ…æ—¶é—´ï¼ˆå¹¶è¡Œæ“ä½œï¼‰
            for i in range(args.num_users):  # éå†æ¯ä¸€ä¸ªç”¨æˆ·
                if args.downlink_compression == 'uniform':
                    # å‹ç¼©æ¨¡å¼ï¼šå®¢æˆ·ç«¯æ¥æ”¶æ‰“åŒ…æ•°æ®å¹¶è§£åŒ…
                    client_received_packed = copy.deepcopy(residual_to_broadcast_packed)
                    
                    # å®¢æˆ·ç«¯ç‹¬ç«‹è§£åŒ…
                    unpacked_downlink_residual, client_unpack_time = unpack_sparse_residual(
                        client_received_packed, zero_weights, enable_timing=True
                    )
                    
                    # æ›´æ–°æœ€å¤§è§£åŒ…æ—¶é—´ï¼ˆå¹¶è¡Œæ“ä½œå–æœ€å¤§å€¼ï¼‰
                    max_client_unpack_time = max(max_client_unpack_time, client_unpack_time)
                else:
                    # å…¨é‡æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨å¯†é›†å¼ é‡ï¼Œè·³è¿‡è§£åŒ…æ­¥éª¤
                    unpacked_downlink_residual = copy.deepcopy(residual_to_broadcast_packed)
                
                # å®¢æˆ·ç«¯æ›´æ–°è‡ªå·±çš„åŒæ­¥æ¨¡å‹
                client_synced_models[i] = model_add(client_synced_models[i], unpacked_downlink_residual)

            # æ›´æ–°è®¡æ—¶ç»Ÿè®¡ (å¹¶è¡Œæƒ…å†µä¸‹å–æœ€å¤§å€¼)
            total_downlink_unpack_time += max_client_unpack_time
            downlink_unpack_count += 1  # ä¸€è½®å¹¶è¡Œè§£åŒ…ç®—ä½œä¸€æ¬¡æ“ä½œ
            if max_client_unpack_time > 0:  # åªæœ‰å½“å‹ç¼©æ¨¡å¼æ‰æ‰“å°
                print(f"[DOWNLINK] æ‰€æœ‰å®¢æˆ·ç«¯åŒæ­¥æ¨¡å‹æ›´æ–°å®Œæˆï¼Œæœ€å¤§è§£åŒ…è€—æ—¶: {max_client_unpack_time:.2f}ms")
        else:
            print(f"\n[DOWNLINK] ç¬¬ {epoch+1} è½®ï¼šæ— ä¸‹è¡Œæ›´æ–°ï¼ˆåˆå§‹è½®ï¼‰")

        # ç»Ÿä¸€è°ƒç”¨select_clientsï¼Œç”±å‡½æ•°å†…éƒ¨å¤„ç† 'random' å’Œ 'smart' é€»è¾‘
        idxs_users = select_clients(epoch, args, user_groups, client_ema_losses, EMA_ALPHA, m)
        
        # åˆå§‹åŒ–å¹¶è¡Œä¸Šè¡Œæ‰“åŒ…æ—¶é—´è®°å½•
        max_uplink_pack_time = 0.0  # è®°å½•æ‰€æœ‰é€‰ä¸­å®¢æˆ·ç«¯ä¸­æœ€å¤§çš„æ‰“åŒ…æ—¶é—´
        
        for idx in idxs_users:
            print(f"\n[CLIENT {idx}] å¼€å§‹æœ¬åœ°è®­ç»ƒ...")
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯æœ¬åœ°æ›´æ–°å¯¹è±¡
            local_model = LocalUpdateResidual(args=args, dataset=train_dataset,
                                              idxs=user_groups[idx], client_id=idx)
            
            # <--- å®¢æˆ·ç«¯ä»å…¶è‡ªå·±çš„åŒæ­¥æ¨¡å‹å¼€å§‹è®­ç»ƒ --->
            current_client_synced_weights = client_synced_models[idx]
            
            # æœ¬åœ°è®­ç»ƒ - ä½¿ç”¨å®¢æˆ·ç«¯è‡ªå·±çš„åŒæ­¥æ¨¡å‹
            updated_weights, loss = local_model.update_weights_memory_efficient(
                global_weights=current_client_synced_weights, global_round=epoch, device=device)
            
            # è®°å½•å®¢æˆ·ç«¯æ€§èƒ½ç”¨äºæ™ºèƒ½é€‰æ‹©
            if idx not in args.client_history['losses']:
                #å¦‚æœç›®å‰è¿™ä¸ªç”¨æˆ·è¿˜æ²¡æœ‰è®°å½•,é‚£ä¹ˆå…ˆæ–°å»ºä¸€ä¸ªç©ºåˆ—è¡¨
                args.client_history['losses'][idx] = []
            args.client_history['losses'][idx].append(loss)
            # åªä¿ç•™æœ€è¿‘10è½®çš„æŸå¤±è®°å½•
            if len(args.client_history['losses'][idx]) > 10:
                args.client_history['losses'][idx] = args.client_history['losses'][idx][-10:]
            
            # <--- æ®‹å·®ç›¸å¯¹äºå®¢æˆ·ç«¯è‡ªå·±çš„èµ·å§‹æ¨¡å‹è®¡ç®— --->
            model_residual = model_subtract(updated_weights, current_client_synced_weights)

            # 1. åº”ç”¨ä¸Šè¡Œè¯¯å·®åé¦ˆ (Error Feedback)
            if args.disable_uplink_ef:
                compensated_residual = model_residual # è·³è¿‡EF
                if epoch == 0: print(f"[CLIENT {idx}] ä¸Šè¡Œè¯¯å·®åé¦ˆ: ç¦ç”¨")
            else:
                compensated_residual = model_add(model_residual, client_residuals[idx]) # é»˜è®¤EF

            residual_to_upload = copy.deepcopy(compensated_residual)

            # 2. åº”ç”¨ä¸Šè¡Œå‹ç¼© (Compression)
            if args.uplink_compression == 'uniform':
                residual_to_upload = apply_residual_compression_fast(
                    residual_to_upload, args.uplink_compression_ratio)
                if epoch == 0: print(f"[CLIENT {idx}] ä¸Šè¡Œå‹ç¼©: uniform (ratio={args.uplink_compression_ratio})")
            else:
                if epoch == 0: print(f"[CLIENT {idx}] ä¸Šè¡Œå‹ç¼©: none")

            # 3. æ›´æ–°ä¸Šè¡Œè¯¯å·®çŠ¶æ€ (Error Feedback State)
            if args.disable_uplink_ef:
                # EFè¢«ç¦ç”¨ï¼Œæ®‹å·®æ¸…é›¶
                new_client_residual = {key: torch.zeros_like(param).cpu() 
                                      for key, param in model_residual.items()}
            else:
                # EFå¯ç”¨ï¼Œè®¡ç®—æœªå‘é€çš„éƒ¨åˆ†
                new_client_residual = model_subtract(compensated_residual, residual_to_upload)

            # æ›´æ–°è¯¥å®¢æˆ·ç«¯çš„å†å²è¯¯å·®ï¼Œä¸ºä¸‹ä¸€è½®åšå‡†å¤‡
            for key, param in new_client_residual.items():
                if key in client_residuals[idx]:
                    client_residuals[idx][key] = param.cpu()

            # æ­¥éª¤ 6: å¤„ç†"æœ€ç»ˆè¦ä¸Šä¼ "çš„æ®‹å·® - æ”¯æŒå…¨é‡å’Œå‹ç¼©æ¨¡å¼
            if args.uplink_compression == 'uniform':
                # å‹ç¼©æ¨¡å¼ï¼šå¯¹æ®‹å·®è¿›è¡Œç¨€ç–åŒ–æ‰“åŒ…å¤„ç†
                pack_result = pack_sparse_residual(residual_to_upload, enable_timing=True)
                
                # åˆå§‹åŒ–æœ€ç»ˆæ®‹å·®å˜é‡
                final_residual = None
                
                # æ£€æŸ¥æ‰“åŒ…ç»“æœæ˜¯å¦åŒ…å«è®¡æ—¶ä¿¡æ¯
                if isinstance(pack_result, tuple):
                    # å¦‚æœè¿”å›çš„æ˜¯å…ƒç»„ï¼Œè¯´æ˜åŒ…å«äº†æ‰“åŒ…åçš„æ®‹å·®å’Œæ‰“åŒ…æ—¶é—´
                    final_residual, pack_time = pack_result
                    # è®°å½•æœ€å¤§æ‰“åŒ…æ—¶é—´ï¼ˆå¹¶è¡Œæ“ä½œå–æœ€å¤§å€¼ï¼‰
                    max_uplink_pack_time = max(max_uplink_pack_time, pack_time)
                else:
                    # å¦‚æœè¿”å›çš„ä¸æ˜¯å…ƒç»„ï¼Œç›´æ¥å°†ç»“æœèµ‹å€¼ä¸ºæœ€ç»ˆæ®‹å·®
                    final_residual = pack_result
                
                if epoch == 0: print(f"[CLIENT {idx}] ä¸Šè¡Œå‹ç¼©æ‰“åŒ…å®Œæˆï¼Œè€—æ—¶: {pack_time:.2f}ms")
            else:
                # å…¨é‡æ¨¡å¼ï¼šç›´æ¥ä¼ é€’å¯†é›†å¼ é‡ï¼Œè·³è¿‡æ‰“åŒ…æ­¥éª¤
                final_residual = copy.deepcopy(residual_to_upload)
                if epoch == 0: print(f"[CLIENT {idx}] å…¨é‡ä¸Šè¡Œä¼ è¾“ï¼Œè·³è¿‡æ‰“åŒ…æ­¥éª¤")
            
            # æ­¥éª¤7: è®°å½•æ®‹å·®å’ŒæŸå¤±
            local_residuals.append(copy.deepcopy(final_residual))  # ä¿å­˜å½“å‰å®¢æˆ·ç«¯çš„æœ€ç»ˆæ®‹å·®
            local_losses.append(copy.deepcopy(loss))  # ä¿å­˜å½“å‰å®¢æˆ·ç«¯çš„è®­ç»ƒæŸå¤±
            
            # è°ƒç”¨å°è£…å‡½æ•°è®¡ç®—å¹¶æ‰“å°é€šä¿¡ç»Ÿè®¡ä¿¡æ¯
            client_transmitted_bytes = calculate_and_print_client_communication_stats(
                idx=idx,
                final_residual=final_residual,
                global_model=global_model,
                args=args,
                total_original_params=total_original_params,
                total_nonzero_values=total_nonzero_values
            )
            # ç´¯åŠ æœ¬è½®é€šä¿¡å¼€é”€
            epoch_comm_cost += client_transmitted_bytes
            
        # æ›´æ–°å¹¶è¡Œä¸Šè¡Œæ‰“åŒ…æ—¶é—´ç»Ÿè®¡
        if max_uplink_pack_time > 0:
            total_pack_time += max_uplink_pack_time
            pack_count += 1  # ä¸€è½®å¹¶è¡Œæ‰“åŒ…ç®—ä½œä¸€æ¬¡æ“ä½œ
            print(f"\nğŸ“¤ ä¸Šè¡Œå¹¶è¡Œæ‰“åŒ…å®Œæˆï¼Œæœ€å¤§æ‰“åŒ…è€—æ—¶: {max_uplink_pack_time:.2f}ms")
        
        # è¿™ä¸ªæ¨¡æ¿å°†å‘Šè¯‰èšåˆå‡½æ•°åœ¨CPUä¸Šè¿›è¡Œè§£åŒ…å’Œè®¡ç®—
        cpu_model_template = {key: param.cpu() for key, param in global_model.state_dict().items()}
            
        # æœåŠ¡å™¨èšåˆæ®‹å·® - æ”¯æŒå…¨é‡å’Œå‹ç¼©æ¨¡å¼ï¼Œæ”¯æŒIIDå’ŒOODåœºæ™¯
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”èšåˆ
        aggregation_method = getattr(args, 'adaptive_aggregation', 'weighted_average')
        use_adaptive_aggregation = aggregation_method == 'diversity_aware'
        
        if use_adaptive_aggregation:
            # è‡ªé€‚åº”èšåˆåœºæ™¯ï¼ˆæ”¯æŒIIDå’ŒOODï¼‰
            client_data_sizes = [len(user_groups[idx]) for idx in idxs_users]
            client_losses_vals = [loss for loss in local_losses]
            
            # åˆ¤æ–­æ•°æ®åˆ†å¸ƒç±»å‹å¹¶ç›¸åº”è°ƒæ•´ç­–ç•¥
            data_distribution = "OOD" if (hasattr(args, 'iid') and args.iid == 0) else "IID"
            
            # å¦‚æœä½¿ç”¨diversity_awareèšåˆï¼Œè®¡ç®—å¤šæ ·æ€§åˆ†æ•°
            diversity_scores = None
            if aggregation_method == 'diversity_aware':
                # å°†CPUæ¨¡å‹æ¨¡æ¿ä¼ å…¥
                diversity_scores = calculate_diversity_scores_residual(
                    local_residuals, 
                    client_data_sizes, 
                    server_model_template=cpu_model_template,  # ä½¿ç”¨CPUæ¨¡æ¿
                    args=args  # ä¼ å…¥argså‚æ•°ä»¥åˆ¤æ–­æ˜¯å¦å‹ç¼©
                )
                print(f"ğŸ§® è®¡ç®—å¤šæ ·æ€§åˆ†æ•° ({data_distribution}): {[f'{score:.4f}' for score in diversity_scores]}")
            
            # æ ¹æ®æ˜¯å¦å¯ç”¨ä¸Šè¡Œå‹ç¼©é€‰æ‹©èšåˆæ–¹å¼
            if args.uplink_compression == 'uniform':
                agg_result = adaptive_client_aggregation(
                    local_residuals, 
                    client_data_sizes, 
                    client_losses_vals,
                    server_model_template=cpu_model_template,  # ä½¿ç”¨CPUæ¨¡æ¿
                    aggregation_method=aggregation_method,
                    diversity_scores=diversity_scores,
                    enable_timing=True  # å¯ç”¨è®¡æ—¶
                )
                if isinstance(agg_result, tuple):
                    aggregated_residual, server_unpack_time = agg_result
                    total_unpack_time += server_unpack_time
                    unpack_count += 1  # æœåŠ¡å™¨ç«¯èšåˆç®—ä½œä¸€æ¬¡è§£åŒ…æ“ä½œ
                else:
                    aggregated_residual = agg_result
            else:
                # å…¨é‡æ¨¡å¼ï¼šç›´æ¥å¯¹å¯†é›†å¼ é‡è¿›è¡Œèšåˆï¼Œæ— éœ€è§£åŒ…ï¼Œä½†ä»ä½¿ç”¨è‡ªé€‚åº”èšåˆ
                agg_result = adaptive_client_aggregation(
                    local_residuals, 
                    client_data_sizes, 
                    client_losses_vals,
                    server_model_template=None,  # å…¨é‡æ¨¡å¼ä¸éœ€è¦æ¨¡æ¿
                    aggregation_method=aggregation_method,
                    diversity_scores=diversity_scores,
                    enable_timing=False # å…¨é‡æ¨¡å¼æ— è§£åŒ…è®¡æ—¶
                )
                # ç¡®ä¿ aggregated_residual è¢«æ­£ç¡®èµ‹å€¼
                if isinstance(agg_result, tuple):
                    aggregated_residual, _ = agg_result
                else:
                    aggregated_residual = agg_result
            
            print(f"ğŸ“Š ä½¿ç”¨è‡ªé€‚åº”èšåˆç­–ç•¥ ({aggregation_method}) - {data_distribution}åœºæ™¯")
        else:
            # æ ‡å‡†èšåˆåœºæ™¯ï¼ˆweighted_averageï¼‰
            data_distribution = "OOD" if (hasattr(args, 'iid') and args.iid == 0) else "IID"
            
            if args.uplink_compression == 'uniform':
                # å‹ç¼©æ¨¡å¼ï¼šéœ€è¦è§£åŒ…å¹¶ä½¿ç”¨CPUæ¨¡æ¿
                agg_result = average_weights_residual(local_residuals, 
                                                      template=cpu_model_template,  # ä¼ å…¥CPUæ¨¡æ¿
                                                      enable_timing=True)
                if isinstance(agg_result, tuple):
                    aggregated_residual, server_unpack_time = agg_result
                    total_unpack_time += server_unpack_time
                    unpack_count += 1  # æœåŠ¡å™¨ç«¯èšåˆç®—ä½œä¸€æ¬¡è§£åŒ…æ“ä½œ
                else:
                    aggregated_residual = agg_result
            else:
                # å…¨é‡æ¨¡å¼ï¼šç›´æ¥å¯¹å¯†é›†å¼ é‡è¿›è¡Œèšåˆï¼Œæ— éœ€è§£åŒ…
                aggregated_residual = average_weights_residual(local_residuals, 
                                                              template=None,  # å…¨é‡æ¨¡å¼ä¸éœ€è¦æ¨¡æ¿
                                                              enable_timing=False)  # å…¨é‡æ¨¡å¼ä¸éœ€è¦è§£åŒ…è®¡æ—¶
            print(f"ğŸ“Š ä½¿ç”¨æ ‡å‡†èšåˆç­–ç•¥ (weighted_average) - {data_distribution}åœºæ™¯")
        
        # æ›´æ–°å…¨å±€æ¨¡å‹ - ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
        # è·å–å½“å‰å…¨å±€æ¨¡å‹çš„æƒé‡ï¼ˆåœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼‰
        current_global_weights = global_model.state_dict()
        updated_global_weights = model_add(current_global_weights, aggregated_residual)
        
        # æ£€æŸ¥æ›´æ–°åçš„æƒé‡æ˜¯å¦åŒ…å«NaNæˆ–Inf
        weights_valid = True
        for key, param in updated_global_weights.items():
            if torch.isnan(param).any() or torch.isinf(param).any():
                print(f"è­¦å‘Š: å‚æ•° {key} åŒ…å«NaN/Infå€¼")
                weights_valid = False
        
        if weights_valid:
            print("\n[SERVER] å‡†å¤‡ä¸‹è¡Œå¹¿æ’­...")

            # 1. åº”ç”¨ä¸‹è¡Œè¯¯å·®åé¦ˆ (Error Feedback)
            if args.disable_downlink_ef:
                compensated_downlink_residual = aggregated_residual # è·³è¿‡EF
                if epoch == 0: print("[SERVER] ä¸‹è¡Œè¯¯å·®åé¦ˆ: ç¦ç”¨")
            else:
                compensated_downlink_residual = model_add(aggregated_residual, server_downlink_error) # é»˜è®¤EF

            # 2. åº”ç”¨ä¸‹è¡Œå‹ç¼© (Compression)
            if args.downlink_compression == 'uniform':
                residual_to_broadcast_unpacked = apply_residual_compression_fast(
                    compensated_downlink_residual, args.downlink_compression_ratio
                )
                if epoch == 0: print(f"[SERVER] ä¸‹è¡Œå‹ç¼©: uniform (ratio={args.downlink_compression_ratio})")
            else:
                residual_to_broadcast_unpacked = compensated_downlink_residual # æ— å‹ç¼©
                if epoch == 0: print("[SERVER] ä¸‹è¡Œå‹ç¼©: none")

            # 3. å¤„ç†æ®‹å·®ç”¨äºä¼ è¾“ - æ”¯æŒå…¨é‡å’Œå‹ç¼©æ¨¡å¼
            if args.downlink_compression == 'uniform':
                # å‹ç¼©æ¨¡å¼ï¼šæ‰“åŒ…æ®‹å·®ç”¨äºä¼ è¾“å¹¶è®¡æ—¶
                pack_result = pack_sparse_residual(residual_to_broadcast_unpacked, enable_timing=True)
                residual_to_broadcast_packed, pack_time = pack_result
                total_downlink_pack_time += pack_time
                downlink_pack_count += 1
                print(f"[SERVER] ä¸‹è¡Œæ®‹å·®æ‰“åŒ…å®Œæˆï¼Œè€—æ—¶: {pack_time:.2f}ms")
            else:
                # å…¨é‡æ¨¡å¼ï¼šç›´æ¥ä¼ é€’å¯†é›†å¼ é‡ï¼Œè·³è¿‡æ‰“åŒ…æ­¥éª¤
                residual_to_broadcast_packed = copy.deepcopy(residual_to_broadcast_unpacked)
                print(f"[SERVER] å…¨é‡ä¸‹è¡Œä¼ è¾“ï¼Œè·³è¿‡æ‰“åŒ…æ­¥éª¤")

            # 4. æ›´æ–°ä¸‹è¡Œè¯¯å·®çŠ¶æ€ (Error Feedback State)
            if args.disable_downlink_ef:
                # EFè¢«ç¦ç”¨ï¼Œæ®‹å·®æ¸…é›¶
                server_downlink_error = {key: torch.zeros_like(param).cpu()
                                         for key, param in aggregated_residual.items()}
            else:
                # EFå¯ç”¨ï¼Œè®¡ç®—æœªå‘é€çš„éƒ¨åˆ†
                server_downlink_error = model_subtract(compensated_downlink_residual, residual_to_broadcast_unpacked)
            
            # 5. ç”¨å®Œæ•´çš„ã€æœªå‹ç¼©çš„èšåˆæ›´æ–°æœåŠ¡å™¨çš„é«˜ä¿çœŸåº¦å…¨å±€æ¨¡å‹
            # è¿™å¯¹äºå‡†ç¡®çš„æœåŠ¡å™¨ç«¯æµ‹è¯•è‡³å…³é‡è¦
            global_model.load_state_dict(updated_global_weights)
            global_weights = updated_global_weights
            
        else:
            print("âš ï¸ æ¨¡å‹æƒé‡åŒ…å«æ— æ•ˆå€¼ï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°ï¼Œä½¿ç”¨ä¸Šä¸€è½®æƒé‡")
            # ä¿æŒåŸæœ‰æƒé‡ä¸å˜ï¼Œè®¾ç½®ç©ºçš„ä¸‹è¡Œå¹¿æ’­
            residual_to_broadcast_packed = None
            global_weights = current_global_weights
        
        # è®¡ç®—å½“å‰è½®æ¬¡è€—æ—¶
        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        epoch_times.append(epoch_duration)
        
        # è®¡ç®—å¹³å‡è½®æ¬¡æ—¶é—´å’Œå‰©ä½™æ—¶é—´ä¼°è®¡
        avg_epoch_time = sum(epoch_times) / len(epoch_times)
        remaining_epochs = args.epochs - (epoch + 1)
        estimated_remaining_time = avg_epoch_time * remaining_epochs
        
        # è¯„ä¼°å½“å‰è½®æ¬¡æ€§èƒ½
        (current_test_acc, test_loss, loss_avg, ema_acc, best_ema_acc, 
         patience_counter, improve_streak, best_global_weights) = evaluate_epoch_performance(
            args, global_model, test_dataset, local_losses, 
            global_test_accuracy, train_loss, ema_acc, ema_alpha,
            best_ema_acc, patience_counter, improve_streak, patience,
            best_global_weights)

        # æ‰“å°è½®æ¬¡æ€»ç»“
        print_epoch_summary(epoch, args, current_test_acc, best_ema_acc, ema_acc, 
                           loss_avg, test_loss, epoch_duration, estimated_remaining_time)
        
        # æ‰“å°é€šä¿¡é‡æ±‡æ€»
        total_round_comm_cost = print_communication_summary(
            epoch, args, idxs_users, global_model, epoch_comm_cost,
            residual_to_broadcast_packed, communication_cost)

        # è®¡ç®—å½“å‰å­¦ä¹ ç‡ä»¥ä¾¿è®°å½•
        current_lr = calculate_current_learning_rate(args, epoch)
                
        history['epoch'].append(epoch + 1)
        history['test_accuracy'].append(current_test_acc)
        history['avg_train_loss'].append(loss_avg)
        history['learning_rate'].append(current_lr)
        # è®°å½•å‹ç¼©æ¯”ä¾‹ä¿¡æ¯ (ç®€åŒ–ä¸ºä¸»è¦å‹ç¼©æ¯”)
        main_compression_ratio = args.uplink_compression_ratio if args.uplink_compression == 'uniform' else 0.0
        history['compression_ratio'].append(main_compression_ratio)
        history['communication_cost'].append(total_round_comm_cost)  # <-- æ–°ä»£ç ï¼šè®°å½•åŒå‘æ€»å’Œ
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= patience:
            print(f'ğŸ›‘ Early stopping triggered after {epoch+1} global rounds')
            if best_global_weights is not None:
                global_model.load_state_dict(best_global_weights)
                print("Loaded best model weights for final testing.")
            break
        
        # æ›´æ–°tqdmæè¿°ä¿¡æ¯
        pbar.set_postfix({
            'Acc': f'{current_test_acc*100:.2f}%',
            'Loss': f'{loss_avg:.4f}',
            'Time': f'{epoch_duration:.1f}s',
            'ETA': f'{estimated_remaining_time/60:.1f}min'
        })
    
    # è¯„ä¼°æœ€ç»ˆæ¨¡å‹æ€§èƒ½
    test_acc, avg_local_test_acc_best_model = evaluate_final_model_performance(
        args, global_model, test_dataset, train_dataset, user_groups, epoch
    )
    
    # æ‰“å°å®Œæ•´è®­ç»ƒè¿‡ç¨‹é€šä¿¡é‡æ±‡æ€»
    print_final_communication_summary(
        args, global_model, communication_cost, epoch,
        total_pack_time, total_unpack_time, pack_count, unpack_count,
        total_downlink_pack_time, total_downlink_unpack_time, 
        downlink_pack_count, downlink_unpack_count, epoch_times
    )

    # ä¿å­˜å®éªŒç»“æœå’Œç”Ÿæˆå›¾åƒ
    save_experiment_results(args, history, train_loss, global_test_accuracy, communication_cost)
# ä¸»å‡½æ•°ç»“æŸ

def evaluate_final_model_performance(args, global_model, test_dataset, train_dataset, user_groups, epoch):
    """
    è¯„ä¼°æœ€ç»ˆæ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬å…¨å±€æµ‹è¯•å’Œå¹³å‡æœ¬åœ°æµ‹è¯•ã€‚
    
    Args:
        args: å®éªŒå‚æ•°
        global_model: å…¨å±€æ¨¡å‹
        test_dataset: æµ‹è¯•æ•°æ®é›†
        train_dataset: è®­ç»ƒæ•°æ®é›†
        user_groups: ç”¨æˆ·ç»„ç´¢å¼•
        epoch: å½“å‰è½®æ•°
        
    Returns:
        tuple: (test_acc, avg_local_test_acc_best_model)
    """
    print("\nè¯„ä¼°æœ€ç»ˆæ¨¡å‹æ€§èƒ½...")
    # æ³¨æ„ï¼šæ­¤æ—¶çš„global_modelå·²ç»æ˜¯æ—©åœæœºåˆ¶åŠ è½½çš„æœ€ä½³æ¨¡å‹
    test_acc, test_loss = test_inference(args, global_model, test_dataset)

    print("ä½¿ç”¨æœ€ä½³æ¨¡å‹è¯„ä¼°å¹³å‡æœ¬åœ°æµ‹è¯•æ€§èƒ½...")
    # éœ€è¦LocalUpdateæ¥è¯„ä¼°ï¼Œæˆ‘ä»¬ä»update.pyå¯¼å…¥
    from update import LocalUpdate
    list_acc_best_model = []
    global_model.eval()
    for c in range(args.num_users):
        local_model = LocalUpdate(args=args, dataset=train_dataset,
                                   idxs=user_groups[c], client_id=c)
        acc, _ = local_model.inference(model=global_model)
        list_acc_best_model.append(acc)
    avg_local_test_acc_best_model = sum(list_acc_best_model) / len(list_acc_best_model)

    print(f' \n Results after {epoch+1} global rounds of training:')
    print("|---- Avg Local Test Accuracy (Best Model): {:.2f}%".format(100*avg_local_test_acc_best_model))
    print("|---- Global Test Accuracy (Best Model): {:.2f}%".format(100*test_acc))
    
    return test_acc, avg_local_test_acc_best_model


def print_final_communication_summary(args, global_model, communication_cost, epoch,
                                     total_pack_time, total_unpack_time, pack_count, unpack_count,
                                     total_downlink_pack_time, total_downlink_unpack_time, 
                                     downlink_pack_count, downlink_unpack_count, epoch_times):
    """
    æ‰“å°å®Œæ•´è®­ç»ƒè¿‡ç¨‹çš„é€šä¿¡é‡æ±‡æ€»å’Œå‹ç¼©ç»Ÿè®¡ã€‚
    
    Args:
        args: å®éªŒå‚æ•°
        global_model: å…¨å±€æ¨¡å‹
        communication_cost: é€šä¿¡å¼€é”€åˆ—è¡¨
        epoch: å½“å‰è½®æ•°
        å„ç§æ—¶é—´ç»Ÿè®¡å‚æ•°
    """
    # è®¡ç®—å®Œæ•´è®­ç»ƒè¿‡ç¨‹çš„å‹ç¼©ç»Ÿè®¡
    total_comm_cost = sum(communication_cost)  # communication_cost ç°åœ¨è®°å½•çš„æ˜¯å­—èŠ‚æ•°
    print(f"|---- æ€»é€šä¿¡å¼€é”€: {total_comm_cost:,} å­—èŠ‚")
    
    # ================= æ•´ä½“è®­ç»ƒè¿‡ç¨‹é€šä¿¡é‡æ±‡æ€» =================
    total_rounds = epoch + 1  # å®é™…è®­ç»ƒè½®æ•°
    single_model_params = sum(torch.numel(param) for _, param in global_model.named_parameters())
    single_model_bytes = single_model_params * 4
    avg_selected_clients = args.frac * args.num_users
    
    print(f"\nğŸ¯ æ•´ä½“è®­ç»ƒè¿‡ç¨‹é€šä¿¡é‡æ±‡æ€»:")
    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å‹ç¼©å¯ç”¨
    has_compression = (args.uplink_compression == 'uniform') or (args.downlink_compression == 'uniform')
    if has_compression:
        # ç†è®ºåŸºå‡†åº”è€ƒè™‘ä¸Šè¡Œå’Œä¸‹è¡Œ
        # ä¸Šè¡ŒåŸºå‡†ï¼šavg_clients * rounds * model_size
        theoretical_uplink_baseline_total = int(single_model_bytes * avg_selected_clients * total_rounds)
        # ä¸‹è¡ŒåŸºå‡†ï¼šall_clients * rounds * model_size (å› ä¸ºæ¯è½®å¹¿æ’­ç»™æ‰€æœ‰å®¢æˆ·ç«¯)
        theoretical_downlink_baseline_total = int(single_model_bytes * args.num_users * total_rounds)

        theoretical_total_baseline_bytes = theoretical_uplink_baseline_total + theoretical_downlink_baseline_total

        # total_comm_cost æ˜¯æ¯è½®åŒå‘é€šä¿¡çš„å’Œï¼Œæ˜¯æ­£ç¡®çš„
        overall_compression_effectiveness = (1 - total_comm_cost / theoretical_total_baseline_bytes) * 100 if theoretical_total_baseline_bytes > 0 else 0

        print(f"   â€¢ å®é™…æ€»ä¼ è¾“å­—èŠ‚æ•°: {int(total_comm_cost):,} B")
        print(f"   â€¢ ç†è®ºåŸºå‡†æ€»å­—èŠ‚æ•°: {theoretical_total_baseline_bytes:,} B")
        print(f"     - ä¸Šè¡ŒåŸºå‡†: {theoretical_uplink_baseline_total:,} B (é€‰ä¸­å®¢æˆ·ç«¯ Ã— {total_rounds} è½® Ã— {single_model_bytes:,} B)")
        print(f"     - ä¸‹è¡ŒåŸºå‡†: {theoretical_downlink_baseline_total:,} B (æ‰€æœ‰å®¢æˆ·ç«¯ Ã— {total_rounds} è½® Ã— {single_model_bytes:,} B)")
        print(f"   â€¢ æ•´ä½“å‹ç¼©æ•ˆæœ: å‡å°‘ {overall_compression_effectiveness:.2f}% é€šä¿¡é‡")
        print(f"   â€¢ æ€»è®­ç»ƒè½®æ•°: {total_rounds}")
        print(f"   â€¢ æ€»å®¢æˆ·ç«¯æ•°: {args.num_users}")
        print(f"   â€¢ å•ä¸ªæ¨¡å‹å‚æ•°é‡: {single_model_params:,} ({single_model_bytes:,} B)")
        
        # <--- åŒå‘å‹ç¼©æ—¶é—´ç»Ÿè®¡ï¼ˆå¹¶è¡Œæ“ä½œï¼‰ --->
        print(f"\nâ±ï¸ ä¸Šè¡Œå‹ç¼©æ—¶é—´ç»Ÿè®¡ (å¹¶è¡Œæ“ä½œ):")
        print(f"   â€¢ æ€»æ‰“åŒ…æ—¶é—´ (å®¢æˆ·ç«¯å¹¶è¡Œ): {total_pack_time:.2f}ms (æ‰§è¡Œ {pack_count} è½®)")
        print(f"   â€¢ æ€»è§£åŒ…æ—¶é—´ (æœåŠ¡å™¨èšåˆ): {total_unpack_time:.2f}ms (æ‰§è¡Œ {unpack_count} æ¬¡)")
            
        print(f"\nâ±ï¸ ä¸‹è¡Œå‹ç¼©æ—¶é—´ç»Ÿè®¡ (å¹¶è¡Œæ“ä½œ):")
        print(f"   â€¢ æ€»æ‰“åŒ…æ—¶é—´ (æœåŠ¡å™¨): {total_downlink_pack_time:.2f}ms (æ‰§è¡Œ {downlink_pack_count} è½®)")
        print(f"   â€¢ æ€»è§£åŒ…æ—¶é—´ (å®¢æˆ·ç«¯å¹¶è¡Œ): {total_downlink_unpack_time:.2f}ms (æ‰§è¡Œ {downlink_unpack_count} è½®)")
            
        total_compression_time = total_pack_time + total_unpack_time + total_downlink_pack_time + total_downlink_unpack_time
        training_duration_ms = sum(epoch_times) * 1000
        print(f"\n   â€¢ åŒå‘å‹ç¼©æ€»æ—¶é—´: {total_compression_time:.2f}ms")
        print(f"   â€¢ å‹ç¼©æ—¶é—´å è®­ç»ƒæ€»æ—¶é—´æ¯”ä¾‹: {total_compression_time / training_duration_ms * 100:.3f}%")
    else:
        theoretical_total_baseline_bytes = int(single_model_bytes * avg_selected_clients * total_rounds)
        print(f"   â€¢ å¯†é›†ä¼ è¾“æ€»å­—èŠ‚æ•°: {total_comm_cost:,} B")
        print(f"   â€¢ ç†è®ºåŸºå‡†æ€»å­—èŠ‚æ•°: {theoretical_total_baseline_bytes:,} B")
        
        # åŒå‘å‹ç¼©æ—¶é—´ç»Ÿè®¡
        total_all_pack_time = total_pack_time + total_downlink_pack_time
        total_all_unpack_time = total_unpack_time + total_downlink_unpack_time
        total_all_process_time = total_all_pack_time + total_all_unpack_time
        total_training_time_ms = sum(epoch_times) * 1000
        
        print(f"\nâ±ï¸ åŒå‘å‹ç¼©æ—¶é—´ç»Ÿè®¡ (å¹¶è¡Œæ“ä½œ):")
        print(f"   ğŸ“¤ ä¸Šè¡Œé€šä¿¡:")
        print(f"      â€¢ å®¢æˆ·ç«¯æ‰“åŒ…æ—¶é—´ (å¹¶è¡Œ): {total_pack_time:.2f}ms (æ‰§è¡Œ {pack_count} è½®)")
        print(f"      â€¢ æœåŠ¡å™¨è§£åŒ…æ—¶é—´: {total_unpack_time:.2f}ms (æ‰§è¡Œ {unpack_count} æ¬¡)")
        
        print(f"   ğŸ“¥ ä¸‹è¡Œé€šä¿¡:")
        print(f"      â€¢ æœåŠ¡å™¨æ‰“åŒ…æ—¶é—´: {total_downlink_pack_time:.2f}ms (æ‰§è¡Œ {downlink_pack_count} è½®)")
        print(f"      â€¢ å®¢æˆ·ç«¯è§£åŒ…æ—¶é—´ (å¹¶è¡Œ): {total_downlink_unpack_time:.2f}ms (æ‰§è¡Œ {downlink_unpack_count} è½®)")
        
        print(f"   ğŸ”„ æ€»è®¡:")
        print(f"      â€¢ æ€»æ‰“åŒ…æ—¶é—´: {total_all_pack_time:.2f}ms")
        print(f"      â€¢ æ€»è§£åŒ…æ—¶é—´: {total_all_unpack_time:.2f}ms")
        print(f"      â€¢ æ€»å¤„ç†æ—¶é—´: {total_all_process_time:.2f}ms")
        print(f"      â€¢ å¤„ç†æ—¶é—´å è®­ç»ƒæ€»æ—¶é—´æ¯”ä¾‹: {total_all_process_time / total_training_time_ms * 100:.3f}%")
        print(f"   â€¢ æ€»è®­ç»ƒè½®æ•°: {total_rounds}")
        print(f"   â€¢ å•ä¸ªæ¨¡å‹å‚æ•°é‡: {single_model_params:,} ({single_model_bytes:,} B)")


def save_experiment_results(args, history, train_loss, global_test_accuracy, communication_cost):
    """
    ä¿å­˜å®éªŒç»“æœï¼ŒåŒ…æ‹¬CSVå†å²è®°å½•ã€è¯¦æƒ…æ–‡ä»¶ã€å›¾åƒç”Ÿæˆå’Œpickleæ–‡ä»¶ã€‚
    
    Args:
        args: å®éªŒå‚æ•°
        history: è®­ç»ƒå†å²è®°å½•
        train_loss: è®­ç»ƒæŸå¤±
        global_test_accuracy: å…¨å±€æµ‹è¯•å‡†ç¡®ç‡
        communication_cost: é€šä¿¡å¼€é”€
    """
    # --- ä¿®æ”¹å¼€å§‹ï¼šå°†å†å²è®°å½•ä¿å­˜ä¸ºCSVæ–‡ä»¶ ---
    # åˆ›å»ºæ—¶é—´æˆ³å‘½åçš„æ–‡ä»¶å¤¹
    current_time = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_dir = os.path.join('./save/logs', current_time)
    os.makedirs(log_dir, exist_ok=True)

    # CSVæ–‡ä»¶åä¸æ–‡ä»¶å¤¹åä¿æŒä¸€è‡´ï¼ˆéƒ½ä½¿ç”¨æ—¶é—´æˆ³ï¼‰
    log_filename = f'{current_time}.csv'
    log_path = os.path.join(log_dir, log_filename)
    
    # ä¿å­˜å®éªŒè¯¦æƒ…åˆ°åŒä¸€æ–‡ä»¶å¤¹
    iid_str = 'iid' if args.iid else 'noniid'
    
    # ç”Ÿæˆå‹ç¼©æè¿°å­—ç¬¦ä¸²
    uplink_comp = f"uplink_{args.uplink_compression_ratio}" if args.uplink_compression == 'uniform' else "uplink_none"
    downlink_comp = f"downlink_{args.downlink_compression_ratio}" if args.downlink_compression == 'uniform' else "downlink_none"
    comp_str = f"{uplink_comp}_{downlink_comp}"
    
    details_content = f"""å®éªŒæ—¶é—´: {current_time}
                          å®éªŒç±»å‹: Residual Federated Learning (Ablation)
                          æ•°æ®é›†: {args.dataset.upper()}
                          æ¨¡å‹: {args.model.upper()}
                          è®­ç»ƒè½®æ•°: {args.epochs}
                          æ•°æ®åˆ†å¸ƒ: {iid_str.upper()}
                          å­¦ä¹ ç‡: {args.lr}
                          æœ¬åœ°è®­ç»ƒè½®æ•°: {args.local_ep}
                          å‚ä¸å®¢æˆ·ç«¯æ•°: {args.num_users}
                          å‚ä¸æ¯”ä¾‹: {args.frac}
                          å®¢æˆ·ç«¯é€‰æ‹©: {args.selection_method}
                          ä¸Šè¡Œå‹ç¼©: {args.uplink_compression} ({args.uplink_compression_ratio if args.uplink_compression == 'uniform' else 'N/A'})
                          ä¸‹è¡Œå‹ç¼©: {args.downlink_compression} ({args.downlink_compression_ratio if args.downlink_compression == 'uniform' else 'N/A'})
                          ä¸Šè¡Œè¯¯å·®åé¦ˆ: {'ç¦ç”¨' if args.disable_uplink_ef else 'å¯ç”¨'}
                          ä¸‹è¡Œè¯¯å·®åé¦ˆ: {'ç¦ç”¨' if args.disable_downlink_ef else 'å¯ç”¨'}"""
    details_path = os.path.join(log_dir, 'experiment_details.txt')
    with open(details_path, 'w', encoding='utf-8') as f:
        f.write(details_content)
    
    df = pd.DataFrame(history)
    df.to_csv(log_path, index=False)
    print(f"ğŸ“ˆ è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {log_path}")
    print(f"ğŸ“‹ å®éªŒè¯¦æƒ…å·²ä¿å­˜åˆ°: {details_path}")
    
    # è‡ªåŠ¨ç”Ÿæˆå›¾åƒ
    try:
        from visualize_results import plot_single_experiment
        plots_dir = './save/plots'
        plot_result = plot_single_experiment(log_path, plots_dir)
        if plot_result:
            print(f"ğŸ“Š å®éªŒå›¾åƒå·²è‡ªåŠ¨ç”Ÿæˆåˆ°: {plot_result}")
        else:
            print("âš ï¸ å›¾åƒç”Ÿæˆå¤±è´¥")
    except Exception as e:
        print(f"âš ï¸ è‡ªåŠ¨ç”Ÿæˆå›¾åƒæ—¶å‡ºé”™: {e}")
        print("ğŸ’¡ ä½ å¯ä»¥æ‰‹åŠ¨è¿è¡Œ: python visualize_results.py --single " + log_path)
    # --- ä¿®æ”¹ç»“æŸ ---

    # ä¿å­˜ç»“æœ
    save_dir = './save/objects'
    os.makedirs(save_dir, exist_ok=True)

    file_name = './save/objects/residual_{}_{}_{}_C[{}]_iid[{}]_E[{}]_B[{}]_comp[{}].pkl'.\
        format(args.dataset, args.model, args.epochs, args.frac, args.iid,
               args.local_ep, args.local_bs, 'smart' if getattr(args, 'compression', 'none') == 'smart' else 'uniform' if getattr(args, 'compression', 'none') == 'uniform' else 'none')

    with open(file_name, 'wb') as f:
        pickle.dump([train_loss, global_test_accuracy, communication_cost], f)

    print(f'è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {file_name}')


def print_training_details(args, ema_alpha, patience):
    """
    æ‰“å°æ®‹å·®è”é‚¦å­¦ä¹ çš„è®­ç»ƒè¯¦æƒ…å’Œå¯ç”¨çš„é«˜çº§ç‰¹æ€§ã€‚
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡ (åŒ…å«æ¶ˆèå®éªŒå‚æ•°)
        ema_alpha (float): EMA å¹³æ»‘å› å­
        patience (int): æ—©åœæœºåˆ¶çš„è€å¿ƒå€¼
    """
    print(f"=== æ¶ˆèå®éªŒé…ç½® ===")
    print(f"ğŸ¯ å®¢æˆ·ç«¯é€‰æ‹©ç­–ç•¥: {args.selection_method}")
    print(f"ğŸ“¤ ä¸Šè¡Œå‹ç¼©: {args.uplink_compression} ({args.uplink_compression_ratio if args.uplink_compression == 'uniform' else 'N/A'})")
    print(f"ğŸ“¥ ä¸‹è¡Œå‹ç¼©: {args.downlink_compression} ({args.downlink_compression_ratio if args.downlink_compression == 'uniform' else 'N/A'})")
    print(f"ğŸ”„ ä¸Šè¡Œè¯¯å·®åé¦ˆ: {'ç¦ç”¨' if args.disable_uplink_ef else 'å¯ç”¨'}")
    print(f"ğŸ”„ ä¸‹è¡Œè¯¯å·®åé¦ˆ: {'ç¦ç”¨' if args.disable_downlink_ef else 'å¯ç”¨'}")
    
    print(f"\n=== å¯ç”¨çš„é«˜çº§ç‰¹æ€§ï¼ˆæ®‹å·®è”é‚¦å­¦ä¹ ï¼‰===")
    print(f"ğŸ“Š EMAå¹³æ»‘å‡†ç¡®ç‡: å¯ç”¨ (Î±={ema_alpha})")
    
    # Label Smoothing æ£€æŸ¥
    criterion_type = getattr(args, 'criterion', 'crossentropy')
    label_smoothing_enabled = criterion_type == 'label_smoothing'
    smoothing_value = getattr(args, 'smoothing', 0.0)
    print(f"ğŸ¯ Label Smoothing: {'å¯ç”¨' if label_smoothing_enabled and smoothing_value > 0 else 'ç¦ç”¨'}")
    
    # SWA æ£€æŸ¥
    swa_enabled = getattr(args, 'enable_swa', 0) == 1
    swa_start = getattr(args, 'swa_start', 150)
    print(f"ğŸ”„ SWA: {'å¯ç”¨' if swa_enabled else 'ç¦ç”¨'}")
    
    # CutMix æ£€æŸ¥
    cutmix_enabled = getattr(args, 'enable_cutmix', 0) == 1
    cutmix_prob = getattr(args, 'cutmix_prob', 0.0)
    print(f"ğŸ”€ CutMix: {'å¯ç”¨' if cutmix_enabled and cutmix_prob > 0 else 'ç¦ç”¨'}")
    
    # Mixup æ£€æŸ¥
    mixup_enabled = getattr(args, 'enable_mixup', 0) == 1
    mixup_alpha = getattr(args, 'mixup_alpha', 0.0)
    print(f"ğŸ¨ Mixup: {'å¯ç”¨' if mixup_enabled and mixup_alpha > 0 else 'ç¦ç”¨'}")
    
    # çŸ¥è¯†è’¸é¦æ£€æŸ¥
    kd_enabled = getattr(args, 'enable_knowledge_distillation', 0) == 1
    print(f"ğŸ§  çŸ¥è¯†è’¸é¦: {'å¯ç”¨' if kd_enabled else 'ç¦ç”¨'}")
    
    # å­¦ä¹ ç‡è°ƒåº¦æ£€æŸ¥
    lr_scheduler_type = getattr(args, 'lr_scheduler', 'none')
    print(f"ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦: {lr_scheduler_type}")
    
    # èšåˆç­–ç•¥æ£€æŸ¥
    aggregation_method = getattr(args, 'adaptive_aggregation', 'standard')
    print(f"ğŸ¤ èšåˆç­–ç•¥: {aggregation_method}")
    
    print(f"========================================")
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ å¼€å§‹æ®‹å·®è”é‚¦å­¦ä¹ è®­ç»ƒ")
    print(f"ğŸ“Š æ€»è½®æ¬¡: {args.epochs}, å®¢æˆ·ç«¯: {args.num_users}, å‚ä¸æ¯”ä¾‹: {args.frac}")
    print(f"ğŸ“‹ æ—©åœæœºåˆ¶: è€å¿ƒå€¼ = {patience} è½®")
    print(f"{'='*70}")

def select_clients(epoch, args, user_groups, client_ema_losses, EMA_ALPHA, m):
    """
    æ ¹æ®å†å²è¡¨ç°å’Œå¤šæ ·æ€§é€‰æ‹©å®¢æˆ·ç«¯ã€‚

    Args:
        epoch (int): å½“å‰è½®æ¬¡
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        user_groups (dict): æ¯ä¸ªå®¢æˆ·ç«¯çš„æ•°æ®ç´¢å¼•
        client_ema_losses (dict): æ¯ä¸ªå®¢æˆ·ç«¯çš„EMAæŸå¤±
        EMA_ALPHA (float): EMAå¹³æ»‘å› å­
        m (int): æ¯è½®é€‰æ‹©çš„å®¢æˆ·ç«¯æ•°é‡

    Returns:
        idxs_users (list): é€‰ä¸­çš„å®¢æˆ·ç«¯ç´¢å¼•åˆ—è¡¨
    """
    # æ£€æŸ¥æ˜¯ 'smart' æ¨¡å¼ä¸”å·²è¿‡é¢„çƒ­æœŸ
    if args.selection_method == 'smart' and epoch > 2:
        # åŸºäºå†å²è¡¨ç°çš„æ™ºèƒ½å®¢æˆ·ç«¯é€‰æ‹©
        client_weights = []
        
        # è®¡ç®—ä¸€æ¬¡å…¨å±€æœ€å¤§æ•°æ®é‡ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤è®¡ç®—
        max_data_size = max(len(user_groups[i]) for i in range(args.num_users))
        
        for idx in range(args.num_users):
            # æ›´æ–°å®¢æˆ·ç«¯çš„EMAæŸå¤±
            last_loss = np.mean(args.client_history['losses'].get(idx, [1.0]))
            if idx not in client_ema_losses:
                client_ema_losses[idx] = last_loss
            else:
                client_ema_losses[idx] = EMA_ALPHA * last_loss + (1 - EMA_ALPHA) * client_ema_losses[idx]

            # ä½¿ç”¨EMAæŸå¤±æ¥è®¡ç®—å¾—åˆ†
            current_ema_loss = client_ema_losses[idx]
            loss_score = 1.0 / (1.0 + current_ema_loss)  # ä½¿ç”¨å¹³æ»‘åçš„æŸå¤±
            
            # æ•°æ®é‡æƒé‡
            data_size = len(user_groups[idx])
            data_score = data_size / max_data_size
            '''
            è®¡ç®—å½“å‰å®¢æˆ·ç«¯æ‹¥æœ‰çš„æ•°æ®æ ·æœ¬é‡(data_size)
            å¹¶å°†å…¶ä¸æ‰€æœ‰å®¢æˆ·ç«¯ä¸­æœ€å¤§çš„æ•°æ®é‡è¿›è¡Œæ¯”è¾ƒ,å¾—å‡ºä¸€ä¸ª0åˆ°1ä¹‹é—´çš„å½’ä¸€åŒ–åˆ†æ•°
            '''
            
            # é¿å…è¿‡åº¦é€‰æ‹©åŒä¸€å®¢æˆ·ç«¯ - æ›´ç²¾ç¡®çš„é¢‘ç‡æƒ©ç½š
            # è®¡ç®—æœ€è¿‘å‡ è½®ä¸­è¯¥å®¢æˆ·ç«¯è¢«é€‰ä¸­çš„æ¬¡æ•°
            recent_window = min(6 * m, len(args.client_history['last_selected']))  # æœ€è¿‘6è½®çš„é€‰æ‹©
            '''
            å®šä¹‰æˆ‘ä»¬è¦å›é¡¾çš„â€œå†å²è®°å½•â€æœ‰å¤šé•¿ã€‚6 * m æ„å‘³ç€æˆ‘ä»¬å…³æ³¨æœ€è¿‘6è½®é€‰æ‹©çš„æ‰€æœ‰å®¢æˆ·ç«¯
            åœ¨è®­ç»ƒåˆšå¼€å§‹æ—¶(æ¯”å¦‚æ‰ç¬¬3è½®),æ€»å…±ä¹Ÿåªé€‰äº†30ä¸ªå®¢æˆ·ç«¯,å†å²è®°å½•æ²¡æœ‰60æ¡é‚£ä¹ˆé•¿
            minå‡½æ•°ç¡®ä¿æˆ‘ä»¬ä¸ä¼šè¯•å›¾æŸ¥çœ‹ä¸å­˜åœ¨çš„å†å²è®°å½•
            '''
            
            # args.client_historyæ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«å¤šä¸ªé”®
            '''
            args.client_history = {
            'losses': {},           # æ¯ä¸ªå®¢æˆ·ç«¯çš„å†å²æŸå¤±(å­—å…¸,é”®ä¸ºå®¢æˆ·ç«¯ID,å€¼ä¸ºæŸå¤±åˆ—è¡¨ï¼‰
            'last_selected': [],    # æ‰€æœ‰å†å²é€‰æ‹©çš„å®¢æˆ·ç«¯ï¼ˆåˆ—è¡¨ï¼Œç”¨äºç®€å•é¢‘ç‡ç»Ÿè®¡ï¼‰
            'round_selections': [], # æŒ‰è½®è®°å½•çš„é€‰æ‹©å†å²(åˆ—è¡¨,æ¯è½®è®°å½•ä¸€ä¸ªå®¢æˆ·ç«¯IDåˆ—è¡¨)
            'performance_scores': {} # (å¯é€‰)æ¯ä¸ªå®¢æˆ·ç«¯çš„æ€§èƒ½åˆ†æ•°(å­—å…¸,é”®ä¸ºå®¢æˆ·ç«¯ID,å€¼ä¸ºåˆ†æ•°)
            }
            '''
            recent_selections = args.client_history['last_selected'][-recent_window:] if recent_window > 0 else []
            frequency_penalty = 1.0 - (recent_selections.count(idx) * 0.15)  
            frequency_penalty = max(frequency_penalty, 0.2)  
            # recent_selections.count(idx)ï¼šæ•°ä¸€ä¸‹åœ¨åˆšåˆšé‚£å¼ â€œå°çº¸æ¡â€ä¸Šï¼Œè¿™ä¸ªå®¢æˆ· idx çš„åå­—å‡ºç°äº†å‡ æ¬¡
            # æœ€å°ä¿æŒ20%æƒé‡ï¼Œä¸ºäº†é˜²æ­¢ä¸€ä¸ªæ€§èƒ½ç‰¹åˆ«å¥½çš„å®¢æˆ·ç«¯å› ä¸ºè¢«é¢‘ç¹é€‰ä¸­è€Œå¯¼è‡´å…¶åˆ†æ•°è¿‡ä½ï¼Œè¢«å®Œå…¨â€œå°æ€â€
            '''
            ä»£ç ä¼šè§‚å¯Ÿä¸€ä¸ªåŠ¨æ€çš„â€œæœ€è¿‘å†å²çª—å£â€(å¤§è‡´æ˜¯æœ€è¿‘6è½®è¢«é€‰ä¸­çš„æ‰€æœ‰å®¢æˆ·ç«¯åˆ—è¡¨),å¹¶æ£€æŸ¥å½“å‰çš„å®¢æˆ·ç«¯(idx)åœ¨è¿™ä¸ªçª—å£ä¸­å‡ºç°äº†å¤šå°‘æ¬¡ã€‚
            æ¯å‡ºç°ä¸€æ¬¡,å®ƒçš„â€œé¢‘ç‡æƒ©ç½šâ€å¾—åˆ†å°±ä¼šé™ä½0.15ã€‚å¦‚æœä¸€ä¸ªå®¢æˆ·ç«¯æœ€è¿‘é¢‘ç¹è¢«é€‰ä¸­ï¼Œå®ƒçš„è¿™ä¸ªåˆ†æ•°å°±ä¼šå¾ˆä½ã€‚
            ä¸€ä¸ªå®¢æˆ·ç«¯æœ€è¿‘è¢«é€‰ä¸­çš„æ¬¡æ•°è¶Šå¤šï¼Œå®ƒçš„ frequency_penalty å¾—åˆ†å°±è¶Šä½ã€‚
            '''
            
            # ç»¼åˆå¾—åˆ†
            combined_score = (0.5 * loss_score + 0.25 * data_score + 0.25 * frequency_penalty)
            client_weights.append(combined_score)
        
        # æ¦‚ç‡é€‰æ‹©ç¡®ä¿å¤šæ ·æ€§
        client_weights = np.array(client_weights)
        client_probs = client_weights / client_weights.sum()
        
        idxs_users = np.random.choice(range(args.num_users), m, replace=False, p=client_probs)
        
        # æ˜¾ç¤ºé€‰æ‹©è¯¦æƒ…ï¼ˆä»…åœ¨verboseæ¨¡å¼ä¸‹ï¼‰
        if args.verbose:
            selected_scores = [client_weights[idx] for idx in idxs_users]
            print(f'ğŸ§  æ™ºèƒ½é€‰æ‹©å®¢æˆ·ç«¯: {list(idxs_users)} (æƒé‡: {[f"{s:.3f}" for s in selected_scores]})')
        else:
            print(f'ğŸ§  æ™ºèƒ½é€‰æ‹©å®¢æˆ·ç«¯: {list(idxs_users)}')
    else:
        # éšæœºé€‰æ‹© (ç”¨äº '--selection_method random' æˆ– 'smart' æ¨¡å¼çš„é¢„çƒ­)
        idxs_users = np.random.choice(range(args.num_users), m, replace=False)
        
        if args.selection_method == 'random':
            print(f'ğŸ² éšæœºé€‰æ‹©å®¢æˆ·ç«¯: {list(idxs_users)} (ç”¨æˆ·æŒ‡å®šä½¿ç”¨éšæœºé€‰æ‹©å®¢æˆ·ç«¯çš„æ–¹æ³•)')
        else:
            print(f'ğŸ² éšæœºé€‰æ‹©å®¢æˆ·ç«¯: {list(idxs_users)} (ç”¨æˆ·æŒ‡å®šä½¿ç”¨æ™ºèƒ½å®¢æˆ·ç«¯é€‰æ‹©æ–¹æ³•,æ­¤æ—¶æ­£åœ¨é¢„çƒ­)')
    
    # è®°å½•é€‰æ‹©çš„å®¢æˆ·ç«¯å†å²
    args.client_history['last_selected'].extend(idxs_users.tolist())
    args.client_history['round_selections'].append(idxs_users.tolist())  # æŒ‰è½®è®°å½•
    
    # ç»´æŠ¤åˆç†çš„å†å²çª—å£å¤§å°
    if len(args.client_history['last_selected']) > (m * 8):  # ä¿æŒ8è½®çš„å†å²
        args.client_history['last_selected'] = args.client_history['last_selected'][-(m*8):]
    if len(args.client_history['round_selections']) > 8:  # ä¿æŒæœ€è¿‘8è½®çš„è½®çº§è®°å½•
        args.client_history['round_selections'] = args.client_history['round_selections'][-8:]
    '''
    'last_selected': [0, 1, 2, 0, 3, 1, 4, 2]
    è¡¨ç¤ºåœ¨è¿‡å»çš„å‡ è½®ä¸­ï¼Œå®¢æˆ·ç«¯ 0ã€1ã€2ã€3ã€4 è¢«é€‰æ‹©çš„æ¬¡æ•°
    'round_selections': [[0, 1, 2], [1, 3, 4], [0, 2, 4]]
    è¡¨ç¤ºï¼š
    ç¬¬ 1 è½®é€‰æ‹©äº†å®¢æˆ·ç«¯ 0ã€1ã€2
    ç¬¬ 2 è½®é€‰æ‹©äº†å®¢æˆ·ç«¯ 1ã€3ã€4
    ç¬¬ 3 è½®é€‰æ‹©äº†å®¢æˆ·ç«¯ 0ã€2ã€4
    '''
    
    print(f"ğŸ¯ é€‰ä¸­çš„å®¢æˆ·ç«¯: {sorted(idxs_users)} (å…±{len(idxs_users)}ä¸ª)")
    return idxs_users

def setup_device(args):
    """
    è®¾ç½®è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰ã€‚
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡ï¼ŒåŒ…å«GPUè®¾ç½®ã€‚
    
    Returns:
        device: torch.device å¯¹è±¡ï¼Œè¡¨ç¤ºä½¿ç”¨çš„è®¾å¤‡ã€‚
    """
    if args.gpu is not None and torch.cuda.is_available():
        try:
            # ç¡®ä¿args.gpuæ˜¯æœ‰æ•ˆçš„æ•´æ•°
            gpu_id = int(args.gpu)
            if gpu_id >= 0 and gpu_id < torch.cuda.device_count():
                torch.cuda.set_device(gpu_id)
                device = torch.device(f'cuda:{gpu_id}')
                print(f"ä½¿ç”¨GPUè®¾å¤‡: {device}")
                
                # æµ‹è¯•GPUæ˜¯å¦æ­£å¸¸å·¥ä½œ
                test_tensor = torch.randn(1, 1, 28, 28).to(device)
                print(f"GPUæµ‹è¯•æˆåŠŸï¼Œå¯ç”¨å†…å­˜: {torch.cuda.get_device_properties(device).total_memory / 1024**2:.0f}MB")
                del test_tensor
                torch.cuda.empty_cache()
            else:
                print(f"GPU ID {gpu_id} æ— æ•ˆï¼Œå¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}, åˆ‡æ¢åˆ°CPU")
                device = torch.device('cpu')
        except Exception as e:
            print(f"GPU {args.gpu} åˆå§‹åŒ–å¤±è´¥: {str(e)}, åˆ‡æ¢åˆ°CPU")
            device = torch.device('cpu')
    else:
        if args.gpu is not None:
            print(f"è¯·æ±‚ä½¿ç”¨GPU {args.gpu}ï¼Œä½†CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        device = torch.device('cpu')
        print("ä½¿ç”¨CPUè®¾å¤‡")
    
    print(f"[DEBUG] è®¾å¤‡è®¾ç½®å®Œæˆ: {device}")
    return device

def build_model(args, train_dataset):
    """
    æ„å»ºå…¨å±€æ¨¡å‹ã€‚
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡ï¼ŒåŒ…å«æ¨¡å‹å’Œæ•°æ®é›†è®¾ç½®ã€‚
        train_dataset: è®­ç»ƒæ•°æ®é›†ï¼Œç”¨äºç¡®å®šè¾“å…¥ç»´åº¦ï¼ˆå¦‚MLPæ¨¡å‹ï¼‰ã€‚
    
    Returns:
        global_model: æ„å»ºå¥½çš„å…¨å±€æ¨¡å‹ã€‚
    """
    print(f"[DEBUG] å¼€å§‹æ„å»ºæ¨¡å‹: {args.model} for dataset: {args.dataset}")
    print(f"æ­£åœ¨æ„å»ºæ¨¡å‹: {args.model} for dataset: {args.dataset}")
    
    try:
        print("[DEBUG] å°è¯•åŠ è½½ä¼˜åŒ–æ¨¡å‹...")
        if args.model in ['cnn']:
            print(f"[DEBUG] æ ‡å‡†CNNæ¨¡å‹ - æ•°æ®é›†: {args.dataset}")
            if args.dataset == 'mnist':
                global_model = get_model('mnist', 'cnn')
            elif args.dataset == 'cifar':
                global_model = get_model('cifar10', 'cnn')
            else:
                raise ValueError(f"æ ‡å‡†CNNä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
                
        elif args.model in ['optimized', 'cnn_optimized', 'cnn_opt', 'cnn_enhanced']:
            print(f"[DEBUG] ä¼˜åŒ–CNNæ¨¡å‹ - æ•°æ®é›†: {args.dataset}")
            if args.dataset == 'mnist':
                global_model = get_model('mnist', 'optimized')
            elif args.dataset == 'cifar':
                global_model = get_model('cifar10', 'cnn')
            else:
                raise ValueError(f"ä¼˜åŒ–CNNç›®å‰ä»…æ”¯æŒMNISTå’ŒCIFARæ•°æ®é›†")
                
        elif args.model in ['optimized_gn', 'cnn_optimized_gn', 'groupnorm']:
            print(f"[DEBUG] GroupNormä¼˜åŒ–CNNæ¨¡å‹ - æ•°æ®é›†: {args.dataset}")
            if args.dataset == 'mnist':
                global_model = get_model('mnist', 'optimized_gn', num_groups=getattr(args, 'num_groups', 8))
            elif args.dataset == 'cifar':
                global_model = get_model('cifar10', 'resnet18_fed', use_groupnorm=True, num_groups=getattr(args, 'num_groups', 8))
            elif args.dataset == 'cifar100':
                use_groupnorm = getattr(args, 'use_groupnorm', True)
                num_groups = getattr(args, 'num_groups', 8)
                global_model = get_model('cifar100', 'resnet18_fed', 
                                        use_groupnorm=use_groupnorm, 
                                        num_groups=num_groups)
            else:
                raise ValueError(f"GroupNormä¼˜åŒ–CNNç›®å‰æ”¯æŒMNISTã€CIFAR-10å’ŒCIFAR-100æ•°æ®é›†")
                
        elif args.model in ['resnet18', 'resnet18_fed', 'resnet', 'resnet_mini', 'resnet18_gn']:
            if args.dataset == 'mnist':
                global_model = get_model('mnist', 'optimized')
            elif args.dataset == 'cifar':
                if args.model == 'resnet18_gn':
                    global_model = get_model('cifar10', 'resnet18_fed', use_groupnorm=True, num_groups=getattr(args, 'num_groups', 8))
                else:
                    global_model = get_model('cifar10', 'resnet18_fed')
            elif args.dataset == 'cifar100':
                if args.model == 'resnet18_gn':
                    use_groupnorm = getattr(args, 'use_groupnorm', True)
                    num_groups = getattr(args, 'num_groups', 8)
                    global_model = get_model('cifar100', 'resnet18_fed', 
                                            use_groupnorm=use_groupnorm, 
                                            num_groups=num_groups)
                else:
                    global_model = get_model('cifar100', 'resnet18_fed')
            else:
                raise ValueError(f"ResNet18ä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
                
        elif args.model in ['efficientnet', 'efficient']:
            if args.dataset == 'cifar':
                global_model = get_model('cifar10', 'efficientnet')
            elif args.dataset == 'cifar100':
                global_model = get_model('cifar100', 'efficientnet')
            else:
                raise ValueError(f"EfficientNetä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
                
        elif args.model == 'densenet':
            if args.dataset == 'cifar100':
                use_attention = getattr(args, 'use_attention', True)
                use_groupnorm = getattr(args, 'use_groupnorm', True)
                global_model = get_model('cifar100', 'densenet', 
                                        use_attention=use_attention, 
                                        use_groupnorm=use_groupnorm)
            else:
                raise ValueError(f"DenseNetä¸æ”¯æŒæ•°æ®é›†: {args.dataset}")
        else:
            raise ValueError("å°è¯•åŸæœ‰æ¨¡å‹")
            
        print(f"[SUCCESS] æˆåŠŸåŠ è½½ä¼˜åŒ–æ¨¡å‹: {global_model.__class__.__name__}")
        
    except Exception as e:
        print(f"[WARNING] ä¼˜åŒ–æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"[INFO] å›é€€åˆ°åŸæœ‰æ¨¡å‹...")
        
        if args.model == 'cnn':
            if args.dataset == 'mnist':
                global_model = CNNMnist(args=args)
            elif args.dataset == 'fmnist':
                global_model = CNNFashion_Mnist(args=args)
            elif args.dataset == 'cifar':
                global_model = CNNCifar(args=args)
            elif args.dataset == 'cifar100':
                global_model = CNNCifar100(args=args)
        elif args.model == 'resnet':
            if args.dataset == 'cifar':
                global_model = ResNet18Fed(num_classes=args.num_classes)
            elif args.dataset == 'cifar100':
                global_model = ResNet18Fed(num_classes=100)
            else:
                print(f"ResNet not implemented for dataset {args.dataset}, using CNN instead")
                if args.dataset == 'mnist':
                    global_model = CNNMnist(args=args)
                elif args.dataset == 'fmnist':
                    global_model = CNNFashion_Mnist(args=args)
        elif args.model == 'mlp':
            img_size = train_dataset[0][0].shape
            len_in = 1
            for x in img_size:
                len_in *= x
            global_model = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes)
        else:
            exit('Error: unrecognized model')
    
    return global_model

def calculate_and_print_client_communication_stats(
    idx, final_residual, global_model, args, 
    total_original_params=None, total_nonzero_values=None
):
    """
    è®¡ç®—å¹¶æ‰“å°æ¯ä¸ªå®¢æˆ·ç«¯çš„é€šä¿¡ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒæ—¶æ”¯æŒå…¨é‡å’Œå‹ç¼©æ¨¡å¼ã€‚

    Args:
        idx (int): å®¢æˆ·ç«¯ç´¢å¼•ã€‚
        final_residual (dict): å®¢æˆ·ç«¯ä¸Šä¼ çš„æœ€ç»ˆæ®‹å·®ã€‚
        global_model (torch.nn.Module): å…¨å±€æ¨¡å‹ï¼Œç”¨äºè®¡ç®—æœªå‹ç¼©æ—¶çš„åŸºå‡†å­—èŠ‚æ•°ã€‚
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡ï¼ŒåŒ…å«å‹ç¼©ç›¸å…³è®¾ç½®ã€‚
        total_original_params (int, optional): å…¨å±€ç»Ÿè®¡ä¸­å‹ç¼©å‰çš„å‚æ•°é‡ç´¯åŠ å™¨ã€‚
        total_nonzero_values (int, optional): å…¨å±€ç»Ÿè®¡ä¸­å®é™…ä¼ è¾“çš„å­—èŠ‚æ•°ç´¯åŠ å™¨ã€‚
    
    Returns:
        int: å®¢æˆ·ç«¯å®é™…ä¼ è¾“çš„å­—èŠ‚æ•°
    """
    # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯çš„é€šä¿¡é‡
    if args.uplink_compression == 'uniform':
        # å‹ç¼©æ¨¡å¼ï¼šè°ƒç”¨æ‰“åŒ…æ ¼å¼çš„é€šä¿¡æˆæœ¬è®¡ç®—
        comm_cost, client_transmitted_bytes, layer_details = calculate_communication_cost_dict(final_residual)
    else:
        # å…¨é‡æ¨¡å¼ï¼šç›´æ¥è®¡ç®—å¯†é›†å¼ é‡çš„å­—èŠ‚æ•°
        client_transmitted_bytes = sum(torch.numel(tensor) * 4 for tensor in final_residual.values())  # float32 = 4å­—èŠ‚
        layer_details = []  # å…¨é‡æ¨¡å¼ä¸éœ€è¦å±‚è¯¦ç»†ä¿¡æ¯
        comm_cost = client_transmitted_bytes / 4.0  # ä¸ºäº†å…¼å®¹æ€§ä¿æŒè¿™ä¸ªå˜é‡

    # æ‰“å°æ¯ä¸ªå®¢æˆ·ç«¯çš„é€šä¿¡ç»Ÿè®¡ä¿¡æ¯
    print(f"[CLIENT {idx}] é€šä¿¡é‡ç»Ÿè®¡:")
    print(f"   â€¢ ä¼ é€’å±‚æ•°: {len(final_residual)}")
    print(f"   â€¢ å®é™…ä¼ è¾“å­—èŠ‚æ•°: {client_transmitted_bytes:,} B")  # ç›´æ¥æ‰“å°å­—èŠ‚æ•°

    # æ‰“å°æ¯å±‚çš„è¯¦ç»†ä¿¡æ¯
    for detail in layer_details:
        total_params = detail['total_params']

        if 'mask_bytes' in detail and 'values_bytes' in detail:
            # æ˜¾ç¤ºæ©ç å’Œå€¼çš„å­—èŠ‚åˆ†è§£
            mask_bytes = detail['mask_bytes']

            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ä½çº§ä¼˜åŒ–
            if detail.get('bit_packed', False):
                # ä½çº§ä¼˜åŒ–æ ¼å¼
                # è®¡ç®—å¦‚æœä¸ä½¿ç”¨ä½çº§ä¼˜åŒ–éœ€è¦å¤šå°‘å­—èŠ‚
                traditional_mask_bytes = total_params * 1  # æ¯ä¸ªå¸ƒå°”å€¼1å­—èŠ‚
                mask_savings = traditional_mask_bytes - mask_bytes

    # æ˜¾ç¤ºå‹ç¼©ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…åœ¨å¯ç”¨ä¸Šè¡Œå‹ç¼©æ—¶ï¼‰
    if args.uplink_compression == 'uniform':
        client_total_params = sum(torch.numel(param) for _, param in global_model.named_parameters())
        baseline_bytes = client_total_params * 4  # æœªå‹ç¼©æ—¶çš„åŸºå‡†å­—èŠ‚æ•°

        compression_effectiveness = (1 - client_transmitted_bytes / baseline_bytes) * 100 if baseline_bytes > 0 else 0

        print(f"   â€¢ æ¨¡å‹æ€»å‚æ•°é‡: {client_total_params:,}")
        print(f"   â€¢ æœªå‹ç¼©æ—¶åŸºå‡†å­—èŠ‚æ•°: {baseline_bytes:,} B")
        print(f"   â€¢ å‹ç¼©æ•ˆæœ: å‡å°‘ {compression_effectiveness:.1f}% é€šä¿¡é‡")

        # ç´¯åŠ åˆ°å…¨å±€ç»Ÿè®¡
        if total_original_params is not None:
            total_original_params += client_total_params  # å‹ç¼©å‰å‚æ•°é‡
        if total_nonzero_values is not None:
            total_nonzero_values += client_transmitted_bytes  # å®é™…ä¼ è¾“çš„å­—èŠ‚æ•°
    else:
        # å¦‚æœæœªå¯ç”¨å‹ç¼©ï¼Œæ˜¾ç¤ºå¯†é›†ä¼ è¾“ä¿¡æ¯
        print(f"   â€¢ å¯†é›†ä¼ è¾“å­—èŠ‚æ•°: {client_transmitted_bytes:,} B")

    # è¿”å›å®¢æˆ·ç«¯å®é™…ä¼ è¾“çš„å­—èŠ‚æ•°ï¼Œç”±è°ƒç”¨è€…ç´¯åŠ åˆ°æœ¬è½®æ€»é€šä¿¡é‡
    return client_transmitted_bytes

def evaluate_epoch_performance(args, global_model, test_dataset, local_losses, 
                               global_test_accuracy, train_loss, ema_acc, ema_alpha,
                               best_ema_acc, patience_counter, improve_streak, patience,
                               best_global_weights):
    """
    è¯„ä¼°å½“å‰è½®æ¬¡çš„æ€§èƒ½å¹¶æ›´æ–°ç›¸å…³ç»Ÿè®¡ä¿¡æ¯ã€‚
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        global_model: å…¨å±€æ¨¡å‹
        test_dataset: æµ‹è¯•æ•°æ®é›†
        local_losses: æœ¬è½®å®¢æˆ·ç«¯è®­ç»ƒæŸå¤±åˆ—è¡¨
        global_test_accuracy: å…¨å±€æµ‹è¯•å‡†ç¡®ç‡å†å²åˆ—è¡¨
        train_loss: è®­ç»ƒæŸå¤±å†å²åˆ—è¡¨
        ema_acc: å½“å‰EMAå¹³æ»‘å‡†ç¡®ç‡
        ema_alpha: EMAå¹³æ»‘å› å­
        best_ema_acc: æœ€ä½³EMAå‡†ç¡®ç‡
        patience_counter: è€å¿ƒè®¡æ•°å™¨
        improve_streak: è¿ç»­æå‡è®¡æ•°å™¨
        patience: æ—©åœè€å¿ƒå€¼
        best_global_weights: æœ€ä½³æ¨¡å‹æƒé‡
    
    Returns:
        tuple: (current_test_acc, test_loss, loss_avg, ema_acc, best_ema_acc, 
                patience_counter, improve_streak, best_global_weights)
    """
    # 1. è¯„ä¼°å½“å‰è½®æ¬¡æ€§èƒ½
    current_test_acc, test_loss = test_inference(args, global_model, test_dataset)
    global_test_accuracy.append(current_test_acc)
    loss_avg = sum(local_losses) / len(local_losses)
    train_loss.append(loss_avg)

    # 2. æ›´æ–°EMAå¹³æ»‘å‡†ç¡®ç‡
    if ema_acc is None:
        ema_acc = current_test_acc
    else:
        ema_acc = ema_alpha * current_test_acc + (1 - ema_alpha) * ema_acc

    # 3. åˆ¤æ–­æ€§èƒ½æ˜¯å¦æå‡ (åŸºäºå¹³æ»‘åçš„å‡†ç¡®ç‡)
    eps = 1e-4  # å°é˜ˆå€¼é˜²æ­¢æŠ–åŠ¨
    if ema_acc > best_ema_acc + eps:
        best_ema_acc = ema_acc
        patience_counter = 0
        improve_streak += 1
        print(f'âœ… æ–°çš„æœ€ä½³å¹³æ»‘å‡†ç¡®ç‡: {100*best_ema_acc:.2f}% (è¿ç»­æå‡ {improve_streak} è½®)')
        best_global_weights = copy.deepcopy(global_model.state_dict())
    else:
        patience_counter += 1
        improve_streak = 0
        print(f'âš ï¸ å¹³æ»‘å‡†ç¡®ç‡æœªæ”¹å–„. è€å¿ƒå€¼: {patience_counter}/{patience}')
    
    return (current_test_acc, test_loss, loss_avg, ema_acc, best_ema_acc, 
            patience_counter, improve_streak, best_global_weights)

def print_epoch_summary(epoch, args, current_test_acc, best_ema_acc, ema_acc, 
                       loss_avg, test_loss, epoch_duration, estimated_remaining_time):
    """
    æ‰“å°è½®æ¬¡æ€»ç»“ä¿¡æ¯ã€‚
    
    Args:
        epoch: å½“å‰è½®æ¬¡
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        current_test_acc: å½“å‰æµ‹è¯•å‡†ç¡®ç‡
        best_ema_acc: æœ€ä½³EMAå‡†ç¡®ç‡
        ema_acc: å½“å‰EMAå‡†ç¡®ç‡
        loss_avg: å¹³å‡è®­ç»ƒæŸå¤±
        test_loss: æµ‹è¯•æŸå¤±
        epoch_duration: è½®æ¬¡è€—æ—¶
        estimated_remaining_time: é¢„è®¡å‰©ä½™æ—¶é—´
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š è½®æ¬¡ {epoch+1} æ€»ç»“:")
    print(f"   ğŸ’¯ å…¨å±€æµ‹è¯•å‡†ç¡®ç‡: {current_test_acc*100:.2f}% (æœ€ä½³: {best_ema_acc*100:.2f}%)")
    print(f"   ğŸ“ˆ å¹³æ»‘å‡†ç¡®ç‡ (ç”¨äºå†³ç­–): {ema_acc*100:.2f}%")
    uplink_ratio = args.uplink_compression_ratio if args.uplink_compression == 'uniform' else 'N/A'
    downlink_ratio = args.downlink_compression_ratio if args.downlink_compression == 'uniform' else 'N/A'
    print(f"   ğŸ§¬ å‹ç¼©ç‡ - ä¸Šè¡Œ: {uplink_ratio}, ä¸‹è¡Œ: {downlink_ratio}")
    print(f"   ğŸ“‰ å¹³å‡è®­ç»ƒæŸå¤±: {loss_avg:.6f}")
    print(f"   ğŸ“‰ æµ‹è¯•æŸå¤±: {test_loss:.6f}")
    print(f"   â±ï¸ è½®æ¬¡è€—æ—¶: {timedelta(seconds=int(epoch_duration))}")
    print(f"   â³ é¢„è®¡å‰©ä½™: {timedelta(seconds=int(estimated_remaining_time))}")

def print_communication_summary(epoch, args, idxs_users, global_model, epoch_comm_cost,
                               residual_to_broadcast_packed, communication_cost):
    """
    æ‰“å°è½®æ¬¡é€šä¿¡é‡æ±‡æ€»ä¿¡æ¯ã€‚
    
    Args:
        epoch: å½“å‰è½®æ¬¡
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        idxs_users: é€‰ä¸­çš„å®¢æˆ·ç«¯åˆ—è¡¨
        global_model: å…¨å±€æ¨¡å‹
        epoch_comm_cost: æœ¬è½®ä¸Šè¡Œé€šä¿¡å¼€é”€
        residual_to_broadcast_packed: ä¸‹è¡Œå¹¿æ’­æ®‹å·®
        communication_cost: é€šä¿¡å¼€é”€å†å²åˆ—è¡¨
    
    Returns:
        int: æœ¬è½®åŒå‘æ€»é€šä¿¡é‡
    """
    print(f"\nğŸ“¡ è½®æ¬¡ {epoch+1} é€šä¿¡é‡æ±‡æ€»:")

    # --- ä¸Šè¡Œé€šä¿¡ç»Ÿè®¡ ---
    num_selected_clients = len(idxs_users)
    single_model_bytes = sum(p.numel() for p in global_model.parameters()) * 4

    print(f"   â¬†ï¸ ä¸Šè¡Œé€šä¿¡ (æ¥è‡ª {num_selected_clients} ä¸ªå®¢æˆ·ç«¯):")
    # epoch_comm_cost å·²ç»æ˜¯æ‰€æœ‰é€‰ä¸­å®¢æˆ·ç«¯ä¸Šä¼ å­—èŠ‚æ•°çš„æ€»å’Œ
    theoretical_uplink_baseline = single_model_bytes * num_selected_clients
    uplink_effectiveness = (1 - epoch_comm_cost / theoretical_uplink_baseline) * 100 if theoretical_uplink_baseline > 0 else 0
    print(f"      â€¢ å®é™…ä¼ è¾“æ€»å­—èŠ‚: {int(epoch_comm_cost):,} B")
    print(f"      â€¢ ç†è®ºåŸºå‡† (æœªå‹ç¼©): {theoretical_uplink_baseline:,} B")
    print(f"      â€¢ æœ¬è½®ä¸Šè¡Œå‹ç¼©æ•ˆæœ: å‡å°‘ {uplink_effectiveness:.1f}%")

    # --- ä¸‹è¡Œé€šä¿¡ç»Ÿè®¡ ---
    print(f"   â¬‡ï¸ ä¸‹è¡Œé€šä¿¡ (æœåŠ¡å™¨å¹¿æ’­ç»™æ‰€æœ‰ {args.num_users} ä¸ªå®¢æˆ·ç«¯):")
    downlink_bytes = 0  # åˆå§‹åŒ–ä¸º0
    if residual_to_broadcast_packed is not None:
        if args.downlink_compression == 'uniform':
            # å‹ç¼©æ¨¡å¼ï¼šè°ƒç”¨å·¥å…·å‡½æ•°è®¡ç®—ä¸‹è¡Œå¹¿æ’­çš„å­—èŠ‚æ•°
            _, single_broadcast_bytes, _ = calculate_communication_cost_dict(residual_to_broadcast_packed)
        else:
            # å…¨é‡æ¨¡å¼ï¼šç›´æ¥è®¡ç®—å¯†é›†å¼ é‡çš„å­—èŠ‚æ•°
            single_broadcast_bytes = sum(torch.numel(tensor) * 4 for tensor in residual_to_broadcast_packed.values())
        
        # ä¸‹è¡Œæ€»é‡ = (æ‰€æœ‰å®¢æˆ·ç«¯æ•°é‡) Ã— (æœåŠ¡å™¨å¹¿æ’­å­—å…¸å¤§å°)
        downlink_bytes = single_broadcast_bytes * args.num_users

        # ä¸‹è¡ŒåŸºå‡†æ˜¯æ‰€æœ‰å®¢æˆ·ç«¯éƒ½æ”¶åˆ°æœªå‹ç¼©æ¨¡å‹çš„å­—èŠ‚æ•°
        theoretical_downlink_baseline = single_model_bytes * args.num_users
        downlink_effectiveness = (1 - downlink_bytes / theoretical_downlink_baseline) * 100 if theoretical_downlink_baseline > 0 else 0

        print(f"      â€¢ å®é™…ä¼ è¾“å­—èŠ‚: {int(downlink_bytes):,} B ({int(single_broadcast_bytes):,} B Ã— {args.num_users} å®¢æˆ·ç«¯)")
        print(f"      â€¢ ç†è®ºåŸºå‡† (æœªå‹ç¼©): {theoretical_downlink_baseline:,} B")
        print(f"      â€¢ æœ¬è½®ä¸‹è¡Œå‹ç¼©æ•ˆæœ: å‡å°‘ {downlink_effectiveness:.1f}%")
    else:
        print("      â€¢ æ— ä¸‹è¡Œä¼ è¾“ (åˆå§‹è½®)")

    # --- åŒå‘æ€»é€šä¿¡é‡ ---
    total_round_comm_cost = epoch_comm_cost + downlink_bytes
    communication_cost.append(total_round_comm_cost)
    print(f"   ğŸ”„ æœ¬è½®åŒå‘æ€»é€šä¿¡é‡: {int(total_round_comm_cost):,} B")
    print(f"{'='*60}")
    
    return total_round_comm_cost

def calculate_current_learning_rate(args, epoch):
    """
    è®¡ç®—å½“å‰å­¦ä¹ ç‡ã€‚
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡
        epoch: å½“å‰è½®æ¬¡
    
    Returns:
        float: å½“å‰å­¦ä¹ ç‡
    """
    current_lr = args.lr
    if args.lr_scheduler == 'cosine':
        total_rounds = args.epochs
        min_lr = args.lr * 0.05
        warmup_rounds = min(5, total_rounds // 10)
        if epoch < warmup_rounds:
            current_lr = args.lr * (epoch + 1) / warmup_rounds
        else:
            effective_round = epoch - warmup_rounds
            effective_total = total_rounds - warmup_rounds
            cosine_factor = 0.5 * (1 + math.cos(math.pi * effective_round / effective_total))
            current_lr = min_lr + (args.lr - min_lr) * cosine_factor
    
    return current_lr

if __name__ == '__main__':
    main()