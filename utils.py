#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6

import copy # ç”¨äºæ·±æ‹·è´å¯¹è±¡ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹æƒé‡
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torchvision import datasets, transforms
# ä» sampling.py å¯¼å…¥æ•°æ®åˆ’åˆ†å‡½æ•°
from sampling import mnist_iid, mnist_noniid, mnist_noniid_unequal
from sampling import cifar_iid, cifar_noniid


class CutMix:
    """
    CutMixæ•°æ®å¢å¼º
    ==============
    
    CutMixæ˜¯ä¸€ç§å¼ºå¤§çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡å‰ªåˆ‡å’Œç²˜è´´å›¾åƒåŒºåŸŸæ¥ç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬ã€‚
    ç›¸æ¯”äºMixupï¼ŒCutMixä¿æŒäº†å›¾åƒçš„ç©ºé—´ç»“æ„ï¼Œåœ¨CIFAR-100ç­‰ç²¾ç»†åˆ†ç±»ä»»åŠ¡ä¸Šæ•ˆæœæ›´å¥½ã€‚
    
    åŸç†ï¼š
    - ä»ä¸€å¼ å›¾åƒä¸­å‰ªåˆ‡ä¸€ä¸ªçŸ©å½¢åŒºåŸŸ
    - ç”¨å¦ä¸€å¼ å›¾åƒçš„å¯¹åº”åŒºåŸŸå¡«å……
    - æ ‡ç­¾æŒ‰ç…§åŒºåŸŸæ¯”ä¾‹æ··åˆ
    """
    
    def __init__(self, alpha=1.0, prob=0.5):
        self.alpha = alpha  # Betaåˆ†å¸ƒå‚æ•°ï¼Œæ§åˆ¶æ··åˆæ¯”ä¾‹
        self.prob = prob    # åº”ç”¨CutMixçš„æ¦‚ç‡
    
    def __call__(self, batch):
        if np.random.rand() > self.prob:
            return batch
        
        images, labels = batch
        batch_size = images.size(0)
        
        # ç”Ÿæˆæ··åˆæ¯”ä¾‹
        lam = np.random.beta(self.alpha, self.alpha)
        
        # éšæœºé€‰æ‹©æ··åˆçš„æ ·æœ¬å¯¹
        index = torch.randperm(batch_size)
        
        # è®¡ç®—å‰ªåˆ‡åŒºåŸŸ
        W, H = images.size(3), images.size(2)
        cut_rat = np.sqrt(1. - lam)
        cut_w = int(W * cut_rat)
        cut_h = int(H * cut_rat)
        
        # éšæœºé€‰æ‹©å‰ªåˆ‡ä½ç½®
        cx = np.random.randint(W)
        cy = np.random.randint(H)
        
        bbx1 = np.clip(cx - cut_w // 2, 0, W)
        bby1 = np.clip(cy - cut_h // 2, 0, H)
        bbx2 = np.clip(cx + cut_w // 2, 0, W)
        bby2 = np.clip(cy + cut_h // 2, 0, H)
        
        # æ‰§è¡Œå‰ªåˆ‡å’Œç²˜è´´
        images[:, :, bby1:bby2, bbx1:bbx2] = images[index, :, bby1:bby2, bbx1:bbx2]
        
        # è°ƒæ•´æ··åˆæ¯”ä¾‹
        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
        
        return images, (labels, labels[index], lam)


class LabelSmoothingCrossEntropy(nn.Module):
    """
    æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±
    ==================
    
    æ ‡ç­¾å¹³æ»‘æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡"è½¯åŒ–"ç¡¬æ ‡ç­¾æ¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚
    ç‰¹åˆ«é€‚ç”¨äºç±»åˆ«æ•°è¾ƒå¤šçš„æ•°æ®é›†ï¼ˆå¦‚CIFAR-100ï¼‰ã€‚
    
    åŸç†ï¼š
    - å°†ç¡¬æ ‡ç­¾ [0, 0, 1, 0, ...] è½¬æ¢ä¸ºè½¯æ ‡ç­¾ [Îµ/K, Îµ/K, 1-Îµ+Îµ/K, Îµ/K, ...]
    - Îµæ˜¯å¹³æ»‘å‚æ•°ï¼ŒKæ˜¯ç±»åˆ«æ•°
    - é¼“åŠ±æ¨¡å‹ä¸è¦è¿‡äºè‡ªä¿¡äºå•ä¸€ç±»åˆ«
    """
    
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing
    
    def forward(self, pred, target):
        """
        pred: [N, C] é¢„æµ‹logits
        target: [N] çœŸå®æ ‡ç­¾
        """
        N, C = pred.size()
        
        # è½¬æ¢ä¸ºone-hotç¼–ç 
        target_one_hot = torch.zeros_like(pred).scatter_(1, target.unsqueeze(1), 1)
        
        # åº”ç”¨æ ‡ç­¾å¹³æ»‘
        target_smooth = target_one_hot * (1 - self.smoothing) + self.smoothing / C
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        log_pred = F.log_softmax(pred, dim=1)
        loss = -(target_smooth * log_pred).sum(dim=1).mean()
        
        return loss


def mixup_data(x, y, alpha=0.4):
    """
    Mixupæ•°æ®å¢å¼º
    =============
    
    é€šè¿‡çº¿æ€§æ’å€¼æ··åˆä¸¤ä¸ªæ ·æœ¬å’Œå¯¹åº”æ ‡ç­¾ã€‚
    """
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size)
    
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """MixupæŸå¤±è®¡ç®—"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


def check_dataset_exists(dataset_name, data_dir):
    """æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²ç»ä¸‹è½½"""
    dataset_name = dataset_name.lower()
    
    if dataset_name == 'mnist':
        check_files = ['MNIST/raw/train-images-idx3-ubyte', 'MNIST/raw/t10k-images-idx3-ubyte']
    elif dataset_name == 'cifar10':
        check_files = ['cifar-10-batches-py/data_batch_1', 'cifar-10-batches-py/test_batch']
    elif dataset_name == 'cifar100':
        check_files = ['cifar-100-python/train', 'cifar-100-python/test']
    elif dataset_name == 'fmnist':
        check_files = ['FashionMNIST/raw/train-images-idx3-ubyte', 'FashionMNIST/raw/t10k-images-idx3-ubyte']
    else:
        return False
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for file_path in check_files:
        if not os.path.exists(os.path.join(data_dir, file_path)):
            return False
    return True


def get_dataset(args): # æ ¹æ®å‚æ•°åŠ è½½å¹¶åˆ’åˆ†æ•°æ®é›†
    """ Returns train and test datasets and a user group which is a dict where
    the keys are the user index and the values are the corresponding data for
    each of those users.
    è¿”å›:
        train_dataset: åŸå§‹çš„å®Œæ•´è®­ç»ƒæ•°æ®é›†
        test_dataset: åŸå§‹çš„å®Œæ•´æµ‹è¯•æ•°æ®é›†
        user_groups: ä¸€ä¸ªå­—å…¸,é”®æ˜¯ç”¨æˆ·ID,å€¼æ˜¯åˆ†é…ç»™è¯¥ç”¨æˆ·çš„æ•°æ®ç´¢å¼•
    """

    if args.dataset == 'cifar': # å¦‚æœæ˜¯ CIFAR-10 æ•°æ®é›†
        data_dir = '../data/cifar/' # æ•°æ®å­˜å‚¨ç›®å½•
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        dataset_exists = check_dataset_exists('cifar10', data_dir)
        if dataset_exists:
            print("[INFO] æ‰¾åˆ°å·²å­˜åœ¨çš„ CIFAR-10 æ•°æ®é›†")
        else:
            print("[INFO] CIFAR-10 æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨ä¸‹è½½")
        
        # å®šä¹‰ CIFAR-10 çš„å¼ºåŒ–æ•°æ®å¢å¼ºç­–ç•¥ï¼ˆè®­ç»ƒæ—¶ï¼‰
        train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),  # éšæœºè£å‰ªï¼Œè¾¹ç¼˜å¡«å……4åƒç´ 
            transforms.RandomHorizontalFlip(p=0.5),  # 50%æ¦‚ç‡æ°´å¹³ç¿»è½¬
            transforms.ToTensor(),
            transforms.RandomErasing(p=0.2, scale=(0.02, 0.33), ratio=(0.3, 3.3)),  # éšæœºæ“¦é™¤
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]) # CIFAR-10æ ‡å‡†åŒ–å‚æ•°

        # æµ‹è¯•æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
        test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])

        # åŠ è½½ CIFAR-10 è®­ç»ƒé›†ï¼ˆä½¿ç”¨å¢å¼ºå˜æ¢ï¼‰
        train_dataset = datasets.CIFAR10(data_dir, train=True, download=not dataset_exists,
                                       transform=train_transform)
        # åŠ è½½ CIFAR-10 æµ‹è¯•é›†ï¼ˆä¸ä½¿ç”¨å¢å¼ºå˜æ¢ï¼‰
        test_dataset = datasets.CIFAR10(data_dir, train=False, download=not dataset_exists,
                                      transform=test_transform)

        # sample training data amongst users (åœ¨ç”¨æˆ·é—´åˆ’åˆ†è®­ç»ƒæ•°æ®)
        if args.iid: # å¦‚æœæ˜¯ IID è®¾ç½®
            # Sample IID user data from Cifar
            user_groups = cifar_iid(train_dataset, args.num_users)
        else: # å¦‚æœæ˜¯ Non-IID è®¾ç½®
            # Sample Non-IID user data from Cifar
            if args.unequal: # å¦‚æœæ•°æ®åˆ’åˆ†ä¸å‡è¡¡
                # Chose uneuqal splits for every user
                raise NotImplementedError() # æ­¤å¤„ä»£ç è¡¨ç¤ºä¸å‡è¡¡çš„ CIFAR Non-IID åˆ’åˆ†æœªå®ç°
            else: # å¦‚æœæ•°æ®åˆ’åˆ†å‡è¡¡
                # æ ¹æ®æ˜¯å¦æä¾› alpha å‚æ•°é€‰æ‹©åˆ’åˆ†æ–¹æ³•
                if hasattr(args, 'alpha') and args.alpha is not None:
                    # ä½¿ç”¨ Dirichlet åˆ†å¸ƒè¿›è¡Œ Non-IID åˆ’åˆ†
                    from sampling import cifar_noniid_dirichlet
                    user_groups = cifar_noniid_dirichlet(train_dataset, args.num_users, args.alpha)
                    print(f"[INFO] ä½¿ç”¨ Dirichlet åˆ†å¸ƒåˆ’åˆ† (alpha={args.alpha})")
                else:
                    # ä½¿ç”¨ä¼ ç»Ÿçš„åˆ†ç‰‡æ–¹æ³•è¿›è¡Œ Non-IID åˆ’åˆ†
                    user_groups = cifar_noniid(train_dataset, args.num_users)
                    print("[INFO] ä½¿ç”¨ä¼ ç»Ÿåˆ†ç‰‡æ–¹æ³•åˆ’åˆ† (æ¯å®¢æˆ·ç«¯2ä¸ªç±»åˆ«)")
                
    elif args.dataset == 'cifar100': # å¦‚æœæ˜¯ CIFAR-100 æ•°æ®é›†
        data_dir = '../data/cifar100/' # æ•°æ®å­˜å‚¨ç›®å½•
        
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        dataset_exists = check_dataset_exists('cifar100', data_dir)
        if dataset_exists:
            print("[INFO] æ‰¾åˆ°å·²å­˜åœ¨çš„ CIFAR-100 æ•°æ®é›†")
        else:
            print("[INFO] CIFAR-100 æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨ä¸‹è½½")
        
        # å®šä¹‰ CIFAR-100 çš„å¼ºåŒ–æ•°æ®å¢å¼ºæ“ä½œ
        apply_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),  # ä½¿ç”¨AutoAugment
            transforms.ToTensor(),
            transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0),  # éšæœºæ“¦é™¤
            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))]) # CIFAR-100æ ‡å‡†åŒ–å‚æ•°
        
        # æµ‹è¯•æ—¶ä¸ç”¨æ•°æ®å¢å¼º
        test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])

        # åŠ è½½ CIFAR-100 è®­ç»ƒé›†
        train_dataset = datasets.CIFAR100(data_dir, train=True, download=not dataset_exists,
                                        transform=apply_transform)
        # åŠ è½½ CIFAR-100 æµ‹è¯•é›†
        test_dataset = datasets.CIFAR100(data_dir, train=False, download=not dataset_exists,
                                       transform=test_transform)

        # sample training data amongst users (åœ¨ç”¨æˆ·é—´åˆ’åˆ†è®­ç»ƒæ•°æ®)
        if args.iid: # å¦‚æœæ˜¯ IID è®¾ç½®
            # Sample IID user data from Cifar100
            user_groups = cifar_iid(train_dataset, args.num_users)
        else: # å¦‚æœæ˜¯ Non-IID è®¾ç½®
            # Sample Non-IID user data from Cifar100
            if args.unequal: # å¦‚æœæ•°æ®åˆ’åˆ†ä¸å‡è¡¡
                # Chose uneuqal splits for every user
                raise NotImplementedError() # æ­¤å¤„ä»£ç è¡¨ç¤ºä¸å‡è¡¡çš„ CIFAR-100 Non-IID åˆ’åˆ†æœªå®ç°
            else: # å¦‚æœæ•°æ®åˆ’åˆ†å‡è¡¡
                # æ ¹æ®æ˜¯å¦æä¾› alpha å‚æ•°é€‰æ‹©åˆ’åˆ†æ–¹æ³•
                if hasattr(args, 'alpha') and args.alpha is not None:
                    # ä½¿ç”¨ Dirichlet åˆ†å¸ƒè¿›è¡Œ Non-IID åˆ’åˆ† (CIFAR-100 æœ‰100ä¸ªç±»åˆ«)
                    from sampling import cifar_noniid_dirichlet
                    # ä¸º CIFAR-100 åˆ›å»ºä¸“é—¨çš„å‡½æ•°æˆ–ä¿®æ”¹ç°æœ‰å‡½æ•°
                    user_groups = cifar_noniid_dirichlet(train_dataset, args.num_users, args.alpha)
                    print(f"[INFO] ä½¿ç”¨ Dirichlet åˆ†å¸ƒåˆ’åˆ† CIFAR-100 (alpha={args.alpha})")
                else:
                    # ä½¿ç”¨ä¼ ç»Ÿçš„åˆ†ç‰‡æ–¹æ³•è¿›è¡Œ Non-IID åˆ’åˆ†
                    user_groups = cifar_noniid(train_dataset, args.num_users)
                    print("[INFO] ä½¿ç”¨ä¼ ç»Ÿåˆ†ç‰‡æ–¹æ³•åˆ’åˆ† CIFAR-100")

    elif args.dataset == 'mnist' or args.dataset == 'fmnist': # å¦‚æœæ˜¯ MNIST æˆ– Fashion-MNIST æ•°æ®é›†
        if args.dataset == 'mnist':
            data_dir = '../data/mnist/'
            dataset_type = 'mnist'
        else: # args.dataset == 'fmnist'
            data_dir = '../data/fmnist/'
            dataset_type = 'fmnist'

        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
        dataset_exists = check_dataset_exists(dataset_type, data_dir)
        if dataset_exists:
            print(f"[INFO] æ‰¾åˆ°å·²å­˜åœ¨çš„ {dataset_type.upper()} æ•°æ®é›†")
        else:
            print(f"[INFO] {dataset_type.upper()} æ•°æ®é›†ä¸å­˜åœ¨ï¼Œå°†è‡ªåŠ¨ä¸‹è½½")

        # å®šä¹‰ MNIST/Fashion-MNIST çš„å›¾åƒé¢„å¤„ç†æ“ä½œ
        # ä¸ºNon-IIDåœºæ™¯æä¾›æ›´å¼ºçš„æ•°æ®å¢å¼º
        if hasattr(args, 'iid') and args.iid == 0 and getattr(args, 'enable_enhanced_augmentation', 1) == 1:  # Non-IIDåœºæ™¯ä¸”å¯ç”¨å¢å¼º
            apply_transform = transforms.Compose([
                transforms.ToPILImage() if not isinstance(datasets.MNIST(data_dir, train=True, download=False, transform=transforms.ToTensor())[0][0], torch.Tensor) else transforms.Lambda(lambda x: x),
                transforms.RandomRotation(degrees=10),  # éšæœºæ—‹è½¬ Â±10åº¦
                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # éšæœºå¹³ç§»
                transforms.ToTensor(),
                transforms.Normalize((0.1307,), (0.3081,)),  # MNIST çš„å‡å€¼å’Œæ ‡å‡†å·®
                transforms.RandomErasing(p=0.1, scale=(0.02, 0.15))  # éšæœºæ“¦é™¤
            ])
            print(f"ğŸ¨ Non-IID {args.dataset.upper()}: å¯ç”¨å¢å¼ºæ•°æ®å˜æ¢")
        else:  # IIDåœºæ™¯æˆ–å…³é—­å¢å¼ºæ—¶ä½¿ç”¨æ ‡å‡†å˜æ¢
            apply_transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.1307,), (0.3081,))]) # MNIST çš„å‡å€¼å’Œæ ‡å‡†å·® (å•é€šé“)
            if hasattr(args, 'iid') and args.iid == 0:
                print(f"ğŸ¨ Non-IID {args.dataset.upper()}: ä½¿ç”¨æ ‡å‡†æ•°æ®å˜æ¢ï¼ˆå¢å¼ºå·²å…³é—­ï¼‰")

        # ä¿®å¤ï¼šæ ¹æ®æ•°æ®é›†ç±»å‹åŠ è½½å¯¹åº”çš„æ•°æ®
        if args.dataset == 'mnist':
            train_dataset = datasets.MNIST(data_dir, train=True, download=not dataset_exists,
                                           transform=apply_transform)
            test_dataset = datasets.MNIST(data_dir, train=False, download=not dataset_exists,
                                          transform=apply_transform)
        else: # Fashion-MNIST
            train_dataset = datasets.FashionMNIST(data_dir, train=True, download=not dataset_exists,
                                                transform=apply_transform)
            test_dataset = datasets.FashionMNIST(data_dir, train=False, download=not dataset_exists,
                                               transform=apply_transform)

        # sample training data amongst users
        if args.iid:
            # Sample IID user data from Mnist
            user_groups = mnist_iid(train_dataset, args.num_users)
        else:
            # Sample Non-IID user data from Mnist
            if args.unequal:
                # Chose uneuqal splits for every user
                user_groups = mnist_noniid_unequal(train_dataset, args.num_users)
            else:
                # Chose euqal splits for every user
                user_groups = mnist_noniid(train_dataset, args.num_users)
    else: # å¦‚æœæ•°æ®é›†åç§°æ— æ³•è¯†åˆ«
        exit(f"Error: unrecognized dataset {args.dataset}")


    return train_dataset, test_dataset, user_groups


def average_weights(w,lens): # è®¡ç®—æ¨¡å‹æƒé‡çš„å¹³å‡å€¼ (FedAvg ç®—æ³•çš„æ ¸å¿ƒ)
    """
    Returns the average of the weights.
    :param w: ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå®¢æˆ·ç«¯çš„æ¨¡å‹æƒé‡ (state_dict)
    :return: å¹³å‡åçš„æ¨¡å‹æƒé‡ (state_dict)
    """
    total = sum(lens)
    w_avg = copy.deepcopy(w[0])
    for key in w_avg.keys():
        w_avg[key] = w[0][key] * (lens[0] / total)
        for i in range(1, len(w)):
            w_avg[key] += w[i][key] * (lens[i] / total)
    return w_avg


import torch
import copy
import math # ç¡®ä¿å¯¼å…¥ math æ¨¡å—

def calculate_diversity_scores(local_weights, client_data_sizes):
    """
    è®¡ç®—å®¢æˆ·ç«¯æ¨¡å‹çš„å¤šæ ·æ€§åˆ†æ•° - [RFL Aligned Version]
    åŸºäºæ¨¡å‹æƒé‡å·®å¼‚å’Œæ•°æ®åˆ†å¸ƒä¸å¹³è¡¡ç¨‹åº¦
    """
    num_clients = len(local_weights)
    diversity_scores = []

    # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯ç›¸å¯¹äºå…¶ä»–å®¢æˆ·ç«¯çš„æ¨¡å‹æƒé‡å·®å¼‚
    for i in range(num_clients):
        total_distance = 0.0
        weight_count = 0

        for j in range(num_clients):
            if i != j:
                # è®¡ç®—ä¸¤ä¸ªæ¨¡å‹æƒé‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
                distance = 0.0
                for key in local_weights[i].keys():
                    if key in local_weights[j]:
                        w1 = local_weights[i][key].flatten().float()  # ç¡®ä¿æ˜¯æµ®ç‚¹å‹
                        w2 = local_weights[j][key].flatten().float()  # ç¡®ä¿æ˜¯æµ®ç‚¹å‹

                        # è®¡ç®—ä½™å¼¦è·ç¦» (1 - cosine_similarity)
                        norm1 = torch.norm(w1)
                        norm2 = torch.norm(w2)

                        if norm1 > 0 and norm2 > 0:
                            cosine_sim = torch.dot(w1, w2) / (norm1 * norm2)
                            cosine_distance = 1.0 - cosine_sim.item()
                            distance += cosine_distance
                            weight_count += 1

                if weight_count > 0:
                    total_distance += distance / weight_count

        # å½’ä¸€åŒ–è·ç¦»åˆ†æ•°
        avg_distance = total_distance / max(1, num_clients - 1)

        # ç»“åˆæ•°æ®é‡ä¸å¹³è¡¡å› å­
        total_samples = sum(client_data_sizes)
        data_imbalance = abs(client_data_sizes[i] / total_samples - 1.0 / num_clients)

        # ç»¼åˆå¤šæ ·æ€§åˆ†æ•° (æƒé‡å·®å¼‚ + æ•°æ®ä¸å¹³è¡¡)
        diversity_score = 0.7 * avg_distance + 0.3 * data_imbalance
        diversity_scores.append(diversity_score)

    return diversity_scores

def adaptive_federated_aggregation(local_weights, client_data_sizes, client_losses, 
                               diversity_scores=None, aggregation_method='weighted_avg'):
    """
    è‡ªé€‚åº”è”é‚¦èšåˆç­–ç•¥ - æ™®é€šè”é‚¦å­¦ä¹ ç‰ˆæœ¬ [RFL Aligned Version]
    æ ¹æ®æ•°æ®é‡ã€æŸå¤±å’Œå¤šæ ·æ€§åŠ¨æ€è°ƒæ•´èšåˆæƒé‡
    """
    if not local_weights:
        return {}

    num_clients = len(local_weights)

    # è®¡ç®—åŸºç¡€æƒé‡ï¼ˆæ•°æ®é‡ï¼‰
    total_samples = sum(client_data_sizes)
    data_weights = [size / total_samples for size in client_data_sizes]

    # å¤„ç†NaNæˆ–æ— ç©·æŸå¤±å€¼
    safe_losses = []
    for loss in client_losses:
        # Create a tensor to check for isnan or isinf
        loss_tensor = torch.tensor(loss)
        if torch.isnan(loss_tensor) or torch.isinf(loss_tensor):
            # If loss is invalid, append a default high loss value (e.g., 1.0)
            safe_losses.append(1.0)
        else:
            safe_losses.append(float(loss))

    if aggregation_method == 'weighted_avg':
        # æ ‡å‡†åŠ æƒå¹³å‡
        weights = data_weights
    elif aggregation_method == 'loss_aware':
        # åŸºäºæŸå¤±çš„æƒé‡è°ƒæ•´
        loss_weights = [1.0 / (1.0 + loss) for loss in safe_losses]
        total_loss_weight = sum(loss_weights)
        if total_loss_weight > 0:
            loss_weights = [w / total_loss_weight for w in loss_weights]
        else:
            loss_weights = [1.0 / num_clients] * num_clients

        # ç»“åˆæ•°æ®é‡å’ŒæŸå¤±æƒé‡
        weights = [0.6 * dw + 0.4 * lw for dw, lw in zip(data_weights, loss_weights)]
    elif aggregation_method == 'diversity_aware' and diversity_scores is not None:
        # åŸºäºå¤šæ ·æ€§çš„æƒé‡è°ƒæ•´ (RFL's robust logic)
        # 1. å…ˆè®¡ç®— loss_weights
        loss_weights = [1.0 / (1.0 + loss) for loss in safe_losses]
        total_loss_weight = sum(loss_weights)
        if total_loss_weight > 0:
            loss_weights = [w / total_loss_weight for w in loss_weights]
        else:
            loss_weights = [1.0 / num_clients] * num_clients

        # 2. è®¡ç®— diversity_weights è°ƒæ•´å› å­
        div_weights = [min(1.5, 1.0 + 0.3 * score) for score in diversity_scores]

        # 3. ä¸‰é‡æƒé‡ç»“åˆ (æ ¸å¿ƒä¿®æ”¹)
        loss_aware_weights = [0.6 * dw + 0.4 * lw for dw, lw in zip(data_weights, loss_weights)]
        weights = [law * divw for law, divw in zip(loss_aware_weights, div_weights)]
    else:
        weights = data_weights

    # å½’ä¸€åŒ–æƒé‡å¹¶ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
    total_weight = sum(weights)
    if total_weight > 0:
        weights = [w / total_weight for w in weights]
    else:
        weights = [1.0 / num_clients] * num_clients

    # æ‰§è¡ŒåŠ æƒèšåˆ
    aggregated_weights = copy.deepcopy(local_weights[0])
    first_weight = local_weights[0]

    for key in first_weight.keys():
        aggregated_weights[key] = torch.zeros_like(first_weight[key])
        for i, weight_dict in enumerate(local_weights):
            aggregated_weights[key] += weights[i] * weight_dict[key]

    return aggregated_weights


def create_personalization_layer(model, client_id, device):
    """
    ä¸ºNon-IIDåœºæ™¯åˆ›å»ºè½»é‡çº§ä¸ªæ€§åŒ–å±‚ - æ™®é€šè”é‚¦å­¦ä¹ ç‰ˆæœ¬
    """
    # è·å–æ¨¡å‹çš„æœ€åä¸€å±‚ç‰¹å¾ç»´åº¦
    if hasattr(model, 'classifier') and hasattr(model.classifier, 'in_features'):
        # å¯¹äºæœ‰classifierçš„æ¨¡å‹
        feature_dim = model.classifier.in_features
    elif hasattr(model, 'fc') and hasattr(model.fc, 'in_features'):
        # å¯¹äºæœ‰fcçš„æ¨¡å‹
        feature_dim = model.fc.in_features
    else:
        # é»˜è®¤ç»´åº¦
        feature_dim = 512
    
    # åˆ›å»ºè½»é‡çº§ä¸ªæ€§åŒ–å±‚ï¼ˆä»…åŒ…å«å°‘é‡å‚æ•°ï¼‰
    personalization_layer = torch.nn.Sequential(
        torch.nn.Linear(feature_dim, feature_dim // 4),
        torch.nn.ReLU(),
        torch.nn.Dropout(0.2),
        torch.nn.Linear(feature_dim // 4, feature_dim)
    ).to(device)
    
    return personalization_layer


def apply_personalization(features, personalization_layer, alpha=0.2):
    """
    åº”ç”¨ä¸ªæ€§åŒ–å˜æ¢ - æ™®é€šè”é‚¦å­¦ä¹ ç‰ˆæœ¬
    features: æ¨¡å‹æå–çš„ç‰¹å¾
    personalization_layer: ä¸ªæ€§åŒ–å±‚
    alpha: ä¸ªæ€§åŒ–ç¨‹åº¦æƒé‡
    """
    if personalization_layer is None:
        return features
    
    # åº”ç”¨ä¸ªæ€§åŒ–å˜æ¢
    personalized_features = personalization_layer(features)
    # æ··åˆåŸå§‹ç‰¹å¾å’Œä¸ªæ€§åŒ–ç‰¹å¾
    mixed_features = (1 - alpha) * features + alpha * personalized_features
    
    return mixed_features


'''
 average_weights å®ç°æ˜¯ç®€å•å¹³å‡ï¼ˆæ¯ä¸ªå®¢æˆ·ç«¯æƒé‡ç›¸åŒï¼‰ï¼Œè€Œæ ‡å‡†çš„ FedAvg ç®—æ³•åº”è¯¥æ˜¯åŠ æƒå¹³å‡ï¼Œæ¯ä¸ªå®¢æˆ·ç«¯çš„æƒé‡åº”ä¸å…¶æœ¬åœ°æ•°æ®é‡æˆæ­£æ¯”
'''


def exp_details(args): # æ‰“å°å®éªŒçš„é…ç½®å‚æ•°
    print('\nExperimental details:')
    print(f'    Model     : {args.model}')
    print(f'    Optimizer : {args.optimizer}')
    print(f'    Learning  : {args.lr}')
    print(f'    Global Rounds   : {args.epochs}\n')

    print('    Federated parameters:')
    if args.iid:
        print('    IID')
    else:
        print('    Non-IID')
    print(f'    Fraction of users  : {args.frac}')
    print(f'    Local Batch size   : {args.local_bs}')
    print(f'    Local Epochs       : {args.local_ep}\n')
    return